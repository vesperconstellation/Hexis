import asyncio
import json
import os
import time
import uuid
from datetime import datetime, timedelta, timezone

import asyncpg
import numpy as np
import pytest
from tenacity import (
    AsyncRetrying,
    RetryError,
    retry_if_exception_type,
    retry_if_result,
    stop_after_delay,
    wait_fixed,
)

from tests.utils import (
    _coerce_json,
    _restore_embedding_retry_config,
    _set_embedding_retry_config,
    get_test_identifier,
)

pytestmark = [pytest.mark.asyncio(loop_scope="session"), pytest.mark.db]

EMBEDDING_DIMENSION = int(os.getenv("EMBEDDING_DIMENSION", os.getenv("EMBEDDING_DIM", "768")))
PERF_CLUSTER_RETRIEVAL_SECONDS = float(os.getenv("HEXIS_TEST_PERF_CLUSTER_SECONDS", "2.0"))
PERF_VECTOR_SEARCH_SECONDS = float(os.getenv("HEXIS_TEST_PERF_VECTOR_SECONDS", "2.5"))
PERF_COMPLEX_QUERY_SECONDS = float(os.getenv("HEXIS_TEST_PERF_COMPLEX_QUERY_SECONDS", "1.5"))
PERF_VIEW_QUERY_SECONDS = float(os.getenv("HEXIS_TEST_PERF_VIEW_QUERY_SECONDS", "1.5"))
PERF_OPTIMIZE_QUERY_SECONDS = float(os.getenv("HEXIS_TEST_PERF_OPTIMIZE_QUERY_SECONDS", "2.0"))
LARGE_DATASET_SIZE = int(os.getenv("HEXIS_TEST_LARGE_DATASET_SIZE", "1000"))

async def _ensure_memory_node(conn, memory_id: uuid.UUID, mem_type: str) -> None:
    await conn.execute("LOAD 'age';")
    await conn.execute("SET search_path = ag_catalog, public;")
    await conn.execute(
        """
        SELECT * FROM cypher('memory_graph', $q$
            MERGE (n:MemoryNode {memory_id: '%s'})
            SET n.type = '%s', n.created_at = '%s'
            RETURN n
        $q$) as (n agtype);
        """
        % (memory_id, mem_type, datetime.now(timezone.utc).isoformat())
    )


async def _fetch_episode_for_memory(conn, memory_id: uuid.UUID):
    return await conn.fetchrow(
        """
        SELECT e.id as episode_id, fem.sequence_order, e.started_at, e.ended_at
        FROM episodes e
        CROSS JOIN LATERAL find_episode_memories_graph(e.id) fem
        WHERE fem.memory_id = $1
        ORDER BY e.started_at DESC
        LIMIT 1
        """,
        memory_id,
    )


async def _set_heartbeat_state(
    conn,
    heartbeat_count: int,
    current_energy: float | None = None,
) -> None:
    payload = {"heartbeat_count": heartbeat_count}
    if current_energy is not None:
        payload["current_energy"] = current_energy
    await conn.execute(
        "SELECT set_state('heartbeat_state', $1::jsonb)",
        json.dumps(payload),
    )


async def _create_goal_memory(conn, content: str) -> uuid.UUID:
    return await conn.fetchval(
        """
        INSERT INTO memories (type, content, embedding)
        VALUES ('goal'::memory_type, $1, array_fill(0.05, ARRAY[embedding_dimension()])::vector)
        RETURNING id
        """,
        content,
    )


async def _create_transformable_belief(
    conn,
    content: str,
    category: str = "belief",
    subcategory: str = "test_belief",
    origin: str = "user_initialized",
    trait: str | None = None,
) -> uuid.UUID:
    return await conn.fetchval(
        """
        INSERT INTO memories (type, content, embedding, metadata)
        VALUES (
            'worldview'::memory_type,
            $1,
            array_fill(0.08, ARRAY[embedding_dimension()])::vector,
            jsonb_build_object(
                'category', $2::text,
                'subcategory', $3::text,
                'origin', $4::text,
                'trait', $5::text,
                'change_requires', 'deliberate_transformation',
                'transformation_state', default_transformation_state()
            )
        )
        RETURNING id
        """,
        content,
        category,
        subcategory,
        origin,
        trait,
    )


async def _create_episode(
    conn,
    started_at: datetime | None = None,
    ended_at: datetime | None = None,
    summary: str | None = None,
    episode_type: str | None = None,
) -> uuid.UUID:
    return await conn.fetchval(
        """
        INSERT INTO episodes (started_at, ended_at, summary, metadata)
        VALUES ($1, $2, $3, jsonb_build_object('episode_type', $4::text))
        RETURNING id
        """,
        started_at or datetime.now(timezone.utc),
        ended_at,
        summary,
        episode_type,
    )

async def test_extensions(db_pool):
    """Test that required PostgreSQL extensions are installed"""
    async with db_pool.acquire() as conn:
        extensions = await conn.fetch("""
            SELECT extname FROM pg_extension
        """)
        ext_names = {ext['extname'] for ext in extensions}
        
        required_extensions = {'vector', 'age', 'btree_gist', 'pg_trgm', 'http'}
        for ext in required_extensions:
            assert ext in ext_names, f"{ext} extension not found"
        # Verify AGE is loaded
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")
        result = await conn.fetchval("""
            SELECT count(*) FROM ag_catalog.ag_graph
        """)
        assert result >= 0, "AGE extension not properly loaded"

async def test_expected_triggers_are_installed(db_pool):
    async with db_pool.acquire() as conn:
        rows = await conn.fetch(
            """
            SELECT t.tgname, p.proname
            FROM pg_trigger t
            JOIN pg_proc p ON p.oid = t.tgfoid
            WHERE NOT t.tgisinternal
            """
        )
        mapping = {r["tgname"]: r["proname"] for r in rows}

        assert mapping.get("trg_memory_timestamp") == "update_memory_timestamp"
        assert mapping.get("trg_importance_on_access") == "update_memory_importance"
        assert "trg_cluster_activation" not in mapping
        assert mapping.get("trg_neighborhood_staleness") == "mark_neighborhoods_stale"
        assert mapping.get("trg_auto_episode_assignment") == "assign_to_episode"
        # Phase 5 (ReduceScopeCreep): trg_sync_worldview_node removed (worldview_primitives table removed)


async def test_memory_tables(db_pool):
    """Test that all memory tables exist with correct columns and constraints"""
    async with db_pool.acquire() as conn:
        # First check if tables exist
        tables = await conn.fetch("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public'
        """)
        table_names = {t['table_name'] for t in tables}
        
        assert 'working_memory' in table_names, "working_memory table not found"
        assert 'memories' in table_names, "memories table not found"
        # Note: episodic_memories, semantic_memories, procedural_memories, strategic_memories
        # have been collapsed into memories.metadata JSONB column

        # Then check columns
        memories = await conn.fetch("""
            SELECT column_name, data_type, is_nullable
            FROM information_schema.columns
            WHERE table_name = 'memories'
        """)
        columns = {col["column_name"]: col for col in memories}

        # Note: relevance_score is computed via calculate_relevance() function, not a column
        assert "importance" in columns, "importance column not found"
        assert "decay_rate" in columns, "decay_rate column not found"
        assert "last_accessed" in columns, "last_accessed column not found"
        assert "id" in columns and columns["id"]["data_type"] == "uuid"
        assert "content" in columns and columns["content"]["is_nullable"] == "NO"
        assert "embedding" in columns
        assert "type" in columns
        assert "metadata" in columns, "metadata column not found"


async def test_memory_storage(db_pool):
    """Test storing and retrieving different types of memories with metadata"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("memory_storage")

        # Test each memory type with appropriate metadata
        memory_types = ['episodic', 'semantic', 'procedural', 'strategic']
        created_memories = []

        for mem_type in memory_types:
            # Build type-specific metadata
            if mem_type == 'episodic':
                metadata = json.dumps({
                    "action_taken": {"action": "test"},
                    "context": {"context": "test"},
                    "result": {"result": "success"},
                    "emotional_valence": 0.5,
                    "event_time": None
                })
            elif mem_type == 'semantic':
                metadata = json.dumps({
                    "confidence": 0.8,
                    "source_references": [],
                    "category": None,
                    "related_concepts": None
                })
            elif mem_type == 'procedural':
                metadata = json.dumps({
                    "steps": [],
                    "prerequisites": None,
                    "success_count": 0,
                    "total_attempts": 0
                })
            else:  # strategic
                metadata = json.dumps({
                    "pattern_description": "test pattern",
                    "confidence_score": 0.8,
                    "supporting_evidence": None
                })

            # Insert memory with metadata
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    metadata
                ) VALUES (
                    $1::memory_type,
                    'Test ' || $1 || ' memory ' || $2,
                    array_fill(0, ARRAY[embedding_dimension()])::vector,
                    $3::jsonb
                ) RETURNING id
            """, mem_type, test_id, metadata)

            assert memory_id is not None
            created_memories.append(memory_id)

        # Verify storage for our specific test memories
        for mem_type in memory_types:
            count = await conn.fetchval("""
                SELECT COUNT(*)
                FROM memories m
                WHERE m.type = $1 AND m.content LIKE '%' || $2
            """, mem_type, test_id)
            assert count > 0, f"No {mem_type} memories stored for test {test_id}"


async def test_memory_importance(db_pool):
    """Test memory importance updating"""
    async with db_pool.acquire() as conn:
        # Create test memory
        memory_id = await conn.fetchval(
            """
            INSERT INTO memories (
                type, 
                content, 
                embedding,
                importance,
                access_count
            ) VALUES (
                'semantic',
                'Important test content',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                0.5,
                0
            ) RETURNING id
        """
        )

        # Update access count to trigger importance recalculation
        await conn.execute(
            """
            UPDATE memories 
            SET access_count = access_count + 1
            WHERE id = $1
        """,
            memory_id,
        )

        # Check that importance was updated
        new_importance = await conn.fetchval(
            """
            SELECT importance 
            FROM memories 
            WHERE id = $1
        """,
            memory_id,
        )

        assert new_importance != 0.5, "Importance should have been updated"


async def test_age_setup(db_pool):
    """Test AGE graph functionality"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        graph_id = await conn.fetchval("""
            SELECT graphid FROM ag_catalog.ag_graph
            WHERE name = 'memory_graph'::name
        """)
        if graph_id is None:
            await conn.execute("SELECT create_graph('memory_graph');")
            graph_id = await conn.fetchval("""
                SELECT graphid FROM ag_catalog.ag_graph
                WHERE name = 'memory_graph'::name
            """)

        label_count = await conn.fetchval("""
            SELECT COUNT(*)
            FROM ag_catalog.ag_label
            WHERE name = 'MemoryNode'::name
              AND graph = $1
        """, graph_id)
        if int(label_count or 0) == 0:
            await conn.execute("SELECT create_vlabel('memory_graph', 'MemoryNode');")

        # Test graph exists
        result = await conn.fetch("""
            SELECT * FROM ag_catalog.ag_graph
            WHERE name = 'memory_graph'::name
        """)
        assert len(result) == 1, "memory_graph not found"

        # Test vertex label
        result = await conn.fetch("""
            SELECT * FROM ag_catalog.ag_label
            WHERE name = 'MemoryNode'::name
            AND graph = (
                SELECT graphid FROM ag_catalog.ag_graph
                WHERE name = 'memory_graph'::name
            )
        """)
        assert len(result) == 1, "MemoryNode label not found"


async def test_memory_relationships(db_pool):
    """Test graph relationships between different memory types"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")
        
        memory_pairs = [
            ('semantic', 'semantic', 'RELATES_TO'),
            ('episodic', 'semantic', 'LEADS_TO'),
            ('procedural', 'strategic', 'IMPLEMENTS')
        ]
        
        for source_type, target_type, rel_type in memory_pairs:
            # Create source and target memories
            source_id = await conn.fetchval("""
                INSERT INTO memories (type, content, embedding)
                VALUES ($1::memory_type, 'Source ' || $1, array_fill(0, ARRAY[embedding_dimension()])::vector)
                RETURNING id
            """, source_type)
            
            target_id = await conn.fetchval("""
                INSERT INTO memories (type, content, embedding)
                VALUES ($1::memory_type, 'Target ' || $1, array_fill(0, ARRAY[embedding_dimension()])::vector)
                RETURNING id
            """, target_type)
            
            # Create nodes and relationship in graph using string formatting for Cypher
            cypher_query = f"""
                SELECT * FROM ag_catalog.cypher(
                    'memory_graph',
                    $$
                    CREATE (a:MemoryNode {{memory_id: '{str(source_id)}', type: '{source_type}'}}),
                           (b:MemoryNode {{memory_id: '{str(target_id)}', type: '{target_type}'}}),
                           (a)-[r:{rel_type}]->(b)
                    RETURN a, r, b
                    $$
                ) as (a ag_catalog.agtype, r ag_catalog.agtype, b ag_catalog.agtype)
            """
            await conn.execute(cypher_query)
            
            # Verify the relationship was created
            verify_query = f"""
                SELECT * FROM ag_catalog.cypher(
                    'memory_graph',
                    $$
                    MATCH (a:MemoryNode)-[r:{rel_type}]->(b:MemoryNode)
                    WHERE a.memory_id = '{str(source_id)}' AND b.memory_id = '{str(target_id)}'
                    RETURN a, r, b
                    $$
                ) as (a ag_catalog.agtype, r ag_catalog.agtype, b ag_catalog.agtype)
            """
            result = await conn.fetch(verify_query)
            assert len(result) > 0, f"Relationship {rel_type} not found"


async def test_memory_type_specifics(db_pool):
    """Test type-specific memory storage via metadata"""
    async with db_pool.acquire() as conn:
        # Test semantic memory with confidence stored in metadata
        semantic_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding, metadata)
            VALUES (
                'semantic'::memory_type,
                'Test fact',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                '{"confidence": 0.85, "category": ["test"]}'::jsonb
            )
            RETURNING id
        """)

        # Verify semantic metadata
        semantic_meta = await conn.fetchval("""
            SELECT metadata FROM memories WHERE id = $1
        """, semantic_id)
        if isinstance(semantic_meta, str):
            semantic_meta = json.loads(semantic_meta)
        assert semantic_meta.get("confidence") == 0.85, "Confidence not stored correctly"

        # Test procedural memory with steps and counts in metadata
        procedural_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding, metadata)
            VALUES (
                'procedural'::memory_type,
                'Test procedure',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                '{"steps": ["step1", "step2"], "success_count": 8, "total_attempts": 10}'::jsonb
            )
            RETURNING id
        """)

        # Verify procedural metadata and calculate success rate
        proc_meta = await conn.fetchval("""
            SELECT metadata FROM memories WHERE id = $1
        """, procedural_id)
        if isinstance(proc_meta, str):
            proc_meta = json.loads(proc_meta)
        success_count = proc_meta.get("success_count", 0)
        total_attempts = proc_meta.get("total_attempts", 0)
        success_rate = success_count / total_attempts if total_attempts > 0 else 0

        assert success_rate == 0.8, "Success rate calculation incorrect"


async def test_memory_status_transitions(db_pool):
    """Test memory status transitions (audit tracking removed in Phase 8)"""
    async with db_pool.acquire() as conn:
        # Create test memory
        memory_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding, status)
            VALUES (
                'semantic'::memory_type,
                'Test content',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                'active'::memory_status
            ) RETURNING id
        """)

        # Archive memory and verify status change works
        await conn.execute("""
            UPDATE memories
            SET status = 'archived'::memory_status
            WHERE id = $1
        """, memory_id)

        new_status = await conn.fetchval("""
            SELECT status FROM memories WHERE id = $1
        """, memory_id)
        assert new_status == "archived", "Status not updated correctly"

        # Invalidate memory
        await conn.execute("""
            UPDATE memories
            SET status = 'invalidated'::memory_status
            WHERE id = $1
        """, memory_id)

        new_status = await conn.fetchval("""
            SELECT status FROM memories WHERE id = $1
        """, memory_id)
        assert new_status == "invalidated", "Status not updated to invalidated"


async def test_vector_search(db_pool):
    """Test vector similarity search"""
    async with db_pool.acquire() as conn:
        # Clear existing test data (metadata is in memories table, no subtables)
        # Note: memory_changes table removed in Phase 8
        await conn.execute("DELETE FROM memories WHERE content LIKE 'Test content%'")
        
        # Create more distinct test vectors
        test_embeddings = [
            # First vector: alternating 1.0 and 0.8
            '[' + ','.join(['1.0' if i % 2 == 0 else '0.8' for i in range(EMBEDDING_DIMENSION)]) + ']',
            # Second vector: alternating 0.5 and 0.3
            '[' + ','.join(['0.5' if i % 2 == 0 else '0.3' for i in range(EMBEDDING_DIMENSION)]) + ']',
            # Third vector: alternating 0.2 and 0.0
            '[' + ','.join(['0.2' if i % 2 == 0 else '0.0' for i in range(EMBEDDING_DIMENSION)]) + ']'
        ]
        
        # Insert test vectors
        for i, emb in enumerate(test_embeddings):
            await conn.execute("""
                INSERT INTO memories (
                    type, 
                    content, 
                    embedding
                ) VALUES (
                    'semantic'::memory_type,
                    'Test content ' || $1,
                    $2::vector
                )
            """, str(i), emb)

        # Query vector more similar to first pattern
        query_vector = '[' + ','.join(['0.95' if i % 2 == 0 else '0.75' for i in range(EMBEDDING_DIMENSION)]) + ']'
        
        results = await conn.fetch("""
            WITH candidates AS MATERIALIZED (
                SELECT id, content, embedding
                FROM memories
                WHERE content LIKE 'Test content%'
            )
            SELECT
                id,
                content,
                embedding <=> $1::vector as cosine_distance
            FROM candidates
            ORDER BY embedding <=> $1::vector
            LIMIT 3
        """, query_vector)

        assert len(results) >= 2, "Wrong number of results"
        
        # Print distances for debugging
        for r in results:
            print(f"Content: {r['content']}, Distance: {r['cosine_distance']}")
            
        # First result should have smaller cosine distance than second
        assert results[0]['cosine_distance'] < results[1]['cosine_distance'], \
            f"Incorrect distance ordering: {results[0]['cosine_distance']} >= {results[1]['cosine_distance']}"


async def test_complex_graph_queries(db_pool):
    """Test more complex graph operations and queries"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")
        
        # Create a chain of related memories
        memory_chain = [
            ('episodic', 'Start event'),
            ('semantic', 'Derived knowledge'),
            ('procedural', 'Applied procedure')
        ]
        
        prev_id = None
        for mem_type, content in memory_chain:
            # Create memory
            curr_id = await conn.fetchval("""
                INSERT INTO memories (type, content, embedding)
                VALUES ($1::memory_type, $2, array_fill(0, ARRAY[embedding_dimension()])::vector)
                RETURNING id
            """, mem_type, content)
            
            # Create graph node
            await conn.execute(f"""
                SELECT * FROM cypher('memory_graph', $$
                    CREATE (n:MemoryNode {{
                        memory_id: '{curr_id}',
                        type: '{mem_type}'
                    }})
                    RETURN n
                $$) as (n ag_catalog.agtype)
            """)
            
            if prev_id:
                await conn.execute(f"""
                    SELECT * FROM cypher('memory_graph', $$
                        MATCH (a:MemoryNode {{memory_id: '{prev_id}'}}),
                              (b:MemoryNode {{memory_id: '{curr_id}'}})
                        CREATE (a)-[r:LEADS_TO]->(b)
                        RETURN r
                    $$) as (r ag_catalog.agtype)
                """)
            
            prev_id = curr_id
        
        # Test path query with fixed syntax
        result = await conn.fetch("""
            SELECT * FROM cypher('memory_graph', $$
                MATCH p = (s:MemoryNode)-[*]->(t:MemoryNode)
                WHERE s.type = 'episodic' AND t.type = 'procedural'
                RETURN p
            $$) as (path ag_catalog.agtype)
        """)
        
        assert len(result) > 0, "No valid paths found"


async def test_memory_storage_episodic(db_pool):
    """Test storing and retrieving episodic memories with metadata"""
    async with db_pool.acquire() as conn:
        # Create memory with episodic metadata
        metadata = json.dumps({
            "action_taken": {"action": "test"},
            "context": {"context": "test"},
            "result": {"result": "success"},
            "emotional_valence": 0.5,
            "verification_status": True,
            "event_time": "2024-01-01T00:00:00Z"
        })
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                decay_rate,
                metadata
            ) VALUES (
                'episodic'::memory_type,
                'Test episodic memory',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                0.5,
                0.01,
                $1::jsonb
            ) RETURNING id
        """, metadata)

        assert memory_id is not None

        # Verify storage including metadata fields
        result = await conn.fetchrow("""
            SELECT metadata
            FROM memories
            WHERE type = 'episodic' AND id = $1
        """, memory_id)

        metadata = result['metadata']
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        assert metadata.get('verification_status') is True, "Verification status not set"
        assert metadata.get('event_time') is not None, "Event time not set"


async def test_memory_storage_semantic(db_pool):
    """Test storing and retrieving semantic memories with metadata"""
    async with db_pool.acquire() as conn:
        metadata = json.dumps({
            "confidence": 0.8,
            "source_references": {"source": "test"},
            "contradictions": {"contradictions": []},
            "category": ["test_category"],
            "related_concepts": ["test_concept"],
            "last_validated": "2024-01-01T00:00:00Z"
        })
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                decay_rate,
                metadata
            ) VALUES (
                'semantic'::memory_type,
                'Test semantic memory',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                0.5,
                0.01,
                $1::jsonb
            ) RETURNING id
        """, metadata)

        assert memory_id is not None

        # Verify including metadata fields
        result = await conn.fetchrow("""
            SELECT metadata
            FROM memories
            WHERE type = 'semantic' AND id = $1
        """, memory_id)

        metadata = result['metadata']
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        assert metadata.get('last_validated') is not None, "Last validated timestamp not set"


async def test_memory_storage_strategic(db_pool):
    """Test storing and retrieving strategic memories with metadata"""
    async with db_pool.acquire() as conn:
        metadata = json.dumps({
            "pattern_description": "Test pattern",
            "supporting_evidence": {"evidence": ["test"]},
            "confidence_score": 0.7,
            "success_metrics": {"metrics": {"success": 0.8}},
            "adaptation_history": {"adaptations": []},
            "context_applicability": {"contexts": ["test_context"]}
        })
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                decay_rate,
                metadata
            ) VALUES (
                'strategic'::memory_type,
                'Test strategic memory',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                0.5,
                0.01,
                $1::jsonb
            ) RETURNING id
        """, metadata)

        assert memory_id is not None

        count = await conn.fetchval("""
            SELECT COUNT(*)
            FROM memories
            WHERE type = 'strategic'
        """)
        assert count > 0, "No strategic memories stored"


async def test_memory_storage_procedural(db_pool):
    """Test storing and retrieving procedural memories with metadata"""
    async with db_pool.acquire() as conn:
        metadata = json.dumps({
            "steps": {"steps": ["step1", "step2"]},
            "prerequisites": {"prereqs": ["prereq1"]},
            "success_count": 5,
            "total_attempts": 10,
            "average_duration_seconds": 3600,
            "failure_points": {"failures": []}
        })
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                decay_rate,
                metadata
            ) VALUES (
                'procedural'::memory_type,
                'Test procedural memory',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                0.5,
                0.01,
                $1::jsonb
            ) RETURNING id
        """, metadata)

        assert memory_id is not None

        count = await conn.fetchval("""
            SELECT COUNT(*)
            FROM memories
            WHERE type = 'procedural'
        """)
        assert count > 0, "No procedural memories stored"
        
async def test_working_memory(db_pool):
    """Test working memory operations"""
    async with db_pool.acquire() as conn:
        # Test inserting into working memory
        working_memory_id = await conn.fetchval("""
            INSERT INTO working_memory (
                content,
                embedding,
                expiry
            ) VALUES (
                'Test working memory',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                CURRENT_TIMESTAMP + interval '1 hour'
            ) RETURNING id
        """)
        
        assert working_memory_id is not None, "Failed to insert working memory"
        
        # Test expiry
        expired_count = await conn.fetchval("""
            SELECT COUNT(*) 
            FROM working_memory 
            WHERE expiry < CURRENT_TIMESTAMP
        """)
        
        assert isinstance(expired_count, int), "Failed to query expired memories"

async def test_memory_relevance(db_pool):
    """Test memory relevance score calculation"""
    async with db_pool.acquire() as conn:
        # Create test memory with known values
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                decay_rate,
                created_at
            ) VALUES (
                'semantic'::memory_type,
                'Test relevance',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                0.8,
                0.01,
                CURRENT_TIMESTAMP - interval '1 day'
            ) RETURNING id
        """)
        
        # Check relevance score using calculate_relevance function
        relevance = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed)
            FROM memories
            WHERE id = $1
        """, memory_id)

        assert relevance is not None, "Relevance score not calculated"
        assert relevance < 0.8, "Relevance should be less than importance due to decay"

async def test_worldview_memories(db_pool):
    """Test worldview memories (Phase 5: replaces worldview_primitives test)"""
    async with db_pool.acquire() as conn:
        # Create worldview memory using the new function
        worldview_id = await conn.fetchval("""
            SELECT create_worldview_memory(
                'Test belief about values',
                'belief',
                0.8,
                0.7,
                0.8,
                'discovered'
            )
        """)

        assert worldview_id is not None, "Worldview memory should be created"

        # Verify it's stored correctly
        mem = await conn.fetchrow("""
            SELECT * FROM memories WHERE id = $1
        """, worldview_id)

        assert mem is not None
        assert str(mem['type']) == 'worldview'
        metadata = json.loads(mem['metadata']) if isinstance(mem['metadata'], str) else mem['metadata']
        assert metadata['category'] == 'belief'
        assert float(metadata['confidence']) == 0.8


# Phase 5 (ReduceScopeCreep): test_identity_model removed - identity_aspects table removed
# Identity aspects are now graph edges from SelfNode

# test_memory_changes_tracking removed - memory_changes table removed in Phase 8 (ReduceScopeCreep)

async def test_enhanced_relevance_scoring(db_pool):
    """Test the enhanced relevance scoring system"""
    async with db_pool.acquire() as conn:
        # Create test memory with specific parameters
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                decay_rate,
                created_at,
                access_count
            ) VALUES (
                'semantic'::memory_type,
                'Test relevance scoring',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                0.8,
                0.01,
                CURRENT_TIMESTAMP - interval '1 day',
                5
            ) RETURNING id
        """)
        
        # Get initial relevance score using calculate_relevance function
        initial_score = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed)
            FROM memories
            WHERE id = $1
        """, memory_id)

        # Update access count to trigger importance change
        await conn.execute("""
            UPDATE memories
            SET access_count = access_count + 1
            WHERE id = $1
        """, memory_id)

        # Get updated relevance score
        updated_score = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed)
            FROM memories
            WHERE id = $1
        """, memory_id)

        assert initial_score is not None, "Initial relevance score not calculated"
        assert updated_score is not None, "Updated relevance score not calculated"
        assert updated_score != initial_score, "Relevance score should change with importance"

async def test_age_in_days_function(db_pool):
    """Test the age_in_days function"""
    async with db_pool.acquire() as conn:
        # Test current timestamp (should be 0 days)
        result = await conn.fetchval("""
            SELECT age_in_days(CURRENT_TIMESTAMP)
        """)
        assert result < 1, "Current timestamp should be less than 1 day old"

        # Test 1 day ago
        result = await conn.fetchval("""
            SELECT age_in_days(CURRENT_TIMESTAMP - interval '1 day')
        """)
        assert abs(result - 1.0) < 0.1, "Should be approximately 1 day"

        # Test 7 days ago
        result = await conn.fetchval("""
            SELECT age_in_days(CURRENT_TIMESTAMP - interval '7 days')
        """)
        assert abs(result - 7.0) < 0.1, "Should be approximately 7 days"


async def test_default_transformation_state(db_pool):
    async with db_pool.acquire() as conn:
        state = _coerce_json(await conn.fetchval("SELECT default_transformation_state()"))
        assert state["active_exploration"] is False
        assert state["exploration_goal_id"] is None
        assert state["evidence_memories"] == []
        assert state["reflection_count"] == 0
        assert state["first_questioned_heartbeat"] is None
        assert state["contemplation_actions"] == 0


async def test_normalize_transformation_state_null_returns_defaults(db_pool):
    async with db_pool.acquire() as conn:
        state = _coerce_json(await conn.fetchval("SELECT normalize_transformation_state(NULL)"))
        baseline = _coerce_json(await conn.fetchval("SELECT default_transformation_state()"))
        assert state == baseline


async def test_normalize_transformation_state_merges_values(db_pool):
    async with db_pool.acquire() as conn:
        state = _coerce_json(
            await conn.fetchval(
                "SELECT normalize_transformation_state($1::jsonb)",
                json.dumps({"active_exploration": True, "reflection_count": 3}),
            )
        )
        assert state["active_exploration"] is True
        assert state["reflection_count"] == 3
        assert state["evidence_memories"] == []
        assert state["contemplation_actions"] == 0


async def test_get_transformation_config_prefers_subcategory(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("transformation_prefers_subcategory")
            sub_key = f"transformation.sub_{test_id}"
            cat_key = f"transformation.cat_{test_id}"
            await conn.execute(
                """
                INSERT INTO config (key, value)
                VALUES ($1, $2::jsonb)
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value
                """,
                sub_key,
                json.dumps({"min_reflections": 2}),
            )
            await conn.execute(
                """
                INSERT INTO config (key, value)
                VALUES ($1, $2::jsonb)
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value
                """,
                cat_key,
                json.dumps({"min_reflections": 5}),
            )
            cfg = _coerce_json(
                await conn.fetchval(
                    "SELECT get_transformation_config($1, $2)",
                    f"sub_{test_id}",
                    f"cat_{test_id}",
                )
            )
            assert cfg["min_reflections"] == 2
        finally:
            await tr.rollback()


async def test_get_transformation_config_falls_back_to_category(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("transformation_fallback_category")
            cat_key = f"transformation.cat_only_{test_id}"
            await conn.execute(
                """
                INSERT INTO config (key, value)
                VALUES ($1, $2::jsonb)
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value
                """,
                cat_key,
                json.dumps({"min_reflections": 7}),
            )
            cfg = _coerce_json(
                await conn.fetchval(
                    "SELECT get_transformation_config($1, $2)",
                    f"missing_{test_id}",
                    f"cat_only_{test_id}",
                )
            )
            assert cfg["min_reflections"] == 7
        finally:
            await tr.rollback()


async def test_get_transformation_config_returns_null_when_missing(db_pool):
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("transformation_missing")
        cfg = _coerce_json(
            await conn.fetchval(
                "SELECT get_transformation_config($1, $2)",
                f"missing_{test_id}",
                f"missing_{test_id}",
            )
        )
        assert cfg is None


async def test_begin_belief_exploration_success(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("begin_explore")
            await _set_heartbeat_state(conn, 12, 10)
            goal_id = await _create_goal_memory(conn, f"Goal {test_id}")
            belief_id = await _create_transformable_belief(
                conn,
                f"Belief {test_id}",
                subcategory=f"sub_{test_id}",
            )

            result = _coerce_json(
                await conn.fetchval(
                    "SELECT begin_belief_exploration($1, $2)",
                    belief_id,
                    goal_id,
                )
            )
            assert result["success"] is True

            state = _coerce_json(
                await conn.fetchval(
                    "SELECT metadata->'transformation_state' FROM memories WHERE id = $1",
                    belief_id,
                )
            )
            assert state["active_exploration"] is True
            assert state["exploration_goal_id"] == str(goal_id)
            assert state["reflection_count"] == 0
            assert state["evidence_memories"] == []
            assert state["contemplation_actions"] == 0
            assert state["first_questioned_heartbeat"] == 12
        finally:
            await tr.rollback()


async def test_begin_belief_exploration_rejects_non_transformable(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            belief_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, metadata)
                VALUES (
                    'worldview'::memory_type,
                    $1,
                    array_fill(0.04, ARRAY[embedding_dimension()])::vector,
                    jsonb_build_object('category', 'belief', 'subcategory', 'test')
                )
                RETURNING id
                """,
                f"Not transformable {get_test_identifier('non_transformable')}",
            )
            result = _coerce_json(
                await conn.fetchval(
                    "SELECT begin_belief_exploration($1, $2)",
                    belief_id,
                    uuid.uuid4(),
                )
            )
            assert result["success"] is False
            assert result["reason"] == "not_transformable"
        finally:
            await tr.rollback()


async def test_record_transformation_effort_tracks_reflections_and_evidence(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("effort")
            await _set_heartbeat_state(conn, 5, 10)
            goal_id = await _create_goal_memory(conn, f"Goal {test_id}")
            belief_id = await _create_transformable_belief(
                conn,
                f"Belief {test_id}",
                subcategory=f"sub_{test_id}",
            )
            await conn.fetchval("SELECT begin_belief_exploration($1, $2)", belief_id, goal_id)

            evidence_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, importance, trust_level)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector, 0.9, 0.9)
                RETURNING id
                """,
                f"Evidence {test_id}",
            )

            result = _coerce_json(
                await conn.fetchval(
                    "SELECT record_transformation_effort($1, 'reflect', $2, $3)",
                    belief_id,
                    "Noted evidence",
                    evidence_id,
                )
            )
            assert result["success"] is True
            assert result["reflection_increment"] == 1
            assert result["new_reflection_count"] == 1

            state = _coerce_json(
                await conn.fetchval(
                    "SELECT metadata->'transformation_state' FROM memories WHERE id = $1",
                    belief_id,
                )
            )
            assert state["reflection_count"] == 1
            assert state["contemplation_actions"] == 1
            assert str(evidence_id) in state["evidence_memories"]

            await conn.fetchval(
                "SELECT record_transformation_effort($1, 'debate_internally', NULL, $2)",
                belief_id,
                evidence_id,
            )
            state = _coerce_json(
                await conn.fetchval(
                    "SELECT metadata->'transformation_state' FROM memories WHERE id = $1",
                    belief_id,
                )
            )
            assert state["reflection_count"] == 3
            assert state["contemplation_actions"] == 2
            assert state["evidence_memories"].count(str(evidence_id)) == 1
        finally:
            await tr.rollback()


async def test_abandon_belief_exploration_resets_state(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("abandon")
            await _set_heartbeat_state(conn, 2, 10)
            goal_id = await _create_goal_memory(conn, f"Goal {test_id}")
            belief_id = await _create_transformable_belief(
                conn,
                f"Belief {test_id}",
                subcategory=f"sub_{test_id}",
            )
            await conn.fetchval(
                "SELECT begin_belief_exploration($1, $2)",
                belief_id,
                goal_id,
            )

            result = _coerce_json(
                await conn.fetchval(
                    "SELECT abandon_belief_exploration($1, $2)",
                    belief_id,
                    "No longer relevant",
                )
            )
            assert result["success"] is True

            state = _coerce_json(
                await conn.fetchval(
                    "SELECT metadata->'transformation_state' FROM memories WHERE id = $1",
                    belief_id,
                )
            )
            baseline = _coerce_json(await conn.fetchval("SELECT default_transformation_state()"))
            assert state == baseline
        finally:
            await tr.rollback()


async def test_get_transformation_progress_reports_metrics(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("progress")
            subcategory = f"sub_{test_id}"
            await conn.execute(
                """
                INSERT INTO config (key, value)
                VALUES ($1, $2::jsonb)
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value
                """,
                f"transformation.{subcategory}",
                json.dumps(
                    {
                        "min_reflections": 2,
                        "min_heartbeats": 3,
                        "evidence_threshold": 0.5,
                        "stability": 0.8,
                        "max_change_per_attempt": 0.3,
                    }
                ),
            )
            await _set_heartbeat_state(conn, 5, 10)
            goal_id = await _create_goal_memory(conn, f"Goal {test_id}")
            belief_id = await _create_transformable_belief(
                conn,
                f"Belief {test_id}",
                subcategory=subcategory,
            )
            await conn.fetchval(
                "SELECT begin_belief_exploration($1, $2)",
                belief_id,
                goal_id,
            )
            await _set_heartbeat_state(conn, 9, 10)

            evidence_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, importance, trust_level)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector, 0.9, 0.9)
                RETURNING id
                """,
                f"Evidence {test_id}",
            )

            await conn.fetchval(
                "SELECT record_transformation_effort($1, 'reflect', NULL, $2)",
                belief_id,
                evidence_id,
            )
            await conn.fetchval(
                "SELECT record_transformation_effort($1, 'debate_internally', NULL, $2)",
                belief_id,
                evidence_id,
            )

            progress = _coerce_json(
                await conn.fetchval(
                    "SELECT get_transformation_progress($1)",
                    belief_id,
                )
            )
            assert progress["status"] == "exploring"
            assert progress["requirements"]["min_reflections"] == 2
            assert progress["requirements"]["min_heartbeats"] == 3
            assert progress["progress"]["reflections"]["current"] == 3
            assert progress["progress"]["time"]["current_heartbeats"] == 4
            assert progress["progress"]["evidence"]["memory_count"] == 1
            samples = progress["evidence_samples"]
            assert any(sample["memory_id"] == str(evidence_id) for sample in samples)
        finally:
            await tr.rollback()


async def test_get_active_transformations_context_returns_active_only(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("active")
            subcategory = f"sub_{test_id}"
            await conn.execute(
                """
                INSERT INTO config (key, value)
                VALUES ($1, $2::jsonb)
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value
                """,
                f"transformation.{subcategory}",
                json.dumps({"min_reflections": 1, "min_heartbeats": 0, "evidence_threshold": 0.1}),
            )
            await _set_heartbeat_state(conn, 1, 10)
            goal_id = await _create_goal_memory(conn, f"Goal {test_id}")
            belief_one = await _create_transformable_belief(
                conn,
                f"Belief one {test_id}",
                subcategory=subcategory,
            )
            belief_two = await _create_transformable_belief(
                conn,
                f"Belief two {test_id}",
                subcategory=subcategory,
            )
            await conn.fetchval("SELECT begin_belief_exploration($1, $2)", belief_one, goal_id)
            await conn.fetchval("SELECT begin_belief_exploration($1, $2)", belief_two, goal_id)

            context = _coerce_json(await conn.fetchval("SELECT get_active_transformations_context(2)"))
            ids = {item["belief_id"] for item in context}
            assert str(belief_one) in ids
            assert str(belief_two) in ids
        finally:
            await tr.rollback()


async def test_calibrate_neutral_belief_updates_metadata_and_content(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("calibrate")
            await _set_heartbeat_state(conn, 4, 10)
            goal_id = await _create_goal_memory(conn, f"Goal {test_id}")
            belief_id = await _create_transformable_belief(
                conn,
                f"Neutral belief {test_id}",
                subcategory="openness",
                origin="neutral_default",
                trait="openness",
            )
            await conn.fetchval("SELECT begin_belief_exploration($1, $2)", belief_id, goal_id)

            evidence_ids = []
            for idx in range(2):
                evidence_ids.append(
                    await conn.fetchval(
                        """
                        INSERT INTO memories (type, content, embedding, importance, trust_level)
                        VALUES ('semantic'::memory_type, $1, array_fill(0.3, ARRAY[embedding_dimension()])::vector, 0.9, 0.9)
                        RETURNING id
                        """,
                        f"Evidence {test_id} {idx}",
                    )
                )
            await conn.fetchval(
                "SELECT record_transformation_effort($1, 'reflect', NULL, $2)",
                belief_id,
                evidence_ids[0],
            )

            result = _coerce_json(
                await conn.fetchval(
                    "SELECT calibrate_neutral_belief($1, 0.8, $2::uuid[])",
                    belief_id,
                    evidence_ids,
                )
            )
            assert result["success"] is True

            row = await conn.fetchrow(
                "SELECT content, metadata FROM memories WHERE id = $1",
                belief_id,
            )
            metadata = _coerce_json(row["metadata"])
            assert metadata["origin"] == "self_discovered"
            assert abs(metadata["value"] - 0.8) < 0.001
            assert metadata["transformation_state"]["active_exploration"] is False
            assert "high" in row["content"]
        finally:
            await tr.rollback()


async def test_initialize_personality_creates_traits(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            result = _coerce_json(
                await conn.fetchval(
                    "SELECT initialize_personality($1::jsonb)",
                    json.dumps({"openness": 0.7}),
                )
            )
            assert result["success"] is True
            assert result["created_traits"] == 5

            count = await conn.fetchval(
                """
                SELECT COUNT(*) FROM memories
                WHERE type = 'worldview' AND metadata->>'subcategory' = 'personality'
                """
            )
            assert int(count) == 5

            row = await conn.fetchrow(
                """
                SELECT metadata FROM memories
                WHERE type = 'worldview' AND metadata->>'trait' = 'openness'
                """
            )
            metadata = _coerce_json(row["metadata"])
            assert metadata["origin"] == "user_initialized"
            assert abs(metadata["value"] - 0.7) < 0.01
        finally:
            await tr.rollback()


async def test_initialize_core_values_creates_values(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            result = _coerce_json(
                await conn.fetchval(
                    "SELECT initialize_core_values($1::jsonb)",
                    json.dumps({"honesty": 0.8}),
                )
            )
            assert result["success"] is True
            assert result["created_values"] == 5

            count = await conn.fetchval(
                """
                SELECT COUNT(*) FROM memories
                WHERE type = 'worldview' AND metadata->>'subcategory' = 'core_value'
                """
            )
            assert int(count) == 5

            row = await conn.fetchrow(
                """
                SELECT metadata FROM memories
                WHERE type = 'worldview' AND metadata->>'value_name' = 'honesty'
                """
            )
            metadata = _coerce_json(row["metadata"])
            assert metadata["origin"] == "user_initialized"
            assert abs(metadata["value"] - 0.8) < 0.01
        finally:
            await tr.rollback()


async def test_initialize_worldview_creates_expected_keys(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            payload = {
                "religion": "I am a humanist",
                "self_identity": "I am a builder",
            }
            result = _coerce_json(
                await conn.fetchval(
                    "SELECT initialize_worldview($1::jsonb)",
                    json.dumps(payload),
                )
            )
            assert result["success"] is True
            assert result["created_worldview"] == 4

            religion = await conn.fetchrow(
                """
                SELECT content, metadata FROM memories
                WHERE type = 'worldview' AND metadata->>'subcategory' = 'religion'
                """
            )
            rel_meta = _coerce_json(religion["metadata"])
            assert rel_meta["origin"] == "user_initialized"
            assert religion["content"] == "I am a humanist"

            ethics = await conn.fetchrow(
                """
                SELECT content, metadata FROM memories
                WHERE type = 'worldview' AND metadata->>'subcategory' = 'ethical_framework'
                """
            )
            ethics_meta = _coerce_json(ethics["metadata"])
            assert ethics_meta["origin"] == "neutral_default"
            assert "still exploring" in ethics["content"]
        finally:
            await tr.rollback()


async def test_check_transformation_readiness_returns_ready_items(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("ready")
            subcategory = f"sub_{test_id}"
            await conn.execute(
                """
                INSERT INTO config (key, value)
                VALUES ($1, $2::jsonb)
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value
                """,
                f"transformation.{subcategory}",
                json.dumps({"min_reflections": 1, "min_heartbeats": 1, "evidence_threshold": 0.4}),
            )
            await _set_heartbeat_state(conn, 3, 10)
            goal_id = await _create_goal_memory(conn, f"Goal {test_id}")
            belief_id = await _create_transformable_belief(
                conn,
                f"Belief {test_id}",
                subcategory=subcategory,
            )
            await conn.fetchval("SELECT begin_belief_exploration($1, $2)", belief_id, goal_id)
            await _set_heartbeat_state(conn, 5, 10)

            evidence_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, importance, trust_level)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector, 0.9, 0.9)
                RETURNING id
                """,
                f"Evidence {test_id}",
            )
            await conn.fetchval(
                "SELECT record_transformation_effort($1, 'reflect', NULL, $2)",
                belief_id,
                evidence_id,
            )

            ready = _coerce_json(await conn.fetchval("SELECT check_transformation_readiness()"))
            assert any(item["belief_id"] == str(belief_id) for item in ready)
        finally:
            await tr.rollback()


async def test_attempt_worldview_transformation_updates_belief(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("attempt")
            subcategory = f"sub_{test_id}"
            await conn.execute(
                """
                INSERT INTO config (key, value)
                VALUES ($1, $2::jsonb)
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value
                """,
                f"transformation.{subcategory}",
                json.dumps({"min_reflections": 1, "min_heartbeats": 0, "evidence_threshold": 0.4}),
            )
            await _set_heartbeat_state(conn, 2, 10)
            goal_id = await _create_goal_memory(conn, f"Goal {test_id}")
            belief_id = await _create_transformable_belief(
                conn,
                f"Belief {test_id}",
                subcategory=subcategory,
            )
            await conn.fetchval("SELECT begin_belief_exploration($1, $2)", belief_id, goal_id)

            evidence_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, importance, trust_level)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector, 0.9, 0.9)
                RETURNING id
                """,
                f"Evidence {test_id}",
            )
            await conn.fetchval(
                "SELECT record_transformation_effort($1, 'reflect', NULL, $2)",
                belief_id,
                evidence_id,
            )

            new_content = f"Updated belief {test_id}"
            result = _coerce_json(
                await conn.fetchval(
                    "SELECT attempt_worldview_transformation($1, $2, $3)",
                    belief_id,
                    new_content,
                    "shift",
                )
            )
            assert result["success"] is True
            assert result["memory_id"] is not None

            row = await conn.fetchrow(
                "SELECT content, metadata FROM memories WHERE id = $1",
                belief_id,
            )
            metadata = _coerce_json(row["metadata"])
            assert row["content"] == new_content
            assert metadata["transformation_state"]["active_exploration"] is False
            assert isinstance(metadata["change_history"], list)
            assert len(metadata["change_history"]) == 1
        finally:
            await tr.rollback()


async def test_normalize_source_reference_handles_invalid_input(db_pool):
    async with db_pool.acquire() as conn:
        result = _coerce_json(await conn.fetchval("SELECT normalize_source_reference(NULL)"))
        assert result == {}
        result = _coerce_json(await conn.fetchval("SELECT normalize_source_reference('\"oops\"'::jsonb)"))
        assert result == {}


async def test_normalize_source_reference_clamps_and_defaults(db_pool):
    async with db_pool.acquire() as conn:
        payload = {
            "kind": "paper",
            "ref": "doi:10.1234/abcd",
            "label": "Test paper",
            "author": "A. Author",
            "observed_at": "2020-01-01T00:00:00Z",
            "trust": 1.7,
            "content_hash": "hashy",
        }
        result = _coerce_json(
            await conn.fetchval(
                "SELECT normalize_source_reference($1::jsonb)",
                json.dumps(payload),
            )
        )
        assert result["kind"] == "paper"
        assert result["ref"] == "doi:10.1234/abcd"
        assert result["label"] == "Test paper"
        assert result["author"] == "A. Author"
        assert result["content_hash"] == "hashy"
        assert abs(float(result["trust"]) - 1.0) < 0.0001
        observed = result.get("observed_at")
        assert observed is not None
        if isinstance(observed, str):
            datetime.fromisoformat(observed.replace("Z", "+00:00"))
        else:
            assert isinstance(observed, datetime)


async def test_normalize_source_references_handles_array_and_object(db_pool):
    async with db_pool.acquire() as conn:
        sources = [
            {"kind": "paper", "ref": "a", "trust": 0.9},
            {"kind": "paper", "ref": "b", "trust": 0.8},
        ]
        result = _coerce_json(
            await conn.fetchval(
                "SELECT normalize_source_references($1::jsonb)",
                json.dumps(sources),
            )
        )
        assert isinstance(result, list)
        assert len(result) == 2

        result = _coerce_json(
            await conn.fetchval(
                "SELECT normalize_source_references($1::jsonb)",
                json.dumps({"kind": "paper", "ref": "solo"}),
            )
        )
        assert len(result) == 1
        assert result[0]["ref"] == "solo"

        result = _coerce_json(
            await conn.fetchval(
                "SELECT normalize_source_references($1::jsonb)",
                json.dumps("oops"),
            )
        )
        assert result == []


async def test_dedupe_source_references_prefers_latest(db_pool):
    async with db_pool.acquire() as conn:
        sources = [
            {"kind": "paper", "ref": "dup", "observed_at": "2020-01-01T00:00:00Z"},
            {"kind": "paper", "ref": "dup", "observed_at": "2022-01-01T00:00:00Z"},
        ]
        result = _coerce_json(
            await conn.fetchval(
                "SELECT dedupe_source_references($1::jsonb)",
                json.dumps(sources),
            )
        )
        assert len(result) == 1
        assert result[0]["ref"] == "dup"
        assert result[0]["observed_at"].startswith("2022-01-01")


async def test_source_reinforcement_score_behaves_monotonic(db_pool):
    async with db_pool.acquire() as conn:
        empty_score = await conn.fetchval("SELECT source_reinforcement_score('[]'::jsonb)")
        assert abs(float(empty_score)) < 0.0001

        one_score = await conn.fetchval(
            "SELECT source_reinforcement_score($1::jsonb)",
            json.dumps([{"ref": "a", "trust": 0.9}]),
        )
        two_score = await conn.fetchval(
            "SELECT source_reinforcement_score($1::jsonb)",
            json.dumps([{"ref": "a", "trust": 0.9}, {"ref": "b", "trust": 0.9}]),
        )
        assert float(two_score) > float(one_score)


async def test_compute_semantic_trust_respects_alignment(db_pool):
    async with db_pool.acquire() as conn:
        sources = [{"ref": "a", "trust": 0.9}, {"ref": "b", "trust": 0.8}]
        neutral = await conn.fetchval(
            "SELECT compute_semantic_trust(0.9, $1::jsonb, 0.0)",
            json.dumps(sources),
        )
        positive = await conn.fetchval(
            "SELECT compute_semantic_trust(0.9, $1::jsonb, 0.5)",
            json.dumps(sources),
        )
        negative = await conn.fetchval(
            "SELECT compute_semantic_trust(0.9, $1::jsonb, -0.5)",
            json.dumps(sources),
        )
        assert float(negative) < float(neutral)
        assert float(positive) >= float(neutral)


async def test_touch_memories_updates_access_fields(db_pool):
    async with db_pool.acquire() as conn:
        memory_id = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding, access_count)
            VALUES ('semantic'::memory_type, $1, array_fill(0.1, ARRAY[embedding_dimension()])::vector, 0)
            RETURNING id
            """,
            f"Touch {get_test_identifier('touch_memories')}",
        )
        updated = await conn.fetchval(
            "SELECT touch_memories($1::uuid[])",
            [memory_id, uuid.uuid4()],
        )
        assert updated == 1

        row = await conn.fetchrow(
            "SELECT access_count, last_accessed FROM memories WHERE id = $1",
            memory_id,
        )
        assert row["access_count"] == 1
        assert row["last_accessed"] is not None


async def test_find_episode_memories_graph_orders_sequence(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("LOAD 'age';")
            episode_id = await _create_episode(
                conn,
                started_at=datetime.now(timezone.utc) + timedelta(days=3650),
                summary="Episode summary",
                episode_type="chat",
            )
            first_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('episodic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Episode mem one {get_test_identifier('episode_mem_one')}",
            )
            second_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('episodic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Episode mem two {get_test_identifier('episode_mem_two')}",
            )
            rows = await conn.fetch("SELECT * FROM find_episode_memories_graph($1)", episode_id)
            assert [row["memory_id"] for row in rows] == [first_id, second_id]
            assert [row["sequence_order"] for row in rows] == [1, 2]
        finally:
            await tr.rollback()


async def test_get_episode_details_returns_metadata(db_pool):
    async with db_pool.acquire() as conn:
        episode_id = await _create_episode(
            conn,
            summary="Episode details",
            episode_type="heartbeat",
        )
        row = await conn.fetchrow("SELECT * FROM get_episode_details($1)", episode_id)
        assert row["id"] == episode_id
        assert row["episode_type"] == "heartbeat"
        assert row["summary"] == "Episode details"


async def test_get_episode_memories_returns_sequence(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("LOAD 'age';")
            episode_id = await _create_episode(
                conn,
                started_at=datetime.now(timezone.utc) + timedelta(days=3650),
                summary="Memories",
                episode_type="chat",
            )
            mem_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('episodic'::memory_type, $1, array_fill(0.3, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Episode mem {get_test_identifier('episode_mem_single')}",
            )
            rows = await conn.fetch("SELECT * FROM get_episode_memories($1)", episode_id)
            assert len(rows) == 1
            assert rows[0]["memory_id"] == mem_id
            assert rows[0]["sequence_order"] == 1
        finally:
            await tr.rollback()


async def test_list_recent_episodes_includes_memory_count(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("LOAD 'age';")
            episode_id = await _create_episode(
                conn,
                started_at=datetime.now(timezone.utc) + timedelta(days=3650),
                summary="Recent episode",
                episode_type="chat",
            )
            mem_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('episodic'::memory_type, $1, array_fill(0.4, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Episode mem {get_test_identifier('episode_mem_recent')}",
            )
            rows = await conn.fetch("SELECT * FROM list_recent_episodes(1)")
            assert rows[0]["id"] == episode_id
            assert rows[0]["memory_count"] == 1
        finally:
            await tr.rollback()


async def test_search_clusters_by_query_orders_by_similarity(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            cluster_a = await conn.fetchval(
                """
                INSERT INTO clusters (cluster_type, name, centroid_embedding)
                VALUES ('theme'::cluster_type, 'Alpha Cluster', get_embedding($1))
                RETURNING id
                """,
                "alpha topic",
            )
            cluster_b = await conn.fetchval(
                """
                INSERT INTO clusters (cluster_type, name, centroid_embedding)
                VALUES ('theme'::cluster_type, 'Beta Cluster', get_embedding($1))
                RETURNING id
                """,
                "beta topic",
            )

            rows = await conn.fetch("SELECT * FROM search_clusters_by_query($1, 2)", "alpha topic")
            assert len(rows) == 2
            assert rows[0]["id"] == cluster_a
            assert rows[0]["similarity"] >= rows[1]["similarity"]
        finally:
            await tr.rollback()


async def test_get_cluster_sample_memories_orders_by_strength(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("LOAD 'age';")
            cluster_id = await conn.fetchval(
                """
                INSERT INTO clusters (cluster_type, name, centroid_embedding)
                VALUES ('theme'::cluster_type, $1, array_fill(0.1, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Cluster {get_test_identifier('cluster_sample')}",
            )
            strong_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Strong {get_test_identifier('cluster_strong')}",
            )
            weak_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Weak {get_test_identifier('cluster_weak')}",
            )
            await conn.fetchval("SELECT link_memory_to_cluster_graph($1, $2, 0.9)", strong_id, cluster_id)
            await conn.fetchval("SELECT link_memory_to_cluster_graph($1, $2, 0.2)", weak_id, cluster_id)

            rows = await conn.fetch("SELECT * FROM get_cluster_sample_memories($1, 2)", cluster_id)
            assert [row["memory_id"] for row in rows] == [strong_id, weak_id]
            assert rows[0]["membership_strength"] >= rows[1]["membership_strength"]
        finally:
            await tr.rollback()


async def test_find_related_concepts_for_memories_returns_counts(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("LOAD 'age';")
            mem_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Concept mem {get_test_identifier('concept_mem')}",
            )
            await conn.fetchval("SELECT create_concept($1)", "focus")
            await conn.fetchval("SELECT sync_memory_node($1)", mem_id)
            await conn.fetchval("SELECT link_memory_to_concept($1, $2, 0.8)", mem_id, "focus")

            rows = await conn.fetch(
                "SELECT * FROM find_related_concepts_for_memories($1::uuid[], $2, 5)",
                [mem_id],
                "",
            )
            assert any(row["name"] == "focus" for row in rows)
        finally:
            await tr.rollback()


async def test_search_procedural_memories_returns_success_rate(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        mem_id = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding, metadata)
            VALUES (
                'procedural'::memory_type,
                $1,
                get_embedding($2),
                jsonb_build_object(
                    'steps', jsonb_build_array('step1', 'step2'),
                    'prerequisites', jsonb_build_object('tool', 'hammer'),
                    'success_count', 4,
                    'total_attempts', 5,
                    'average_duration_seconds', 30
                )
            )
            RETURNING id
            """,
            f"Procedural {get_test_identifier('procedural_search')}",
            "Build a shelf",
        )

        rows = await conn.fetch("SELECT * FROM search_procedural_memories($1, 3)", "Build a shelf")
        assert any(row["memory_id"] == mem_id for row in rows)
        for row in rows:
            if row["memory_id"] == mem_id:
                assert abs(float(row["success_rate"]) - 0.8) < 0.01


async def test_search_strategic_memories_returns_pattern(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        mem_id = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding, metadata)
            VALUES (
                'strategic'::memory_type,
                $1,
                get_embedding($2),
                jsonb_build_object(
                    'pattern_description', 'Use spaced repetition',
                    'confidence_score', 0.7,
                    'context_applicability', jsonb_build_object('domain', 'learning')
                )
            )
            RETURNING id
            """,
            f"Strategic {get_test_identifier('strategic_search')}",
            "Study effectively",
        )

        rows = await conn.fetch("SELECT * FROM search_strategic_memories($1, 3)", "Study effectively")
        assert any(row["memory_id"] == mem_id for row in rows)
        for row in rows:
            if row["memory_id"] == mem_id:
                assert row["pattern_description"] == "Use spaced repetition"


async def test_update_memory_timestamp_trigger(db_pool):
    """Test the update_memory_timestamp trigger"""
    async with db_pool.acquire() as conn:
        # Create test memory
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding
            ) VALUES (
                'semantic'::memory_type,
                'Test timestamp update',
                array_fill(0, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)

        # Get initial timestamp
        initial_updated_at = await conn.fetchval("""
            SELECT updated_at FROM memories WHERE id = $1
        """, memory_id)

        # Wait briefly
        await asyncio.sleep(0.1)

        # Update memory
        await conn.execute("""
            UPDATE memories 
            SET content = 'Updated content'
            WHERE id = $1
        """, memory_id)

        # Get new timestamp
        new_updated_at = await conn.fetchval("""
            SELECT updated_at FROM memories WHERE id = $1
        """, memory_id)

        assert new_updated_at > initial_updated_at, "updated_at should be newer"

async def test_update_memory_importance_trigger(db_pool):
    """Test the update_memory_importance trigger"""
    async with db_pool.acquire() as conn:
        # Create test memory
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                access_count
            ) VALUES (
                'semantic'::memory_type,
                'Test importance update',
                array_fill(0, ARRAY[embedding_dimension()])::vector,
                0.5,
                0
            ) RETURNING id
        """)

        # Get initial importance
        initial_importance = await conn.fetchval("""
            SELECT importance FROM memories WHERE id = $1
        """, memory_id)

        # Update access count
        await conn.execute("""
            UPDATE memories 
            SET access_count = access_count + 1
            WHERE id = $1
        """, memory_id)

        # Get new importance
        new_importance = await conn.fetchval("""
            SELECT importance FROM memories WHERE id = $1
        """, memory_id)

        assert new_importance > initial_importance, "Importance should increase"
        
        # Test multiple accesses
        await conn.execute("""
            UPDATE memories 
            SET access_count = access_count + 5
            WHERE id = $1
        """, memory_id)
        
        final_importance = await conn.fetchval("""
            SELECT importance FROM memories WHERE id = $1
        """, memory_id)
        
        assert final_importance > new_importance, "Importance should increase with more accesses"

async def test_create_memory_relationship_function(db_pool):
    """Test the create_memory_relationship function"""
    async with db_pool.acquire() as conn:
        # Create two test memories
        memory_ids = []
        for i in range(2):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding
                ) VALUES (
                    'semantic'::memory_type,
                    'Test memory ' || $1::text,
                    array_fill(0, ARRAY[embedding_dimension()])::vector
                ) RETURNING id
            """, str(i))
            memory_ids.append(memory_id)

        # Ensure AGE graph/label exist without dropping the graph.
        await conn.execute("""
            LOAD 'age';
            SET search_path = ag_catalog, public;
            DO $$
            BEGIN
                IF NOT EXISTS (SELECT 1 FROM ag_catalog.ag_graph WHERE name = 'memory_graph') THEN
                    PERFORM create_graph('memory_graph');
                END IF;
                BEGIN
                    PERFORM create_vlabel('memory_graph', 'MemoryNode');
                EXCEPTION WHEN duplicate_object OR invalid_schema_name THEN
                    NULL;
                END;
            END;
            $$;
        """)

        # Create nodes in graph using string formatting for Cypher
        for memory_id in memory_ids:
            cypher_query = f"""
                SELECT * FROM ag_catalog.cypher('memory_graph', $$
                    CREATE (n:MemoryNode {{
                        memory_id: '{str(memory_id)}',
                        type: 'semantic'
                    }})
                    RETURN n
                $$) as (result agtype)
            """
            await conn.execute(cypher_query)

        properties = {"weight": 0.8}

        # Create relationship using valid graph_edge_type enum value
        await conn.execute("""
            SELECT create_memory_relationship($1, $2, $3::graph_edge_type, $4)
        """, memory_ids[0], memory_ids[1], 'ASSOCIATED', json.dumps(properties))

        # Verify relationship exists
        verify_query = f"""
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (a:MemoryNode)-[r:ASSOCIATED]->(b:MemoryNode)
                WHERE a.memory_id = '{str(memory_ids[0])}' AND b.memory_id = '{str(memory_ids[1])}'
                RETURN r
            $$) as (result agtype)
        """
        result = await conn.fetch(verify_query)
        assert len(result) > 0, "Relationship not created"

async def test_memory_health_view(db_pool):
    """Test the memory_health view"""
    async with db_pool.acquire() as conn:
        # Create test memories of different types
        memory_types = ['episodic', 'semantic', 'procedural', 'strategic']
        for mem_type in memory_types:
            await conn.execute("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    access_count
                ) VALUES (
                    $1::memory_type,
                    'Test ' || $1,
                    array_fill(0, ARRAY[embedding_dimension()])::vector,
                    0.5,
                    5
                )
            """, mem_type)

        # Query view
        results = await conn.fetch("""
            SELECT * FROM memory_health
        """)

        assert len(results) > 0, "Memory health view should return results"
        
        # Verify each type has stats
        result_types = {r['type'] for r in results}
        for mem_type in memory_types:
            assert mem_type in result_types, f"Missing stats for {mem_type}"
            
        # Verify computed values
        for row in results:
            assert row['total_memories'] > 0, "Should have memories"
            assert row['avg_importance'] is not None, "Should have importance"
            assert row['avg_access_count'] is not None, "Should have access count"



async def test_memory_tables_and_columns(db_pool):
    """Test that all memory tables exist with correct columns and constraints"""
    async with db_pool.acquire() as conn:
        # First check if tables exist
        tables = await conn.fetch("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
        """)
        table_names = {t['table_name'] for t in tables}

        assert 'working_memory' in table_names, "working_memory table not found"
        assert 'memories' in table_names, "memories table not found"
        # Note: episodic_memories, semantic_memories etc have been collapsed into memories.metadata
        assert 'clusters' in table_names, "clusters table not found"
        # Note: memory_cluster_members removed in Phase 3 (ReduceScopeCreep) - now graph edges (MEMBER_OF)
        # Note: cluster_relationships removed in Phase 3 (ReduceScopeCreep) - now in graph

        # Then check columns
        memories = await conn.fetch("""
            SELECT column_name, data_type, is_nullable
            FROM information_schema.columns
            WHERE table_name = 'memories'
        """)
        columns = {col["column_name"]: col for col in memories}

        assert "last_accessed" in columns, "last_accessed column not found"
        assert "id" in columns and columns["id"]["data_type"] == "uuid"
        assert "content" in columns and columns["content"]["is_nullable"] == "NO"
        assert "embedding" in columns
        assert "type" in columns
        assert "metadata" in columns, "metadata column not found"

async def test_clusters(db_pool):
    """Test memory clustering functionality"""
    async with db_pool.acquire() as conn:
        # Load AGE for this connection
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        # Create test cluster
        cluster_id = await conn.fetchval(
            """
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Test Theme Cluster',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
            """
        )
        
        assert cluster_id is not None, "Failed to create cluster"
        
        # Create test memories and add to cluster via graph
        # Phase 3 (ReduceScopeCreep): memory_cluster_members table removed, uses graph edges
        memory_ids = []
        strengths = [0.8, 0.7, 0.6]
        for i in range(3):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding
                ) VALUES (
                    'semantic'::memory_type,
                    'Test memory for clustering ' || $1,
                    array_fill($2::float, ARRAY[embedding_dimension()])::vector
                ) RETURNING id
            """, str(i), float(i) * 0.1)
            memory_ids.append(memory_id)

            # Sync memory to graph and add to cluster via graph edge
            await conn.execute("SELECT sync_memory_node($1)", memory_id)
            await conn.execute(
                "SELECT link_memory_to_cluster_graph($1, $2, $3)",
                memory_id, cluster_id, strengths[i]
            )

        # Verify cluster membership via graph
        members = await conn.fetch("""
            SELECT * FROM get_cluster_members_graph($1)
            ORDER BY membership_strength DESC
        """, cluster_id)

        assert len(members) == 3, "Wrong number of cluster members"
        assert members[0]['membership_strength'] == 0.8, "Incorrect membership strength"

async def test_cluster_relationships(db_pool):
    """Test relationships between clusters (Phase 3: uses graph instead of cluster_relationships table)"""
    async with db_pool.acquire() as conn:
        # Create two clusters
        cluster_ids = []
        for i, name in enumerate(['Loneliness', 'Connection']):
            cluster_id = await conn.fetchval(
                """
                INSERT INTO clusters (
                    cluster_type,
                    name,
                    centroid_embedding
                ) VALUES (
                    'emotion'::cluster_type,
                    $1,
                    array_fill($2::float, ARRAY[embedding_dimension()])::vector
                ) RETURNING id
                """,
                name,
                float(i) * 0.5,
            )
            cluster_ids.append(cluster_id)

        # Phase 3: Create relationship between clusters via graph
        result = await conn.fetchval("""
            SELECT link_cluster_relationship($1, $2, 'relates', 0.7)
        """, cluster_ids[0], cluster_ids[1])

        assert result is True, "Cluster relationship should be created in graph"

        # Verify relationship via graph query
        related = await conn.fetch("""
            SELECT * FROM find_related_clusters($1)
        """, cluster_ids[0])

        assert len(related) >= 1, "Should find related cluster"
        # Find the relationship we just created
        found = any(str(r['related_cluster_id']) == str(cluster_ids[1]) for r in related)
        assert found, "Should find the linked cluster"

async def test_cluster_activation_history(db_pool):
    """Test cluster schema after simplification (activation tracking removed)."""
    async with db_pool.acquire() as conn:
        columns = await conn.fetch(
            """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name = 'clusters'
            """
        )
        column_names = {row["column_name"] for row in columns}

        for required in {"id", "created_at", "updated_at", "cluster_type", "name", "centroid_embedding"}:
            assert required in column_names

        for removed in {"activation_count", "last_activated", "importance_score", "coherence_score", "keywords", "emotional_signature"}:
            assert removed not in column_names

async def test_identity_core_clusters(db_pool):
    """Test identity-related memories can be linked to clusters via graph."""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("identity_clusters")
        cluster_id = await conn.fetchval(
            """
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                $1,
                array_fill(0.8, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
            """,
            f"Self-as-Helper {test_id}",
        )

        worldview_id = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'self', 0.8, 0.7, 0.8, 'test')",
            f"I help others {test_id}",
        )
        await conn.execute("SELECT link_memory_to_cluster_graph($1, $2, 0.9)", worldview_id, cluster_id)

        members = await conn.fetch("SELECT memory_id FROM get_cluster_members_graph($1)", cluster_id)
        assert any(row["memory_id"] == worldview_id for row in members), "Self memory should be in cluster graph"

async def test_assign_memory_to_clusters_function(db_pool):
    """Test the assign_memory_to_clusters function
    
    Note: This test requires the updated db/*.sql to be applied to the database.
    The assign_memory_to_clusters function was updated to use 
    'WHERE centroid_embedding IS NOT NULL' instead of 'WHERE status = 'active''
    """
    async with db_pool.acquire() as conn:
        # Create test clusters with different centroids
        cluster_ids = []
        for i in range(3):
            # Create distinct centroid embeddings
            centroid = [0.0] * EMBEDDING_DIMENSION
            centroid[i*100:(i+1)*100] = [1.0] * 100  # Make each cluster distinct
            
            cluster_id = await conn.fetchval("""
                INSERT INTO clusters (
                    cluster_type,
                    name,
                    centroid_embedding
                ) VALUES (
                    'theme'::cluster_type,
                    'Test Cluster ' || $1,
                    $2::vector
                ) RETURNING id
            """, str(i), str(centroid))
            cluster_ids.append(cluster_id)
        
        # Create memory with embedding similar to first cluster
        memory_embedding = [0.0] * EMBEDDING_DIMENSION
        memory_embedding[0:100] = [0.9] * 100  # Similar to first cluster
        
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding
            ) VALUES (
                'semantic'::memory_type,
                'Test memory for auto-clustering',
                $1::vector
            ) RETURNING id
        """, str(memory_embedding))
        
        # Assign to clusters (uses graph edges via Phase 3)
        await conn.execute("""
            SELECT assign_memory_to_clusters($1, 2)
        """, memory_id)

        # Phase 3 (ReduceScopeCreep): Query graph for cluster memberships instead of table
        # The assign_memory_to_clusters function now creates MEMBER_OF edges in graph
        # We verify by checking if the memory is linked to any cluster via graph
        memberships = await conn.fetch("""
            SELECT mc.id as cluster_id, gcm.membership_strength
            FROM clusters mc
            JOIN get_cluster_members_graph(mc.id) gcm ON TRUE
            WHERE gcm.memory_id = $1
            ORDER BY gcm.membership_strength DESC
        """, memory_id)

        assert len(memberships) > 0, "Memory not assigned to any clusters"
        assert memberships[0]['membership_strength'] >= 0.7, "Expected high similarity"

async def test_recalculate_cluster_centroid_function(db_pool):
    """Test the recalculate_cluster_centroid function"""
    async with db_pool.acquire() as conn:
        # Create cluster
        cluster_id = await conn.fetchval("""
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Test Centroid Cluster',
                array_fill(0.0, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)
        
        # Add memories with different embeddings
        # Phase 3 (ReduceScopeCreep): Use graph edges instead of memory_cluster_members table
        for i in range(3):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    status
                ) VALUES (
                    'semantic'::memory_type,
                    'Memory ' || $1,
                    array_fill($2::float, ARRAY[embedding_dimension()])::vector,
                    'active'::memory_status
                ) RETURNING id
            """, str(i), float(i+1) * 0.2)

            # Sync to graph and create MEMBER_OF edge
            await conn.execute("SELECT sync_memory_node($1)", memory_id)
            await conn.execute(
                "SELECT link_memory_to_cluster_graph($1, $2, $3)",
                memory_id, cluster_id, 0.8
            )
        
        # Recalculate centroid
        await conn.execute("""
            SELECT recalculate_cluster_centroid($1)
        """, cluster_id)
        
        # Check if centroid was updated
        result = await conn.fetchrow("""
            SELECT (vector_to_float4(centroid_embedding, embedding_dimension(), false))[1] as first_value
            FROM clusters
            WHERE id = $1
        """, cluster_id)
        
        # The average of 0.2, 0.4, 0.6 should be 0.4
        assert result['first_value'] is not None, "Centroid not updated"

async def test_cluster_insights_view(db_pool):
    """Test the cluster_insights view"""
    async with db_pool.acquire() as conn:
        # Create cluster with members using unique name
        import time
        unique_name = f'Insight Test Cluster {get_test_identifier("insight_cluster")}'
        cluster_id = await conn.fetchval(
            """
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                $1,
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
            """,
            unique_name,
        )
        
        # Add memories using graph edges (Phase 3: memory_cluster_members removed)
        for i in range(5):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding
                ) VALUES (
                    'episodic'::memory_type,
                    'Insight memory ' || $1,
                    array_fill(0.5, ARRAY[embedding_dimension()])::vector
                ) RETURNING id
            """, str(i))

            # Sync to graph and create MEMBER_OF edge
            await conn.execute("SELECT sync_memory_node($1)", memory_id)
            await conn.execute(
                "SELECT link_memory_to_cluster_graph($1, $2, $3)",
                memory_id, cluster_id, 1.0
            )
        
        # Query view
        insights = await conn.fetch("""
            SELECT * FROM cluster_insights
            WHERE name = $1
        """, unique_name)
        
        assert len(insights) == 1
        assert insights[0]['memory_count'] == 5

async def test_active_themes_view(db_pool):
    """Test cluster_insights view returns clusters without members."""
    async with db_pool.acquire() as conn:
        unique_name = f'Recent Anxiety {get_test_identifier("recent_anxiety")}'
        cluster_id = await conn.fetchval(
            """
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'emotion'::cluster_type,
                $1,
                array_fill(0.3, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
            """,
            unique_name,
        )

        themes = await conn.fetch(
            """
            SELECT * FROM cluster_insights
            WHERE id = $1
            """,
            cluster_id,
        )

        assert len(themes) > 0
        assert themes[0]['memory_count'] == 0

async def test_update_cluster_activation_trigger(db_pool):
    """Test that cluster rows update without activation tracking."""
    async with db_pool.acquire() as conn:
        unique_name = f'Activation Test {get_test_identifier("activation_test")}'
        cluster_id = await conn.fetchval(
            """
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                $1,
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
            """,
            unique_name,
        )

        await conn.execute(
            """
            UPDATE clusters
            SET name = name
            WHERE id = $1
            """,
            cluster_id,
        )

        updated = await conn.fetchrow(
            """
            SELECT id, name
            FROM clusters
            WHERE id = $1
            """,
            cluster_id,
        )
        assert updated is not None

async def test_cluster_types(db_pool):
    """Test all cluster types"""
    async with db_pool.acquire() as conn:
        cluster_types = ['theme', 'emotion', 'temporal', 'person', 'pattern', 'mixed']
        
        for c_type in cluster_types:
            cluster_id = await conn.fetchval("""
                INSERT INTO clusters (
                    cluster_type,
                    name,
                    centroid_embedding
                ) VALUES (
                    $1::cluster_type,
                    'Test ' || $1 || ' cluster',
                    array_fill(0.5, ARRAY[embedding_dimension()])::vector
                ) RETURNING id
            """, c_type)
            
            assert cluster_id is not None, f"Failed to create {c_type} cluster"
        
        # Verify all types exist
        count = await conn.fetchval("""
            SELECT COUNT(DISTINCT cluster_type)
            FROM clusters
        """)
        
        assert count >= len(cluster_types)

async def test_cluster_memory_retrieval_performance(db_pool):
    """Test performance of cluster-based memory retrieval"""
    async with db_pool.acquire() as conn:
        # Create cluster
        cluster_id = await conn.fetchval(
            """
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Loneliness',
                array_fill(0.3, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
            """
        )
        
        # Add many memories to cluster using graph edges (Phase 3)
        memory_ids = []
        for i in range(50):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance
                ) VALUES (
                    'episodic'::memory_type,
                    'Loneliness memory ' || $1,
                    array_fill(0.3, ARRAY[embedding_dimension()])::vector,
                    $2
                ) RETURNING id
            """, str(i), 0.5 + (i * 0.01))

            # Sync to graph and create MEMBER_OF edge
            await conn.execute("SELECT sync_memory_node($1)", memory_id)
            await conn.execute(
                "SELECT link_memory_to_cluster_graph($1, $2, $3)",
                memory_id, cluster_id, 0.7 + (i * 0.001)
            )

            memory_ids.append(memory_id)

        # Test retrieval by cluster using graph
        import time
        start_time = time.time()

        # Phase 3: Use graph-based retrieval
        results = await conn.fetch("""
            SELECT m.*, gcm.membership_strength
            FROM memories m
            JOIN get_cluster_members_graph($1) gcm ON m.id = gcm.memory_id
            ORDER BY gcm.membership_strength DESC, m.importance DESC
            LIMIT 10
        """, cluster_id)

        retrieval_time = time.time() - start_time

        assert len(results) == 10
        assert retrieval_time < PERF_CLUSTER_RETRIEVAL_SECONDS, (
            f"Cluster retrieval too slow: {retrieval_time}s"
        )

        # Verify ordering
        strengths = [r['membership_strength'] for r in results]
        assert strengths == sorted(strengths, reverse=True)


# HIGH PRIORITY ADDITIONAL TESTS

async def test_constraint_violations(db_pool):
    """Test constraint violations and error handling"""
    async with db_pool.acquire() as conn:
        # Test invalid trust_level constraint (must be between 0 and 1)
        with pytest.raises(asyncpg.PostgresError):
            await conn.execute(
                """
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    trust_level
                ) VALUES (
                    'semantic'::memory_type,
                    'Invalid trust level',
                    array_fill(0, ARRAY[embedding_dimension()])::vector,
                    1.5
                )
                """
            )

        # Test foreign key violation
        with pytest.raises(asyncpg.PostgresError):
            fake_uuid = 'aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee'
            await conn.execute(
                """
                INSERT INTO memory_neighborhoods (memory_id, neighbors)
                VALUES ($1::uuid, '{}'::jsonb)
                """,
                fake_uuid,
            )
        
        # Test invalid vector dimension
        with pytest.raises(asyncpg.PostgresError):
            await conn.execute("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding
                ) VALUES (
                    'semantic'::memory_type,
                    'Test content',
                    array_fill(0, ARRAY[100])::vector
                )
            """)
        
        # Test null constraint violation
        with pytest.raises(asyncpg.PostgresError):
            await conn.execute("""
                INSERT INTO memories (
                    type,
                    embedding
                ) VALUES (
                    'semantic'::memory_type,
                    array_fill(0, ARRAY[embedding_dimension()])::vector
                )
            """)


async def test_memory_consolidation_workflow(db_pool):
    """Test complete memory consolidation from working memory to long-term storage"""
    async with db_pool.acquire() as conn:
        # Step 1: Create working memory entries
        working_memories = []
        for i in range(5):
            wm_id = await conn.fetchval("""
                INSERT INTO working_memory (
                    content,
                    embedding,
                    expiry
                ) VALUES (
                    'Working memory content ' || $1,
                    array_fill($2::float, ARRAY[embedding_dimension()])::vector,
                    CURRENT_TIMESTAMP + interval '1 hour'
                ) RETURNING id
            """, str(i), float(i) * 0.1)
            working_memories.append(wm_id)
        
        # Step 2: Simulate consolidation process
        consolidated_memories = []
        for wm_id in working_memories:
            # Get working memory content
            wm_data = await conn.fetchrow("""
                SELECT content, embedding FROM working_memory WHERE id = $1
            """, wm_id)
            
            # Create long-term memory
            ltm_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    metadata
                ) VALUES (
                    'episodic'::memory_type,
                    'Consolidated: ' || $1,
                    $2,
                    0.7,
                    jsonb_build_object(
                        'action_taken', '{"action": "consolidation"}'::jsonb,
                        'context', '{"source": "working_memory"}'::jsonb,
                        'result', '{"status": "consolidated"}'::jsonb,
                        'emotional_valence', 0.0
                    )
                ) RETURNING id
            """, wm_data['content'], wm_data['embedding'])
            
            consolidated_memories.append(ltm_id)
            
            # Remove from working memory
            await conn.execute("""
                DELETE FROM working_memory WHERE id = $1
            """, wm_id)
        
        # Step 3: Verify consolidation
        # Check working memory is empty
        wm_count = await conn.fetchval("""
            SELECT COUNT(*) FROM working_memory
            WHERE id = ANY($1::uuid[])
        """, working_memories)
        assert wm_count == 0, "Working memory not properly cleared"
        
        # Check long-term memories exist
        ltm_count = await conn.fetchval("""
            SELECT COUNT(*) FROM memories
            WHERE id = ANY($1::uuid[])
        """, consolidated_memories)
        assert ltm_count == 5, "Not all memories consolidated"
        
        # Step 4: Test memory clustering after consolidation
        for memory_id in consolidated_memories:
            await conn.execute("""
                SELECT assign_memory_to_clusters($1, 2)
            """, memory_id)
        
        # Verify cluster assignments via graph (Phase 3)
        cluster_assignments = await conn.fetchval("""
            SELECT COUNT(*)
            FROM clusters mc
            JOIN get_cluster_members_graph(mc.id) gcm ON TRUE
            WHERE gcm.memory_id = ANY($1::uuid[])
        """, consolidated_memories)
        assert cluster_assignments > 0, "Memories not assigned to clusters"


async def test_large_dataset_performance(db_pool):
    """Test system performance with large datasets"""
    async with db_pool.acquire() as conn:
        import time
        tr = conn.transaction()
        await tr.start()
        try:
            # Create large number of memories (1000 for testing, would be 10K+ in production)
            total_memories = LARGE_DATASET_SIZE
            memory_types = ["episodic", "semantic", "procedural", "strategic"]
            type_values: list[str] = []
            content_values: list[str] = []
            embedding_values: list[str] = []
            importance_values: list[float] = []

            print(f"Creating {total_memories} memories in a single batch...")

            for i in range(total_memories):
                embedding = [0.0] * EMBEDDING_DIMENSION
                pattern_start = (i % 10) * 150
                pattern_end = min(pattern_start + 150, EMBEDDING_DIMENSION)
                embedding[pattern_start:pattern_end] = [0.8] * (pattern_end - pattern_start)

                type_values.append(memory_types[i % 4])
                content_values.append(f"Large dataset memory {i}")
                embedding_values.append(str(embedding))
                importance_values.append(0.1 + (i % 100) * 0.01)

            await conn.execute(
                """
                INSERT INTO memories (type, content, embedding, importance)
                SELECT t::memory_type, c, e::vector, i
                FROM unnest($1::text[], $2::text[], $3::text[], $4::float[]) AS x(t, c, e, i)
                """,
                type_values,
                content_values,
                embedding_values,
                importance_values,
            )

            print(f"Created {total_memories} memories")
            
            # Test 1: Vector similarity search performance
            head_len = min(150, EMBEDDING_DIMENSION)
            query_embedding = [0.8] * head_len + [0.0] * (EMBEDDING_DIMENSION - head_len)
            
            start_time = time.time()
            similar_memories = await conn.fetch("""
                SELECT id, content, embedding <=> $1::vector as distance
                FROM memories
                ORDER BY embedding <=> $1::vector
                LIMIT 50
            """, str(query_embedding))
            vector_search_time = time.time() - start_time
            
            assert len(similar_memories) == 50
            assert vector_search_time < PERF_VECTOR_SEARCH_SECONDS, (
                f"Vector search too slow: {vector_search_time}s"
            )
            print(f"Vector search time: {vector_search_time:.3f}s")
            
            # Test 2: Complex query performance
            start_time = time.time()
            complex_results = await conn.fetch("""
                SELECT m.type, COUNT(*) as count, AVG(m.importance) as avg_importance
                FROM memories m
                WHERE m.status = 'active'
                AND m.importance > 0.5
                GROUP BY m.type
                ORDER BY avg_importance DESC
            """)
            complex_query_time = time.time() - start_time
            
            assert len(complex_results) > 0
            assert complex_query_time < PERF_COMPLEX_QUERY_SECONDS, (
                f"Complex query too slow: {complex_query_time}s"
            )
            print(f"Complex query time: {complex_query_time:.3f}s")
            
            # Test 3: Memory health view performance
            start_time = time.time()
            health_stats = await conn.fetch("""
                SELECT * FROM memory_health
            """)
            view_query_time = time.time() - start_time
            
            assert len(health_stats) > 0
            assert view_query_time < PERF_VIEW_QUERY_SECONDS, (
                f"View query too slow: {view_query_time}s"
            )
            print(f"View query time: {view_query_time:.3f}s")
        finally:
            await tr.rollback()


async def test_concurrency_safety(db_pool):
    """Test concurrent operations for race conditions"""
    async with db_pool.acquire() as conn:
        # Create test memory for concurrent updates
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                access_count
            ) VALUES (
                'semantic'::memory_type,
                'Concurrency test memory',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                0.5,
                0
            ) RETURNING id
        """)
        
        # Test concurrent access count updates
        async def update_access_count(pool, mem_id, increment):
            async with pool.acquire() as connection:
                for _ in range(increment):
                    await connection.execute("""
                        UPDATE memories 
                        SET access_count = access_count + 1
                        WHERE id = $1
                    """, mem_id)
        
        # Run concurrent updates
        import asyncio
        tasks = [
            update_access_count(db_pool, memory_id, 10),
            update_access_count(db_pool, memory_id, 10),
            update_access_count(db_pool, memory_id, 10)
        ]
        
        await asyncio.gather(*tasks)
        
        # Verify final access count
        final_count = await conn.fetchval("""
            SELECT access_count FROM memories WHERE id = $1
        """, memory_id)
        
        assert final_count == 30, f"Expected 30 accesses, got {final_count}"
        
        # Test concurrent cluster assignments
        cluster_id = await conn.fetchval("""
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Concurrency Test Cluster',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)
        
        # Create multiple memories for concurrent cluster assignment
        test_memories = []
        for i in range(5):
            mem_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding
                ) VALUES (
                    'semantic'::memory_type,
                    'Concurrent memory ' || $1,
                    array_fill(0.5, ARRAY[embedding_dimension()])::vector
                ) RETURNING id
            """, str(i))
            test_memories.append(mem_id)
        
        # Concurrent cluster assignments using graph edges (Phase 3)
        async def assign_to_cluster(pool, mem_id, clust_id):
            async with pool.acquire() as connection:
                try:
                    # Load AGE for this connection
                    await connection.execute("LOAD 'age';")
                    await connection.execute("SET search_path = ag_catalog, public;")
                    # Sync memory to graph and create MEMBER_OF edge
                    await connection.execute("SELECT sync_memory_node($1)", mem_id)
                    await connection.execute(
                        "SELECT link_memory_to_cluster_graph($1, $2, $3)",
                        mem_id, clust_id, 0.8
                    )
                except Exception as e:
                    # Expected for some concurrent operations
                    pass

        assignment_tasks = [
            assign_to_cluster(db_pool, mem_id, cluster_id)
            for mem_id in test_memories
        ]

        await asyncio.gather(*assignment_tasks)

        # Verify assignments via graph
        # Note: With concurrent graph operations, some MERGE operations may fail due to race conditions
        # This is expected behavior - the important thing is no corrupt data is created
        assignment_count = await conn.fetchval("""
            SELECT COUNT(*) FROM get_cluster_members_graph($1)
        """, cluster_id)

        assert assignment_count >= 4, f"Expected at least 4 assignments, got {assignment_count}"


async def test_cascade_delete_integrity(db_pool):
    """Test referential integrity with cascade deletes"""
    async with db_pool.acquire() as conn:
        # Create memory with all related data (using metadata for type-specific fields)
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                metadata
            ) VALUES (
                'episodic'::memory_type,
                'Test cascade delete',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                jsonb_build_object(
                    'action_taken', '{"action": "test"}'::jsonb,
                    'context', '{"context": "test"}'::jsonb,
                    'result', '{"result": "test"}'::jsonb
                )
            ) RETURNING id
        """)
        
        # Add to cluster
        cluster_id = await conn.fetchval("""
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Cascade Test Cluster',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)
        
        # Phase 3 (ReduceScopeCreep): Use graph edges instead of memory_cluster_members
        await conn.execute("SELECT sync_memory_node($1)", memory_id)
        await conn.execute(
            "SELECT link_memory_to_cluster_graph($1, $2, $3)",
            memory_id, cluster_id, 1.0
        )

        # Note: memory_changes table removed in Phase 8 (ReduceScopeCreep)

        # Verify all related data exists
        cluster_member_count = await conn.fetchval("""
            SELECT COUNT(*) FROM get_cluster_members_graph($1) WHERE memory_id = $2
        """, cluster_id, memory_id)
        memory_exists = await conn.fetchval("""
            SELECT COUNT(*) FROM memories WHERE id = $1
        """, memory_id)

        assert memory_exists == 1
        assert cluster_member_count == 1

        # Delete the memory - graph edges should be removed when we DETACH DELETE the node
        # First delete from graph, then from table
        await conn.execute("""
            SELECT * FROM cypher('memory_graph', $q$
                MATCH (m:MemoryNode {memory_id: '%s'})
                DETACH DELETE m
            $q$) as (result agtype)
        """ % memory_id)
        await conn.execute("""
            DELETE FROM memories WHERE id = $1
        """, memory_id)

        # Verify deletes worked
        cluster_member_count_after = await conn.fetchval("""
            SELECT COUNT(*) FROM get_cluster_members_graph($1) WHERE memory_id = $2
        """, cluster_id, memory_id)
        memory_exists_after = await conn.fetchval("""
            SELECT COUNT(*) FROM memories WHERE id = $1
        """, memory_id)

        # These should all be deleted now
        assert cluster_member_count_after == 0, "Cluster membership not deleted from graph"
        assert memory_exists_after == 0, "Memory not deleted"


async def test_memory_lifecycle_workflow(db_pool):
    """Test complete memory lifecycle from creation to archival"""
    async with db_pool.acquire() as conn:
        # Step 1: Create new memory
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                access_count
            ) VALUES (
                'semantic'::memory_type,
                'Lifecycle test memory',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                0.3,
                0
            ) RETURNING id
        """)
        
        initial_relevance = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score FROM memories WHERE id = $1
        """, memory_id)
        
        # Step 2: Simulate memory access and importance growth
        for i in range(5):
            await conn.execute("""
                UPDATE memories 
                SET access_count = access_count + 1
                WHERE id = $1
            """, memory_id)
        
        mid_relevance = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score FROM memories WHERE id = $1
        """, memory_id)
        
        assert mid_relevance > initial_relevance, "Relevance should increase with access"
        
        # Step 3: Simulate time passage and decay
        await conn.execute("""
            UPDATE memories 
            SET created_at = CURRENT_TIMESTAMP - interval '30 days'
            WHERE id = $1
        """, memory_id)
        
        aged_relevance = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score FROM memories WHERE id = $1
        """, memory_id)
        
        assert aged_relevance < mid_relevance, "Relevance should decrease with age"
        
        # Step 4: Archive low-relevance memory
        await conn.execute("""
            UPDATE memories
            SET status = 'archived'::memory_status
            WHERE id = $1 AND calculate_relevance(importance, decay_rate, created_at, last_accessed) < 0.1
        """, memory_id)
        
        final_status = await conn.fetchval("""
            SELECT status FROM memories WHERE id = $1
        """, memory_id)
        
        # Memory might or might not be archived depending on exact relevance calculation
        assert final_status in ['active', 'archived'], "Memory should have valid status"
        
        # Step 5: Test memory retrieval excludes archived memories
        active_memories = await conn.fetch("""
            SELECT * FROM memories 
            WHERE status = 'active' AND id = $1
        """, memory_id)
        
        if final_status == 'archived':
            assert len(active_memories) == 0, "Archived memory should not appear in active queries"


# CRITICAL MISSING TESTS

async def test_edge_cases_empty_clusters(db_pool):
    """Test edge cases with empty clusters"""
    async with db_pool.acquire() as conn:
        # Create empty cluster
        cluster_id = await conn.fetchval("""
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Empty Test Cluster',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)
        
        # Test recalculating centroid on empty cluster
        await conn.execute("""
            SELECT recalculate_cluster_centroid($1)
        """, cluster_id)
        
        # Verify cluster still exists but centroid might be null
        cluster = await conn.fetchrow("""
            SELECT * FROM clusters WHERE id = $1
        """, cluster_id)
        assert cluster is not None, "Empty cluster should still exist"
        
        # Test cluster insights view with empty cluster
        insights = await conn.fetch("""
            SELECT * FROM cluster_insights WHERE id = $1
        """, cluster_id)
        assert len(insights) == 1, "Empty cluster should appear in insights"
        assert insights[0]['memory_count'] == 0, "Empty cluster should have 0 memories"


async def test_edge_cases_circular_relationships(db_pool):
    """Test edge cases with circular cluster relationships (Phase 3: uses graph)"""
    async with db_pool.acquire() as conn:
        a_id = await conn.fetchval(
            """
            INSERT INTO clusters (cluster_type, name, centroid_embedding)
            VALUES ('theme'::cluster_type, 'Cycle A', array_fill(0.2, ARRAY[embedding_dimension()])::vector)
            RETURNING id
            """
        )
        b_id = await conn.fetchval(
            """
            INSERT INTO clusters (cluster_type, name, centroid_embedding)
            VALUES ('theme'::cluster_type, 'Cycle B', array_fill(0.3, ARRAY[embedding_dimension()])::vector)
            RETURNING id
            """
        )

        await conn.execute("SELECT link_cluster_relationship($1, $2, 'relates', 0.7)", a_id, b_id)
        await conn.execute("SELECT link_cluster_relationship($1, $2, 'relates', 0.7)", b_id, a_id)

        rel_a = await conn.fetch("SELECT related_cluster_id FROM find_related_clusters($1)", a_id)
        rel_b = await conn.fetch("SELECT related_cluster_id FROM find_related_clusters($1)", b_id)
        assert any(row["related_cluster_id"] == b_id for row in rel_a)
        assert any(row["related_cluster_id"] == a_id for row in rel_b)


async def test_edge_cases_extreme_values(db_pool):
    """Test edge cases with extreme values"""
    async with db_pool.acquire() as conn:
        # Test memory with very high importance
        high_importance_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                access_count
            ) VALUES (
                'semantic'::memory_type,
                'Extremely important memory',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                999999.0,
                999999
            ) RETURNING id
        """)
        
        # Test memory with very old timestamp
        old_memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                created_at
            ) VALUES (
                'episodic'::memory_type,
                'Ancient memory',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                0.5,
                '1900-01-01'::timestamp
            ) RETURNING id
        """)
        
        # Test relevance calculation with extreme values
        high_relevance = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score FROM memories WHERE id = $1
        """, high_importance_id)
        
        old_relevance = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score FROM memories WHERE id = $1
        """, old_memory_id)
        
        assert high_relevance > 1000, "High importance should result in high relevance"
        assert old_relevance < 0.01, "Very old memory should have very low relevance"
        
        # Test with zero vectors
        zero_vector_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding
            ) VALUES (
                'semantic'::memory_type,
                'Zero vector memory',
                array_fill(0.0, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)
        
        # Test similarity search with zero vector
        zero_results = await conn.fetch("""
            SELECT id, embedding <=> array_fill(0.0, ARRAY[embedding_dimension()])::vector as distance
            FROM memories
            WHERE id = $1
        """, zero_vector_id)
        
        assert len(zero_results) == 1
        # Zero vectors result in NaN for cosine distance, which is expected behavior
        import math
        assert math.isnan(zero_results[0]['distance']), "Zero vector cosine distance should be NaN"


async def test_data_integrity_orphaned_records(db_pool):
    """Test data integrity with potential orphaned records"""
    async with db_pool.acquire() as conn:
        # Create memory with related data
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                metadata
            ) VALUES (
                'semantic'::memory_type,
                'Test orphan memory',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                jsonb_build_object('confidence', 0.8)
            ) RETURNING id
        """)
        
        # Add to cluster
        cluster_id = await conn.fetchval("""
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Orphan Test Cluster',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)
        
        # Phase 3 (ReduceScopeCreep): Use graph edges instead of memory_cluster_members
        await conn.execute("SELECT sync_memory_node($1)", memory_id)
        await conn.execute(
            "SELECT link_memory_to_cluster_graph($1, $2, $3)",
            memory_id, cluster_id, 1.0
        )

        # With graph-based cluster membership, orphaned records work differently.
        # The graph edges reference nodes that must exist - if a ClusterNode is deleted,
        # the MEMBER_OF edges should be cleaned up by the graph's DETACH DELETE.

        # Verify the cluster membership exists in graph
        member_count = await conn.fetchval("""
            SELECT COUNT(*) FROM get_cluster_members_graph($1)
        """, cluster_id)

        # Should have exactly 1 member
        assert member_count == 1, f"Expected 1 cluster member, got {member_count}"
        
        # Note: With the subtable consolidation into metadata JSONB column,
        # orphaned type-specific records are no longer possible since all
        # type-specific data is stored directly in the memories.metadata column.


async def test_computed_field_accuracy(db_pool):
    """Test accuracy of computed fields"""
    async with db_pool.acquire() as conn:
        # Test procedural memory success rate calculation from metadata
        # With subtable consolidation, success_count and total_attempts are in metadata
        test_cases = [
            (0, 0, 0.0),      # No attempts
            (5, 10, 0.5),     # 50% success
            (10, 10, 1.0),    # 100% success
            (0, 5, 0.0),      # 0% success
            (1, 3, 0.333333)  # 33.33% success
        ]

        for success_count, total_attempts, expected_rate in test_cases:
            metadata = json.dumps({
                "steps": ["test"],
                "success_count": success_count,
                "total_attempts": total_attempts
            })
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    metadata
                ) VALUES (
                    'procedural'::memory_type,
                    'Success rate test',
                    array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                    $1::jsonb
                ) RETURNING id
            """, metadata)

            # Calculate success rate from metadata
            stored_metadata = await conn.fetchval("""
                SELECT metadata FROM memories WHERE id = $1
            """, memory_id)
            if isinstance(stored_metadata, str):
                stored_metadata = json.loads(stored_metadata)

            sc = stored_metadata.get("success_count", 0)
            ta = stored_metadata.get("total_attempts", 0)
            actual_rate = sc / ta if ta > 0 else 0.0

            if expected_rate == 0.333333:
                assert abs(actual_rate - expected_rate) < 0.000001, f"Success rate calculation incorrect: expected {expected_rate}, got {actual_rate}"
            else:
                assert actual_rate == expected_rate, f"Success rate calculation incorrect: expected {expected_rate}, got {actual_rate}"

        # Test relevance score accuracy
        test_memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance,
                decay_rate,
                created_at
            ) VALUES (
                'semantic'::memory_type,
                'Relevance test',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                1.0,
                0.1,
                CURRENT_TIMESTAMP - interval '1 day'
            ) RETURNING id
        """)
        
        relevance = await conn.fetchval("""
            SELECT calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score FROM memories WHERE id = $1
        """, test_memory_id)
        
        # The calculate_relevance function uses a more complex formula:
        # importance * exp(-decay_rate * LEAST(age_in_days, age_of_last_access * 0.5))
        # Since last_accessed is NULL, it uses age_in_days(created_at) * 0.5
        # So for 1 day old: 1.0 * exp(-0.1 * 0.5)  0.9512
        expected_relevance = 1.0 * 2.718281828459045 ** (-0.1 * 0.5)
        assert abs(relevance - expected_relevance) < 0.01, f"Relevance calculation incorrect: expected ~{expected_relevance}, got {relevance}"


async def test_trigger_consistency(db_pool):
    """Test that all triggers fire correctly and consistently"""
    async with db_pool.acquire() as conn:
        # Test memory timestamp trigger
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding
            ) VALUES (
                'semantic'::memory_type,
                'Trigger test memory',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)
        
        initial_updated_at = await conn.fetchval("""
            SELECT updated_at FROM memories WHERE id = $1
        """, memory_id)
        
        # Wait and update
        await asyncio.sleep(0.1)
        await conn.execute("""
            UPDATE memories SET content = 'Updated content' WHERE id = $1
        """, memory_id)
        
        new_updated_at = await conn.fetchval("""
            SELECT updated_at FROM memories WHERE id = $1
        """, memory_id)
        
        assert new_updated_at > initial_updated_at, "Timestamp trigger should fire on update"
        
        # Test importance trigger - set initial importance first
        await conn.execute("""
            UPDATE memories SET importance = 0.5 WHERE id = $1
        """, memory_id)
        
        initial_importance = await conn.fetchval("""
            SELECT importance FROM memories WHERE id = $1
        """, memory_id)
        
        await conn.execute("""
            UPDATE memories SET access_count = access_count + 1 WHERE id = $1
        """, memory_id)
        
        new_importance = await conn.fetchval("""
            SELECT importance FROM memories WHERE id = $1
        """, memory_id)
        
        assert new_importance > initial_importance, "Importance trigger should fire on access count change"
        
        # Cluster activation tracking removed in simplified clusters table.


async def test_view_calculation_accuracy(db_pool):
    """Test accuracy of view calculations"""
    async with db_pool.acquire() as conn:
        # Create test data for memory_health view with unique content to avoid interference
        import time
        unique_suffix = uuid.uuid4().hex[:8]
        test_memories = []
        for i in range(10):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    access_count,
                    last_accessed
                ) VALUES (
                    'semantic'::memory_type,
                    'Health test memory ' || $1 || ' ' || $2,
                    array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                    $3,
                    $4,
                    CASE WHEN $5 THEN CURRENT_TIMESTAMP - interval '12 hours' ELSE NULL END
                ) RETURNING id
            """, str(i), unique_suffix, float(i) * 0.1, i, i % 2 == 0)
            test_memories.append(memory_id)
        
        # Query memory_health view for just our test memories
        health_stats = await conn.fetchrow("""
            SELECT
                type,
                COUNT(*) as total_memories,
                AVG(importance) as avg_importance,
                AVG(access_count) as avg_access_count,
                COUNT(*) FILTER (WHERE last_accessed > CURRENT_TIMESTAMP - INTERVAL '1 day') as accessed_last_day,
                AVG(calculate_relevance(importance, decay_rate, created_at, last_accessed)) as avg_relevance
            FROM memories
            WHERE type = 'semantic' AND content LIKE '%' || $1
            GROUP BY type
        """, unique_suffix)
        
        # Verify calculations
        assert health_stats['total_memories'] == 10, "Should count exactly our 10 test memories"
        
        # Calculate expected average importance: (0.0 + 0.1 + 0.2 + ... + 0.9) / 10 = 4.5 / 10 = 0.45
        expected_avg_importance = sum(i * 0.1 for i in range(10)) / 10
        actual_avg_importance = float(health_stats['avg_importance'])
        assert abs(actual_avg_importance - expected_avg_importance) < 0.01, f"Average importance calculation incorrect: expected {expected_avg_importance}, got {actual_avg_importance}"
        
        # Test cluster_insights view accuracy
        cluster_id = await conn.fetchval(
            """
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Accuracy Test Cluster',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
            """
        )
        
        # Add some memories to cluster via graph (Phase 3)
        for memory_id in test_memories[:5]:
            await conn.execute("SELECT sync_memory_node($1)", memory_id)
            await conn.execute(
                "SELECT link_memory_to_cluster_graph($1, $2, $3)",
                memory_id, cluster_id, 1.0
            )
        
        cluster_insight = await conn.fetchrow("""
            SELECT * FROM cluster_insights WHERE name = 'Accuracy Test Cluster'
        """)
        
        assert cluster_insight['memory_count'] == 5, "Should count cluster members correctly"


async def test_error_recovery_scenarios(db_pool):
    """Test error recovery scenarios"""
    async with db_pool.acquire() as conn:
        # Test recovery from invalid JSON in JSONB fields
        # Create memory with episodic metadata
        memory_id = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                metadata
            ) VALUES (
                'episodic'::memory_type,
                'Error recovery test',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                jsonb_build_object(
                    'action_taken', '{"action": "valid_json"}'::jsonb,
                    'context', '{"context": "test"}'::jsonb,
                    'result', '{"result": "success"}'::jsonb
                )
            ) RETURNING id
        """)

        # Test that we can query the record
        episodic_data = await conn.fetchrow("""
            SELECT metadata FROM memories WHERE id = $1
        """, memory_id)

        assert episodic_data is not None, "Should be able to query episodic memory"

        # Test updating with new valid JSON in metadata
        await conn.execute("""
            UPDATE memories
            SET metadata = metadata || jsonb_build_object('action_taken', '{"action": "updated_action"}'::jsonb)
            WHERE id = $1
        """, memory_id)

        updated_data = await conn.fetchrow("""
            SELECT metadata FROM memories WHERE id = $1
        """, memory_id)

        # Parse the JSON if it's returned as a string
        metadata = updated_data['metadata']
        if isinstance(metadata, str):
            metadata = json.loads(metadata)

        action_taken = metadata['action_taken']
        if isinstance(action_taken, str):
            action_taken = json.loads(action_taken)

        assert action_taken['action'] == 'updated_action', "Should update JSON correctly"

        # Test transaction rollback scenario
        try:
            async with conn.transaction():
                # Create a memory
                temp_memory_id = await conn.fetchval("""
                    INSERT INTO memories (
                        type,
                        content,
                        embedding
                    ) VALUES (
                        'semantic'::memory_type,
                        'Rollback test unique content',
                        array_fill(0.5, ARRAY[embedding_dimension()])::vector
                    ) RETURNING id
                """)

                # Force an error by trying to insert a duplicate primary key
                await conn.execute("""
                    INSERT INTO memories (
                        id,
                        type,
                        content,
                        embedding
                    ) VALUES (
                        $1,
                        'semantic'::memory_type,
                        'Duplicate PK test',
                        array_fill(0.5, ARRAY[embedding_dimension()])::vector
                    )
                """, temp_memory_id)  # This should fail due to duplicate primary key
        except Exception:
            # Expected to fail
            pass

        # Verify the memory was rolled back
        rollback_check = await conn.fetchval("""
            SELECT COUNT(*) FROM memories WHERE content = 'Rollback test unique content'
        """)

        assert rollback_check == 0, "Transaction should have been rolled back"


async def test_worldview_driven_memory_filtering(db_pool):
    """Test how worldview affects memory retrieval and filtering"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("worldview_filter")
        worldview_id = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'values', 0.9, 0.8, 0.7, 'test')",
            f"Positive thinking matters {test_id}",
        )

        # Create memories with different emotional valences (stored in metadata)
        positive_memory_id = await conn.fetchval(
            """
            SELECT create_episodic_memory(
                'Positive experience',
                '{"action": "celebration"}',
                '{"context": "achievement"}',
                '{"result": "joy"}',
                0.8,
                CURRENT_TIMESTAMP,
                0.5
            )
            """
        )

        negative_memory_id = await conn.fetchval(
            """
            SELECT create_episodic_memory(
                'Negative experience',
                '{"action": "failure"}',
                '{"context": "disappointment"}',
                '{"result": "sadness"}',
                -0.8,
                CURRENT_TIMESTAMP,
                0.5
            )
            """
        )

        await conn.execute(
            "SELECT create_memory_relationship($1::uuid, $2::uuid, 'SUPPORTS'::graph_edge_type, $3::jsonb)",
            positive_memory_id,
            worldview_id,
            json.dumps({"strength": 1.0}),
        )
        await conn.execute(
            "SELECT create_memory_relationship($1::uuid, $2::uuid, 'CONTRADICTS'::graph_edge_type, $3::jsonb)",
            negative_memory_id,
            worldview_id,
            json.dumps({"strength": 1.0}),
        )

        pos_align = await conn.fetchval("SELECT compute_worldview_alignment($1)", positive_memory_id)
        neg_align = await conn.fetchval("SELECT compute_worldview_alignment($1)", negative_memory_id)
        assert pos_align > 0, "Positive memory should align with worldview"
        assert neg_align < 0, "Negative memory should contradict worldview"


# EMBEDDING INTEGRATION TESTS

async def test_embedding_service_integration(db_pool, ensure_embedding_service):
    """Test integration with embeddings microservice"""
    async with db_pool.acquire() as conn:
        # Test embedding service health check
        health_status = await conn.fetchval("""
            SELECT check_embedding_service_health()
        """)

        assert health_status is True, "Embedding service should be healthy"

        embedding = await conn.fetchval("""
            SELECT get_embedding('test content for embedding')
        """)
        assert embedding is not None, "Should generate embedding"

        # Test embedding cache
        cached_embedding = await conn.fetchval("""
            SELECT get_embedding('test content for embedding')
        """)
        assert cached_embedding == embedding, "Should return cached embedding"


async def test_create_memory_with_auto_embedding(db_pool, ensure_embedding_service):
    """Test creating memories with automatic embedding generation"""
    async with db_pool.acquire() as conn:
        # Test creating semantic memory with auto-embedding
        memory_id = await conn.fetchval("""
            SELECT create_semantic_memory(
                'User prefers dark mode interfaces',
                0.9,
                ARRAY['preference', 'UI'],
                ARRAY['interface', 'theme', 'dark mode']
            )
        """)

        assert memory_id is not None, "Should create memory with auto-embedding"

        # Verify memory was created with embedding
        memory_data = await conn.fetchrow("""
            SELECT content, embedding, type FROM memories WHERE id = $1
        """, memory_id)

        assert memory_data['content'] == 'User prefers dark mode interfaces'
        assert memory_data['embedding'] is not None
        assert memory_data['type'] == 'semantic'

        # Test episodic memory creation
        episodic_id = await conn.fetchval("""
            SELECT create_episodic_memory(
                'User clicked the help button',
                '{"action": "click", "element": "help_button"}',
                '{"page": "settings", "section": "account"}',
                '{"modal_opened": true, "help_displayed": true}',
                0.1
            )
        """)

        assert episodic_id is not None, "Should create episodic memory"

        await conn.execute("""
            DELETE FROM memories WHERE content IN (
                'User prefers dark mode interfaces',
                'User clicked the help button'
            )
        """)


async def test_search_with_auto_embedding(db_pool, ensure_embedding_service):
    """Test searching memories with automatic query embedding"""
    async with db_pool.acquire() as conn:
        # Create some test memories first
        test_contents = [
            'User interface design principles',
            'Dark mode reduces eye strain',
            'Accessibility features for visually impaired users'
        ]

        for content in test_contents:
            await conn.fetchval("""
                SELECT create_semantic_memory($1, 0.8)
            """, content)

        # Test similarity search with auto-embedding
        results = await conn.fetch("""
            SELECT * FROM search_similar_memories('user interface preferences', 5)
        """)

        assert len(results) > 0, "Should find similar memories"

        # Verify results have expected fields
        for result in results:
            assert 'memory_id' in result
            assert 'content' in result
            assert 'similarity' in result
            assert result['similarity'] >= 0 and result['similarity'] <= 1


async def test_working_memory_with_embedding(db_pool, ensure_embedding_service):
    """Test working memory operations with automatic embedding"""
    async with db_pool.acquire() as conn:
        # Add to working memory with auto-embedding
        wm_id = await conn.fetchval("""
            SELECT add_to_working_memory(
                'Current user is browsing settings page',
                INTERVAL '30 minutes'
            )
        """)

        assert wm_id is not None, "Should add to working memory"

        # Search working memory
        results = await conn.fetch("""
            SELECT * FROM search_working_memory('user settings', 3)
        """)

        assert len(results) > 0, "Should find working memory items"


async def test_embedding_cache_functionality(db_pool, ensure_embedding_service):
    """Test embedding caching functionality"""
    async with db_pool.acquire() as conn:
        # Check if embedding service is available
        health_status = await conn.fetchval("""
            SELECT check_embedding_service_health()
        """)
        
        if not health_status:
            print("Skipping cache tests - service not available")
            return
        
        try:
            # Clear any existing cache for our test content
            test_content = 'unique test content for caching'
            content_hash = await conn.fetchval("""
                SELECT encode(sha256($1::text::bytea), 'hex')
            """, test_content)
            
            await conn.execute("""
                DELETE FROM embedding_cache WHERE content_hash = $1
            """, content_hash)
            
            # First call should hit the service and cache the result
            embedding1 = await conn.fetchval("""
                SELECT get_embedding($1)
            """, test_content)
            
            # Verify it was cached
            cached_count = await conn.fetchval("""
                SELECT COUNT(*) FROM embedding_cache WHERE content_hash = $1
            """, content_hash)
            
            assert cached_count == 1, "Should cache the embedding"
            
            # Second call should use cache
            embedding2 = await conn.fetchval("""
                SELECT get_embedding($1)
            """, test_content)
            
            assert embedding1 == embedding2, "Should return same embedding from cache"
            
            # Test cache cleanup
            deleted_count = await conn.fetchval("""
                SELECT cleanup_embedding_cache(INTERVAL '0 seconds')
            """)
            
            assert deleted_count >= 1, "Should clean up cache entries"
            
        except Exception as e:
            print(f"Cache test failed: {e}")


async def test_embedding_error_handling(db_pool, ensure_embedding_service):
    """Test error handling for embedding operations"""
    async with db_pool.acquire() as conn:
        # Phase 7 (ReduceScopeCreep): Use unified config only
        original_url = await conn.fetchval(
            "SELECT get_config_text('embedding.service_url')"
        )
        original_retry_seconds, original_retry_interval_seconds = await _set_embedding_retry_config(
            conn,
            retry_seconds=0,
            retry_interval_seconds=0.0,
        )

        # Test with invalid service URL
        await conn.execute(
            "SELECT set_config('embedding.service_url', '\"http://invalid-service:9999/embed\"'::jsonb)"
        )

        # This should fail gracefully
        try:
            await conn.fetchval("""
                SELECT get_embedding('test content')
            """)
            assert False, "Should have failed with invalid service URL"
        except Exception as e:
            assert "Failed to get embedding" in str(e), "Should have proper error message"

        # Restore valid URL and retry settings
        await conn.execute(
            "SELECT set_config('embedding.service_url', $1::jsonb)",
            json.dumps(original_url),
        )
        await _restore_embedding_retry_config(
            conn,
            original_retry_seconds,
            original_retry_interval_seconds,
        )


async def test_memory_cluster_with_embeddings(db_pool, ensure_embedding_service):
    """Test memory clustering with automatic embeddings"""
    async with db_pool.acquire() as conn:
        # Check if embedding service is available
        health_status = await conn.fetchval("""
            SELECT check_embedding_service_health()
        """)
        
        if not health_status:
            print("Skipping cluster embedding tests - service not available")
            return
        
        try:
            # Create memories with related content
            memory_ids = []
            related_contents = [
                'User interface design best practices',
                'UI accessibility guidelines',
                'Interface usability principles'
            ]
            
            for content in related_contents:
                memory_id = await conn.fetchval("""
                    SELECT create_semantic_memory($1, 0.8)
                """, content)
                memory_ids.append(memory_id)
            
            # Create cluster with these memories
            cluster_id = await conn.fetchval("""
                SELECT create_memory_cluster(
                    'UI Design Principles',
                    'theme',
                    'Cluster for UI design related memories',
                    $1
                )
            """, memory_ids)
            
            assert cluster_id is not None, "Should create cluster"
            
            # Verify cluster has centroid embedding
            centroid = await conn.fetchval("""
                SELECT centroid_embedding FROM clusters WHERE id = $1
            """, cluster_id)
            
            assert centroid is not None, "Cluster should have centroid embedding"
            
        except Exception as e:
            print(f"Cluster embedding test failed: {e}")


# MEDIUM PRIORITY ADDITIONAL TESTS

async def test_complex_graph_traversals(db_pool):
    """Test complex multi-hop graph traversals and path finding"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")
        
        # Create a complex memory network
        memory_chain = []
        memory_types = ['episodic', 'semantic', 'procedural', 'strategic', 'episodic']
        
        # Create 5 connected memories
        for i, mem_type in enumerate(memory_types):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (type, content, embedding)
                VALUES ($1::memory_type, $2, array_fill($3::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
            """, mem_type, f'Complex memory {i}', float(i) * 0.1)
            memory_chain.append(memory_id)
            
            # Create graph node
            await conn.execute(f"""
                SELECT * FROM cypher('memory_graph', $$
                    CREATE (n:MemoryNode {{
                        memory_id: '{memory_id}',
                        type: '{mem_type}',
                        step: {i}
                    }})
                    RETURN n
                $$) as (n ag_catalog.agtype)
            """)
        
        # Create linear chain relationships
        for i in range(len(memory_chain) - 1):
            await conn.execute(f"""
                SELECT * FROM cypher('memory_graph', $$
                    MATCH (a:MemoryNode {{memory_id: '{memory_chain[i]}'}}),
                          (b:MemoryNode {{memory_id: '{memory_chain[i+1]}'}})
                    CREATE (a)-[r:LEADS_TO {{strength: {0.8 - i*0.1}}}]->(b)
                    RETURN r
                $$) as (r ag_catalog.agtype)
            """)
        
        # Create some cross-connections
        await conn.execute(f"""
            SELECT * FROM cypher('memory_graph', $$
                MATCH (a:MemoryNode {{memory_id: '{memory_chain[0]}'}}),
                      (b:MemoryNode {{memory_id: '{memory_chain[3]}'}})
                CREATE (a)-[r:INFLUENCES {{strength: 0.6}}]->(b)
                RETURN r
            $$) as (r ag_catalog.agtype)
        """)
        
        # Test 1: Find all paths between first and last memory
        paths = await conn.fetch(f"""
            SELECT * FROM cypher('memory_graph', $$
                MATCH p = (start:MemoryNode {{memory_id: '{memory_chain[0]}'}})-[*1..5]->(finish:MemoryNode {{memory_id: '{memory_chain[-1]}'}})
                RETURN p
            $$) as (p ag_catalog.agtype)
        """)
        
        assert len(paths) >= 1, "Should find at least one path"
        
        # Test 2: Find memories within 2 hops of the first memory
        nearby_memories = await conn.fetch(f"""
            SELECT * FROM cypher('memory_graph', $$
                MATCH (start:MemoryNode {{memory_id: '{memory_chain[0]}'}})-[*1..2]->(nearby:MemoryNode)
                RETURN DISTINCT nearby.memory_id as memory_id, nearby.type as type
            $$) as (memory_id ag_catalog.agtype, type ag_catalog.agtype)
        """)
        
        assert len(nearby_memories) >= 2, "Should find nearby memories"
        
        # Test 3: Find any path with relationship properties
        weighted_path = await conn.fetch(f"""
            SELECT * FROM cypher('memory_graph', $$
                MATCH p = (start:MemoryNode {{memory_id: '{memory_chain[0]}'}})-[*1..5]->(finish:MemoryNode {{memory_id: '{memory_chain[-1]}'}})
                RETURN p
                LIMIT 1
            $$) as (path ag_catalog.agtype)
        """)
        
        assert len(weighted_path) > 0, "Should find a path"
        
        # Test 4: Complex pattern matching
        patterns = await conn.fetch("""
            SELECT * FROM cypher('memory_graph', $$
                MATCH (e:MemoryNode {type: 'episodic'})-[:LEADS_TO]->(s:MemoryNode {type: 'semantic'})-[:LEADS_TO]->(p:MemoryNode {type: 'procedural'})
                RETURN e.memory_id as episodic_id, s.memory_id as semantic_id, p.memory_id as procedural_id
            $$) as (episodic_id ag_catalog.agtype, semantic_id ag_catalog.agtype, procedural_id ag_catalog.agtype)
        """)
        
        assert len(patterns) > 0, "Should find episodic->semantic->procedural patterns"


async def test_memory_lifecycle_management(db_pool):
    """Test comprehensive memory lifecycle management"""
    async with db_pool.acquire() as conn:
        # Create memories with different lifecycle stages
        lifecycle_memories = []
        
        # Stage 1: Fresh memories (high importance, recent)
        for i in range(3):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    access_count,
                    created_at
                ) VALUES (
                    'semantic'::memory_type,
                    'Fresh memory ' || $1,
                    array_fill(0.8, ARRAY[embedding_dimension()])::vector,
                    0.9,
                    10 + $2,
                    CURRENT_TIMESTAMP - interval '1 hour' * $2
                ) RETURNING id
            """, str(i), i)
            lifecycle_memories.append(('fresh', memory_id))
        
        # Stage 2: Aging memories (medium importance, older)
        for i in range(3):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    access_count,
                    created_at
                ) VALUES (
                    'episodic'::memory_type,
                    'Aging memory ' || $1,
                    array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                    0.5,
                    5 + $2,
                    CURRENT_TIMESTAMP - interval '7 days' * ($2 + 1)
                ) RETURNING id
            """, str(i), i)
            lifecycle_memories.append(('aging', memory_id))
        
        # Stage 3: Stale memories (low importance, very old)
        for i in range(3):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    access_count,
                    created_at
                ) VALUES (
                    'procedural'::memory_type,
                    'Stale memory ' || $1,
                    array_fill(0.2, ARRAY[embedding_dimension()])::vector,
                    0.1,
                    1,
                    CURRENT_TIMESTAMP - interval '30 days' * ($2 + 1)
                ) RETURNING id
            """, str(i), i)
            lifecycle_memories.append(('stale', memory_id))
        
        # Test lifecycle categorization
        fresh_count = await conn.fetchval("""
            SELECT COUNT(*) FROM memories
            WHERE created_at > CURRENT_TIMESTAMP - interval '1 day'
            AND importance > 0.7
        """)
        
        aging_count = await conn.fetchval("""
            SELECT COUNT(*) FROM memories
            WHERE created_at BETWEEN CURRENT_TIMESTAMP - interval '30 days' 
                                 AND CURRENT_TIMESTAMP - interval '1 day'
            AND importance BETWEEN 0.3 AND 0.7
        """)
        
        stale_count = await conn.fetchval("""
            SELECT COUNT(*) FROM memories
            WHERE created_at < CURRENT_TIMESTAMP - interval '30 days'
            AND importance < 0.3
        """)
        
        assert fresh_count >= 3, "Should have fresh memories"
        assert aging_count >= 3, "Should have aging memories"
        assert stale_count >= 3, "Should have stale memories"
        
        # Test memory promotion (accessing old memory should increase importance)
        stale_memory = [m for stage, m in lifecycle_memories if stage == 'stale'][0]
        
        initial_importance = await conn.fetchval("""
            SELECT importance FROM memories WHERE id = $1
        """, stale_memory)
        
        # Simulate multiple accesses
        for _ in range(5):
            await conn.execute("""
                UPDATE memories 
                SET access_count = access_count + 1
                WHERE id = $1
            """, stale_memory)
        
        final_importance = await conn.fetchval("""
            SELECT importance FROM memories WHERE id = $1
        """, stale_memory)
        
        assert final_importance > initial_importance, "Accessed memory should gain importance"
        
        # Test memory archival workflow
        archival_candidates = await conn.fetch("""
            SELECT id, calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score
            FROM memories
            WHERE calculate_relevance(importance, decay_rate, created_at, last_accessed) < 0.1
            AND status = 'active'
            ORDER BY calculate_relevance(importance, decay_rate, created_at, last_accessed) ASC
            LIMIT 5
        """)
        
        for candidate in archival_candidates:
            await conn.execute("""
                UPDATE memories 
                SET status = 'archived'::memory_status
                WHERE id = $1
            """, candidate['id'])
        
        # Verify archival
        archived_count = await conn.fetchval("""
            SELECT COUNT(*) FROM memories WHERE status = 'archived'
        """)
        
        assert archived_count > 0, "Should have archived some memories"


async def test_memory_pruning_operations(db_pool):
    """Test memory pruning and cleanup operations"""
    async with db_pool.acquire() as conn:
        # Create memories for pruning
        pruning_memories = []
        
        # Create very old, low-relevance memories
        for i in range(10):
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    access_count,
                    created_at,
                    last_accessed
                ) VALUES (
                    'semantic'::memory_type,
                    'Pruning candidate ' || $1,
                    array_fill(0.1, ARRAY[embedding_dimension()])::vector,
                    0.05,
                    0,
                    CURRENT_TIMESTAMP - interval '90 days',
                    CURRENT_TIMESTAMP - interval '60 days'
                ) RETURNING id
            """, str(i))
            pruning_memories.append(memory_id)
        
        # Create some memories worth keeping
        for i in range(5):
            await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    access_count,
                    created_at
                ) VALUES (
                    'episodic'::memory_type,
                    'Important memory ' || $1,
                    array_fill(0.8, ARRAY[embedding_dimension()])::vector,
                    0.8,
                    20,
                    CURRENT_TIMESTAMP - interval '30 days'
                ) RETURNING id
            """, str(i))
        
        # Test pruning criteria identification
        pruning_candidates = await conn.fetch("""
            SELECT id, calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score,
                   importance,
                   age_in_days(created_at) as age_days,
                   COALESCE(age_in_days(last_accessed), 999) as days_since_access
            FROM memories
            WHERE calculate_relevance(importance, decay_rate, created_at, last_accessed) < 0.1
            AND (last_accessed IS NULL OR last_accessed < CURRENT_TIMESTAMP - interval '30 days')
            AND importance < 0.1
            ORDER BY calculate_relevance(importance, decay_rate, created_at, last_accessed) ASC
        """)
        
        assert len(pruning_candidates) >= 10, "Should identify pruning candidates"
        
        # Test safe pruning (archive first, then delete)
        for candidate in pruning_candidates[:5]:
            # First archive
            await conn.execute("""
                UPDATE memories 
                SET status = 'archived'::memory_status
                WHERE id = $1
            """, candidate['id'])
        
        # Then delete archived memories older than threshold
        deleted_count = await conn.fetchval("""
            WITH deleted_memories AS (
                DELETE FROM memories 
                WHERE status = 'archived'
                AND created_at < CURRENT_TIMESTAMP - interval '120 days'
                RETURNING id
            )
            SELECT COUNT(*) FROM deleted_memories
        """)
        
        # Test working memory cleanup
        # Create expired working memory entries
        for i in range(5):
            await conn.execute("""
                INSERT INTO working_memory (
                    content,
                    embedding,
                    expiry
                ) VALUES (
                    'Expired working memory ' || $1,
                    array_fill(0.5, ARRAY[embedding_dimension()])::vector,
                    CURRENT_TIMESTAMP - interval '1 hour'
                )
            """, str(i))
        
        # Clean up expired working memory
        expired_cleaned = await conn.fetchval("""
            WITH cleaned AS (
                DELETE FROM working_memory
                WHERE expiry < CURRENT_TIMESTAMP
                RETURNING id
            )
            SELECT COUNT(*) FROM cleaned
        """)
        
        assert expired_cleaned >= 5, "Should clean up expired working memory"


async def test_database_optimization_operations(db_pool):
    """Test database optimization and maintenance operations"""
    async with db_pool.acquire() as conn:
        # Test index usage analysis
        index_usage = await conn.fetch("""
            SELECT 
                schemaname,
                relname as tablename,
                indexrelname as indexname,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch
            FROM pg_stat_user_indexes
            WHERE schemaname = 'public'
            AND relname IN ('memories', 'clusters')
            ORDER BY idx_scan DESC
        """)
        
        assert len(index_usage) > 0, "Should have index usage statistics"
        
        # Test table statistics
        table_stats = await conn.fetch("""
            SELECT 
                schemaname,
                relname as tablename,
                n_tup_ins,
                n_tup_upd,
                n_tup_del,
                n_live_tup,
                n_dead_tup
            FROM pg_stat_user_tables
            WHERE schemaname = 'public'
            AND relname IN ('memories', 'clusters')
            ORDER BY n_live_tup DESC
        """)
        
        assert len(table_stats) > 0, "Should have table statistics"
        
        # Test query performance analysis
        # Create a complex query and analyze its performance
        import time
        
        # Phase 3 (ReduceScopeCreep): memory_cluster_members removed - use simplified query
        start_time = time.time()
        complex_query_result = await conn.fetch("""
            SELECT
                m.type,
                COUNT(*) as memory_count,
                AVG(m.importance) as avg_importance,
                AVG(calculate_relevance(m.importance, m.decay_rate, m.created_at, m.last_accessed)) as avg_relevance
            FROM memories m
            WHERE m.status = 'active'
            GROUP BY m.type
            HAVING COUNT(*) > 0
            ORDER BY avg_importance DESC
        """)
        query_time = time.time() - start_time
        
        assert len(complex_query_result) > 0, "Complex query should return results"
        assert query_time < PERF_OPTIMIZE_QUERY_SECONDS, (
            f"Complex query too slow: {query_time}s"
        )
        
        # Test vacuum and analyze simulation (read-only operations)
        vacuum_info = await conn.fetch("""
            SELECT
                schemaname,
                relname as tablename,
                last_vacuum,
                last_autovacuum,
                last_analyze,
                last_autoanalyze,
                vacuum_count,
                autovacuum_count
            FROM pg_stat_user_tables
            WHERE schemaname = 'public'
            AND relname = 'memories'
        """)
        
        assert len(vacuum_info) > 0, "Should have vacuum statistics"


async def test_backup_restore_consistency(db_pool):
    """Test backup and restore data consistency"""
    async with db_pool.acquire() as conn:
        # Clean up any existing test data first
        # Phase 3 (ReduceScopeCreep): memory_cluster_members removed - cleanup graph edges handled separately
        await conn.execute("""
            DELETE FROM memories WHERE content LIKE 'Backup test memory%'
        """)
        await conn.execute("""
            DELETE FROM clusters WHERE name = 'Backup Test Cluster'
        """)

        # Create a known dataset for backup testing
        backup_test_data = []

        # Create test memories with relationships (metadata contains semantic details)
        for i in range(5):
            metadata = json.dumps({
                "confidence": 0.8,
                "category": [f'category_{i}']
            })
            memory_id = await conn.fetchval("""
                INSERT INTO memories (
                    type,
                    content,
                    embedding,
                    importance,
                    metadata
                ) VALUES (
                    'semantic'::memory_type,
                    'Backup test memory ' || $1,
                    array_fill($2::float, ARRAY[embedding_dimension()])::vector,
                    $3,
                    $4::jsonb
                ) RETURNING id
            """, str(i), float(i) * 0.1, 0.5 + (i * 0.1), metadata)

            backup_test_data.append(memory_id)
        
        # Create cluster and relationships
        cluster_id = await conn.fetchval("""
            INSERT INTO clusters (
                cluster_type,
                name,
                centroid_embedding
            ) VALUES (
                'theme'::cluster_type,
                'Backup Test Cluster',
                array_fill(0.5, ARRAY[embedding_dimension()])::vector
            ) RETURNING id
        """)
        
        # Add memories to cluster via graph (Phase 3)
        for memory_id in backup_test_data:
            await conn.execute("SELECT sync_memory_node($1)", memory_id)
            await conn.execute(
                "SELECT link_memory_to_cluster_graph($1, $2, $3)",
                memory_id, cluster_id, 0.8
            )
        
        # Simulate backup verification by checking data consistency
        # Test 1: Verify all semantic memories have metadata with confidence
        memories_with_metadata = await conn.fetch("""
            SELECT m.id, m.metadata
            FROM memories m
            WHERE m.type = 'semantic'
            AND m.content LIKE 'Backup test memory%'
        """)

        for mem in memories_with_metadata:
            metadata = mem['metadata']
            if isinstance(metadata, str):
                metadata = json.loads(metadata)
            assert metadata.get('confidence') is not None, "Semantic memory should have confidence in metadata"

        # Test 2: Verify cluster relationships are intact via graph (Phase 3)
        cluster_members = await conn.fetchval("""
            SELECT COUNT(*)
            FROM get_cluster_members_graph($1) gcm
            JOIN memories m ON gcm.memory_id = m.id
            WHERE m.content LIKE 'Backup test memory%'
        """, cluster_id)

        assert cluster_members == 5, "All test memories should be in cluster"

        # Test 3: Referential integrity now handled by graph - just verify cluster exists
        cluster_exists = await conn.fetchval("""
            SELECT COUNT(*) FROM clusters WHERE name = 'Backup Test Cluster'
        """)
        assert cluster_exists == 1, "Backup Test Cluster should exist"
        
        # Test 4: Verify computed fields are consistent
        computed_field_check = await conn.fetch("""
            SELECT
                id,
                importance,
                calculate_relevance(importance, decay_rate, created_at, last_accessed) as relevance_score,
                (importance * exp(-decay_rate * age_in_days(created_at))) as expected_relevance
            FROM memories
            WHERE content LIKE 'Backup test memory%'
        """)

        for row in computed_field_check:
            actual = float(row['relevance_score'])
            expected = float(row['expected_relevance'])
            # Relaxed tolerance since calculate_relevance has more complex formula
            assert abs(actual - expected) < 0.5, f"Relevance score mismatch: {actual} vs {expected}"


async def test_schema_migration_compatibility(db_pool):
    """Test schema migration and version compatibility"""
    async with db_pool.acquire() as conn:
        # Test 1: Check current schema version (simulate version tracking)
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS schema_version (
                version INTEGER PRIMARY KEY,
                applied_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
                description TEXT
            )
        """)
        
        # Record current schema version
        await conn.execute("""
            INSERT INTO schema_version (version, description)
            VALUES (1, 'Initial Hexis Memory System schema')
            ON CONFLICT (version) DO NOTHING
        """)
        
        # Test 2: Simulate adding a new column (non-breaking change)
        await conn.execute("""
            ALTER TABLE memories 
            ADD COLUMN IF NOT EXISTS migration_test_field TEXT DEFAULT 'test_value'
        """)
        
        # Verify the column was added
        column_exists = await conn.fetchval("""
            SELECT COUNT(*) 
            FROM information_schema.columns 
            WHERE table_name = 'memories' 
            AND column_name = 'migration_test_field'
        """)
        
        assert column_exists == 1, "Migration test column should exist"
        
        # Test 3: Verify existing data integrity after migration
        memory_count_before = await conn.fetchval("""
            SELECT COUNT(*) FROM memories
        """)
        
        # Test that existing memories have the default value
        default_value_count = await conn.fetchval("""
            SELECT COUNT(*) FROM memories 
            WHERE migration_test_field = 'test_value'
        """)
        
        assert default_value_count == memory_count_before, "All existing memories should have default value"
        
        # Test 4: Simulate index migration
        await conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_memories_migration_test 
            ON memories(migration_test_field)
        """)
        
        # Verify index was created
        index_exists = await conn.fetchval("""
            SELECT COUNT(*) 
            FROM pg_indexes 
            WHERE tablename = 'memories' 
            AND indexname = 'idx_memories_migration_test'
        """)
        
        assert index_exists == 1, "Migration test index should exist"
        
        # Test 5: Simulate rollback capability
        await conn.execute("""
            ALTER TABLE memories 
            DROP COLUMN IF EXISTS migration_test_field
        """)
        
        # Verify rollback worked
        column_exists_after = await conn.fetchval("""
            SELECT COUNT(*) 
            FROM information_schema.columns 
            WHERE table_name = 'memories' 
            AND column_name = 'migration_test_field'
        """)
        
        assert column_exists_after == 0, "Migration test column should be removed"
        
        # Clean up
        await conn.execute("DROP TABLE IF EXISTS schema_version")


async def test_monitoring_and_alerting_metrics(db_pool):
    """Test monitoring metrics and alerting thresholds"""
    async with db_pool.acquire() as conn:
        # Test 1: Memory system health metrics
        health_metrics = await conn.fetchrow("""
            SELECT
                COUNT(*) as total_memories,
                COUNT(*) FILTER (WHERE status = 'active') as active_memories,
                COUNT(*) FILTER (WHERE status = 'archived') as archived_memories,
                AVG(importance) as avg_importance,
                AVG(calculate_relevance(importance, decay_rate, created_at, last_accessed)) as avg_relevance,
                AVG(access_count) as avg_access_count,
                COUNT(*) FILTER (WHERE last_accessed > CURRENT_TIMESTAMP - interval '24 hours') as recently_accessed
            FROM memories
        """)
        
        assert health_metrics['total_memories'] > 0, "Should have memories for monitoring"
        
        # Test 2: Cluster health metrics (Phase 3: using graph for cluster members)
        cluster_metrics = await conn.fetchrow(
            """
            SELECT
                COUNT(*) as total_clusters
            FROM clusters
            """
        )
        
        assert cluster_metrics['total_clusters'] >= 0, "Should have cluster metrics"
        
        # Test 3: Performance metrics
        performance_metrics = await conn.fetch("""
            -- Phase 3 (ReduceScopeCreep): memory_cluster_members removed - use simplified metrics
            SELECT
                'vector_search' as metric_type,
                COUNT(*) as operations,
                AVG(EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - created_at))) as avg_age_seconds
            FROM memories
            WHERE created_at > CURRENT_TIMESTAMP - interval '1 hour'
        """)
        
        assert len(performance_metrics) > 0, "Should have performance metrics"
        
        # Test 4: Alert threshold simulation
        alert_conditions = await conn.fetch("""
            SELECT 
                'low_memory_activity' as alert_type,
                CASE 
                    WHEN COUNT(*) FILTER (WHERE last_accessed > CURRENT_TIMESTAMP - interval '24 hours') < 10 
                    THEN 'ALERT' 
                    ELSE 'OK' 
                END as status,
                COUNT(*) FILTER (WHERE last_accessed > CURRENT_TIMESTAMP - interval '24 hours') as recent_access_count
            FROM memories
            
            UNION ALL
            
            SELECT 
                'high_archive_rate' as alert_type,
                CASE 
                    WHEN COUNT(*) FILTER (WHERE status = 'archived') > COUNT(*) * 0.5 
                    THEN 'ALERT' 
                    ELSE 'OK' 
                END as status,
                COUNT(*) FILTER (WHERE status = 'archived') as archived_count
            FROM memories
            
        """)

        # Phase 3 (ReduceScopeCreep): memory_cluster_members removed - reduced alert checks
        assert len(alert_conditions) == 2, "Should have two alert conditions"
        
        # Test 5: Resource usage metrics (Phase 3: memory_cluster_members removed)
        resource_metrics = await conn.fetchrow("""
            SELECT
                pg_size_pretty(pg_total_relation_size('memories')) as memories_table_size,
                pg_size_pretty(pg_total_relation_size('clusters')) as clusters_table_size,
                (SELECT COUNT(*) FROM memories) as memory_count,
                (SELECT COUNT(*) FROM clusters) as cluster_count
        """)
        
        assert resource_metrics is not None, "Should have resource metrics"


async def test_multi_hexis_considerations(db_pool):
    """Test considerations for multi-Hexis support (current limitations)"""
    async with db_pool.acquire() as conn:
        # Clean up any existing test data first
        await conn.execute("""
            DELETE FROM memories WHERE content LIKE '%Hexis-% believes X is%'
        """)
        
        # Test 1: Identify single-Hexis assumptions in current schema
        # Phase 5: identity_aspects and worldview_primitives were removed - worldview is now
        # stored in memories table with type='worldview'. Check for other singleton tables.
        single_hexis_tables = await conn.fetch("""
            SELECT
                table_name,
                CASE
                    WHEN table_name IN ('agent_state', 'emotional_state') THEN 'singleton_table'
                    WHEN table_name LIKE '%memory%' THEN 'memory_table'
                    ELSE 'other'
                END as table_category
            FROM information_schema.tables
            WHERE table_schema = 'public'
            AND table_type = 'BASE TABLE'
            ORDER BY table_name
        """)

        singleton_tables = [t for t in single_hexis_tables if t['table_category'] == 'singleton_table']
        # Note: agent_state and emotional_state are singleton tables (single-Hexis design)
        assert len(singleton_tables) >= 0, "May have singleton tables"
        
        # Test 2: Simulate multi-Hexis data isolation requirements
        # This test demonstrates what would need to change for multi-Hexis support
        
        # Check if any tables have Hexis instance identification
        hexis_id_columns = await conn.fetch("""
            SELECT 
                table_name,
                column_name
            FROM information_schema.columns
            WHERE table_schema = 'public'
            AND column_name LIKE '%hexis%'
            ORDER BY table_name, column_name
        """)
        
        # Current schema should have no Hexis ID columns (single-Hexis design)
        assert len(hexis_id_columns) == 0, "Current schema should not have Hexis ID columns"
        
        # Test 3: Demonstrate memory isolation challenges
        # Create test scenario showing how memories could conflict between Hexis instances
        
        # Hexis 1 memories
        hexis1_memory = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance
            ) VALUES (
                'semantic'::memory_type,
                'Hexis-1 believes X is true',
                array_fill(0.8, ARRAY[embedding_dimension()])::vector,
                0.9
            ) RETURNING id
        """)
        
        # Hexis 2 memories (conflicting belief)
        hexis2_memory = await conn.fetchval("""
            INSERT INTO memories (
                type,
                content,
                embedding,
                importance
            ) VALUES (
                'semantic'::memory_type,
                'Hexis-2 believes X is false',
                array_fill(0.8, ARRAY[embedding_dimension()])::vector,
                0.9
            ) RETURNING id
        """)
        
        # Demonstrate conflict: both memories exist in same space
        conflicting_memories = await conn.fetch("""
            SELECT 
                id,
                content,
                importance,
                'conflict_detected' as issue_type
            FROM memories
            WHERE content LIKE '%Hexis-% believes X is%'
            ORDER BY content
        """)
        
        assert len(conflicting_memories) == 2, "Should find conflicting Hexis memories"
        
        # Test 4: Demonstrate worldview storage in memories table
        # Phase 5: worldview is now stored in memories with type='worldview'
        worldview_count = await conn.fetchval("""
            SELECT COUNT(*) FROM memories WHERE type = 'worldview'
        """)
        # This count may be 0 if no worldview memories exist yet - that's ok

        # Test 5: Show what would be needed for multi-Hexis support
        # Phase 5: identity_aspects and worldview_primitives were consolidated into memories
        multi_hexis_requirements = {
            'schema_changes_needed': [
                'Add hexis_instance_id to memories table',
                'Add hexis_instance_id to agent_state, emotional_state',
                'Add row-level security policies',
                'Modify all views to filter by Hexis instance',
                'Update all functions to include Hexis context'
            ],
            'isolation_challenges': [
                'Memory similarity search across Hexis boundaries',
                'Cluster centroid calculations per Hexis',
                'Graph relationships between Hexis instances',
                'Shared vs private memory spaces',
                'Cross-Hexis learning and knowledge transfer'
            ]
        }
        
        # This test documents the current single-Hexis limitations
        assert len(multi_hexis_requirements['schema_changes_needed']) > 0, "Multi-Hexis support requires significant changes"


# ============================================================================
# NEW TEST CASES - Acceleration Layer, Concepts, Core Functions
# ============================================================================

# -----------------------------------------------------------------------------
# EPISODE & TEMPORAL SEGMENTATION TESTS
# -----------------------------------------------------------------------------

async def test_episodes_table_structure(db_pool):
    """Test episodes table with time_range TSTZRANGE generated column"""
    async with db_pool.acquire() as conn:
        # Check table structure
        columns = await conn.fetch("""
            SELECT column_name, data_type, is_nullable, generation_expression
            FROM information_schema.columns
            WHERE table_name = 'episodes'
            ORDER BY ordinal_position
        """)
        column_dict = {col['column_name']: col for col in columns}

        assert 'id' in column_dict
        assert 'started_at' in column_dict
        assert 'ended_at' in column_dict
        assert 'metadata' in column_dict
        assert 'summary' in column_dict
        assert 'summary_embedding' in column_dict
        assert 'time_range' in column_dict

        # Create an episode and verify time_range is auto-generated
        episode_id = await conn.fetchval(
            """
            INSERT INTO episodes (started_at, ended_at, metadata)
            VALUES (
                '2024-01-01 10:00:00'::timestamptz,
                '2024-01-01 11:00:00'::timestamptz,
                jsonb_build_object('episode_type', 'conversation')
            ) RETURNING id
            """
        )

        time_range = await conn.fetchval("""
            SELECT time_range FROM episodes WHERE id = $1
        """, episode_id)

        assert time_range is not None, "time_range should be auto-generated"


async def test_auto_episode_assignment_trigger(db_pool, ensure_embedding_service):
    """Test trg_auto_episode_assignment trigger creates episodes automatically"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        # Clean up any existing open episodes for this test
        await conn.execute("""
            UPDATE episodes SET ended_at = started_at
            WHERE ended_at IS NULL
        """)

        # Create first memory - should create new episode
        memory1_id = await conn.fetchval(
            """
            SELECT create_memory('episodic'::memory_type, $1, 0.5)
            """,
            "First memory in episode",
        )

        # Verify episode was created
        episode1 = await _fetch_episode_for_memory(conn, memory1_id)

        assert episode1 is not None, "Episode should be created for first memory"
        assert episode1['sequence_order'] == 1, "First memory should have sequence_order 1"
        assert episode1['ended_at'] is None, "Episode should still be open"

        # Create second memory immediately - should be in same episode
        memory2_id = await conn.fetchval(
            """
            SELECT create_memory('episodic'::memory_type, $1, 0.5)
            """,
            "Second memory in same episode",
        )

        episode2 = await _fetch_episode_for_memory(conn, memory2_id)

        assert episode2['episode_id'] == episode1['episode_id'], "Second memory should be in same episode"
        assert episode2['sequence_order'] == 2, "Second memory should have sequence_order 2"

        # Verify memory_neighborhoods was initialized
        neighborhood = await conn.fetchrow("""
            SELECT memory_id, is_stale FROM memory_neighborhoods
            WHERE memory_id = $1
        """, memory1_id)

        assert neighborhood is not None, "memory_neighborhoods should be initialized"
        assert neighborhood['is_stale'] == True, "New neighborhood should be marked stale"


async def test_episode_30_minute_gap_detection(db_pool, ensure_embedding_service):
    """Test that episodes close and new ones open after 30-minute gap"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        # Close any open episodes
        await conn.execute("""
            UPDATE episodes SET ended_at = started_at
            WHERE ended_at IS NULL
        """)

        # Create memory with specific timestamp
        base_time = await conn.fetchval("SELECT CURRENT_TIMESTAMP")
        memory1_id = uuid.uuid4()

        await _ensure_memory_node(conn, memory1_id, "semantic")
        await conn.execute(
            """
            INSERT INTO memories (id, type, content, embedding, created_at)
            VALUES ($1, 'semantic'::memory_type, $2, get_embedding($2), $3)
            """,
            memory1_id,
            "Memory before gap",
            base_time,
        )

        episode1 = await _fetch_episode_for_memory(conn, memory1_id)
        episode1_id = episode1["episode_id"]

        # Create memory 31 minutes later - should trigger new episode
        later_time = base_time + timedelta(minutes=31)
        memory2_id = uuid.uuid4()

        await _ensure_memory_node(conn, memory2_id, "semantic")
        await conn.execute(
            """
            INSERT INTO memories (id, type, content, embedding, created_at)
            VALUES ($1, 'semantic'::memory_type, $2, get_embedding($2), $3)
            """,
            memory2_id,
            "Memory after gap",
            later_time,
        )

        episode2 = await _fetch_episode_for_memory(conn, memory2_id)
        episode2_id = episode2["episode_id"]

        # Verify new episode was created
        assert episode2_id != episode1_id, "New episode should be created after 30-minute gap"

        # Verify old episode was closed
        old_episode = await conn.fetchrow("""
            SELECT ended_at FROM episodes WHERE id = $1
        """, episode1_id)

        assert old_episode['ended_at'] is not None, "Old episode should be closed"


async def test_episode_summary_view(db_pool, ensure_embedding_service):
    """Test episode_summary view calculations"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        # Create episode with summary
        episode_id = await conn.fetchval(
            """
            INSERT INTO episodes (started_at, ended_at, metadata, summary)
            VALUES (
                CURRENT_TIMESTAMP - interval '2 hours',
                CURRENT_TIMESTAMP - interval '1 hour',
                jsonb_build_object('episode_type', 'reflection'),
                'Test episode summary'
            ) RETURNING id
            """
        )

        # Add memories to episode
        for i in range(3):
            memory_id = await conn.fetchval(
                """
                SELECT create_memory('semantic'::memory_type, $1, 0.5)
                """,
                f"Episode summary test memory {i}",
            )

            await conn.execute(
                "SELECT link_memory_to_episode_graph($1::uuid, $2::uuid, $3::int)",
                memory_id,
                episode_id,
                i + 1,
            )

        # Query the view
        summary = await conn.fetchrow("""
            SELECT * FROM episode_summary WHERE id = $1
        """, episode_id)

        assert summary is not None, "Episode should appear in summary view"
        assert summary['memory_count'] == 3, "Should count 3 memories"
        assert summary['episode_type'] == 'reflection'
        assert summary['summary'] == 'Test episode summary'


async def test_episode_time_range_gist_index(db_pool):
    """Test GiST index on episodes.time_range for temporal queries"""
    async with db_pool.acquire() as conn:
        # Create episodes with different time ranges
        for i in range(5):
            await conn.execute(
                """
                INSERT INTO episodes (started_at, ended_at, metadata)
                VALUES (
                    CURRENT_TIMESTAMP - $1 * interval '1 day',
                    CURRENT_TIMESTAMP - $1 * interval '1 day' + interval '1 hour',
                    jsonb_build_object('episode_type', 'autonomous')
                )
                """,
                i,
            )

        # Query using time range overlap - should use GiST index
        result = await conn.fetch("""
            EXPLAIN (FORMAT JSON)
            SELECT * FROM episodes
            WHERE time_range && tstzrange(
                CURRENT_TIMESTAMP - interval '2 days',
                CURRENT_TIMESTAMP
            )
        """)

        # Verify the query plan (index usage)
        plan = result[0]['QUERY PLAN']
        # The query should complete successfully with index available
        assert plan is not None


# -----------------------------------------------------------------------------
# MEMORY NEIGHBORHOODS TESTS
# -----------------------------------------------------------------------------

async def test_memory_neighborhoods_initialization(db_pool):
    """Test that memory_neighborhoods record is created on memory insert"""
    async with db_pool.acquire() as conn:
        # Create a memory
        memory_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'Neighborhood init test',
                    array_fill(0.6, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        # Verify neighborhood record was created by trigger
        neighborhood = await conn.fetchrow("""
            SELECT * FROM memory_neighborhoods WHERE memory_id = $1
        """, memory_id)

        assert neighborhood is not None, "Neighborhood record should be auto-created"
        assert neighborhood['is_stale'] == True, "New neighborhood should be stale"
        # neighbors is JSONB, may be dict or empty object string
        neighbors = neighborhood['neighbors']
        assert neighbors == {} or neighbors == '{}' or len(neighbors) == 0, "Neighbors should be empty initially"


async def test_neighborhoods_staleness_trigger(db_pool):
    """Test trg_neighborhood_staleness marks neighborhoods stale on changes"""
    async with db_pool.acquire() as conn:
        # Create memory
        memory_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'Staleness trigger test',
                    array_fill(0.7, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        # Manually set neighborhood to not stale with some neighbors
        await conn.execute("""
            UPDATE memory_neighborhoods
            SET is_stale = FALSE,
                neighbors = '{"test-uuid": 0.8}'::jsonb,
                computed_at = CURRENT_TIMESTAMP
            WHERE memory_id = $1
        """, memory_id)

        # Verify it's not stale
        is_stale_before = await conn.fetchval("""
            SELECT is_stale FROM memory_neighborhoods WHERE memory_id = $1
        """, memory_id)
        assert is_stale_before == False

        # Update importance - should trigger staleness
        await conn.execute("""
            UPDATE memories SET importance = 0.9 WHERE id = $1
        """, memory_id)

        is_stale_after = await conn.fetchval("""
            SELECT is_stale FROM memory_neighborhoods WHERE memory_id = $1
        """, memory_id)
        assert is_stale_after == True, "Neighborhood should be marked stale after importance change"


async def test_neighborhoods_staleness_on_status_change(db_pool):
    """Test neighborhood becomes stale when memory status changes"""
    async with db_pool.acquire() as conn:
        memory_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'Status change staleness test',
                    array_fill(0.75, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        # Set not stale
        await conn.execute("""
            UPDATE memory_neighborhoods
            SET is_stale = FALSE
            WHERE memory_id = $1
        """, memory_id)

        # Change status
        await conn.execute("""
            UPDATE memories SET status = 'archived' WHERE id = $1
        """, memory_id)

        is_stale = await conn.fetchval("""
            SELECT is_stale FROM memory_neighborhoods WHERE memory_id = $1
        """, memory_id)
        assert is_stale == True, "Neighborhood should be stale after status change"


async def test_stale_neighborhoods_view(db_pool):
    """Test stale_neighborhoods view shows correct memories"""
    async with db_pool.acquire() as conn:
        # Create memories
        stale_memory_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'Stale neighborhood view test',
                    array_fill(0.8, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        fresh_memory_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'Fresh neighborhood view test',
                    array_fill(0.81, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        # Set one as not stale
        await conn.execute("""
            UPDATE memory_neighborhoods SET is_stale = FALSE
            WHERE memory_id = $1
        """, fresh_memory_id)

        # Check view
        stale_records = await conn.fetch("""
            SELECT memory_id FROM stale_neighborhoods
            WHERE memory_id IN ($1, $2)
        """, stale_memory_id, fresh_memory_id)

        stale_ids = [r['memory_id'] for r in stale_records]
        assert stale_memory_id in stale_ids, "Stale memory should appear in view"
        assert fresh_memory_id not in stale_ids, "Fresh memory should not appear in view"


async def test_neighborhoods_gin_index(db_pool):
    """Test GIN index on neighbors JSONB works correctly"""
    async with db_pool.acquire() as conn:
        # Create memory with neighbors
        memory_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'GIN index test',
                    array_fill(0.85, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        # Update with neighbors
        await conn.execute("""
            UPDATE memory_neighborhoods
            SET neighbors = '{"neighbor1": 0.9, "neighbor2": 0.7}'::jsonb
            WHERE memory_id = $1
        """, memory_id)

        # Query using JSONB operators
        result = await conn.fetch("""
            SELECT memory_id FROM memory_neighborhoods
            WHERE neighbors ? 'neighbor1'
        """)

        assert len(result) > 0, "Should find memory with neighbor1 key"


# -----------------------------------------------------------------------------
# ACTIVATION CACHE TESTS
# -----------------------------------------------------------------------------

async def test_activation_cache_operations(db_pool):
    """Test activation_cache basic operations"""
    async with db_pool.acquire() as conn:
        session_id = await conn.fetchval("SELECT gen_random_uuid()")
        memory_id = await conn.fetchval("SELECT gen_random_uuid()")

        # Insert activation
        await conn.execute("""
            INSERT INTO activation_cache (session_id, memory_id, activation_level)
            VALUES ($1, $2, 0.75)
        """, session_id, memory_id)

        # Read back
        activation = await conn.fetchval("""
            SELECT activation_level FROM activation_cache
            WHERE session_id = $1 AND memory_id = $2
        """, session_id, memory_id)

        assert activation == 0.75

        # Update activation
        await conn.execute("""
            INSERT INTO activation_cache (session_id, memory_id, activation_level)
            VALUES ($1, $2, 0.9)
            ON CONFLICT (session_id, memory_id)
            DO UPDATE SET activation_level = EXCLUDED.activation_level
        """, session_id, memory_id)

        updated = await conn.fetchval("""
            SELECT activation_level FROM activation_cache
            WHERE session_id = $1 AND memory_id = $2
        """, session_id, memory_id)

        assert updated == 0.9


async def test_activation_cache_session_isolation(db_pool):
    """Test activation levels are isolated by session_id"""
    async with db_pool.acquire() as conn:
        session1_id = await conn.fetchval("SELECT gen_random_uuid()")
        session2_id = await conn.fetchval("SELECT gen_random_uuid()")
        memory_id = await conn.fetchval("SELECT gen_random_uuid()")

        # Insert different activations for same memory in different sessions
        await conn.execute("""
            INSERT INTO activation_cache (session_id, memory_id, activation_level)
            VALUES ($1, $2, 0.3), ($3, $2, 0.8)
        """, session1_id, memory_id, session2_id)

        # Verify isolation
        session1_activation = await conn.fetchval("""
            SELECT activation_level FROM activation_cache
            WHERE session_id = $1 AND memory_id = $2
        """, session1_id, memory_id)

        session2_activation = await conn.fetchval("""
            SELECT activation_level FROM activation_cache
            WHERE session_id = $1 AND memory_id = $2
        """, session2_id, memory_id)

        assert session1_activation == 0.3
        assert session2_activation == 0.8


# -----------------------------------------------------------------------------
# CONCEPTS LAYER TESTS
# -----------------------------------------------------------------------------
# Note: concepts and memory_concepts tables removed in Phase 2 (ReduceScopeCreep)
# Concepts are now stored entirely in the graph as ConceptNode vertices.


async def test_link_memory_to_concept_creates_graph_edge(db_pool):
    """Test link_memory_to_concept() creates ConceptNode and INSTANCE_OF edge in graph.
    Phase 2 (ReduceScopeCreep): Concepts are now graph-only.
    """
    async with db_pool.acquire() as conn:
        # Create memory with graph node
        memory_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'Cats are independent',
                    array_fill(0.88, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        # Create graph node for memory
        await conn.execute("""
            LOAD 'age';
            SET search_path = ag_catalog, public;
        """)

        await conn.execute(f"""
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                CREATE (n:MemoryNode {{memory_id: '{memory_id}', type: 'semantic'}})
                RETURN n
            $$) as (result agtype)
        """)

        await conn.execute("SET search_path = public, ag_catalog")

        # Link to concept using function (now returns boolean)
        result = await conn.fetchval("""
            SELECT link_memory_to_concept($1, 'Independence', 0.85)
        """, memory_id)

        assert result is True, "link_memory_to_concept should return true on success"

        # Verify graph edge (INSTANCE_OF) exists
        await conn.execute("""
            LOAD 'age';
            SET search_path = ag_catalog, public;
        """)

        edge_result = await conn.fetch(f"""
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (m:MemoryNode {{memory_id: '{memory_id}'}})-[r:INSTANCE_OF]->(c:ConceptNode)
                RETURN c.name as concept_name, r.strength as strength
            $$) as (concept_name agtype, strength agtype)
        """)

        await conn.execute("SET search_path = public, ag_catalog")

        assert len(edge_result) > 0, "INSTANCE_OF edge should exist in graph"
        # Note: AGE returns agtype which includes quotes
        assert "Independence" in str(edge_result[0]["concept_name"])


async def test_create_concept_sets_description_and_depth(db_pool):
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        concept_name = f"Concept_{get_test_identifier('create_concept')}"
        result = await conn.fetchval(
            "SELECT create_concept($1, $2, $3)",
            concept_name,
            "Concept description",
            2,
        )
        assert result is True

        row = await conn.fetchrow(
            """
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (c:ConceptNode {name: '%s'})
                RETURN c.description, c.depth
            $$) as (description agtype, depth agtype)
            """
            % concept_name
        )
        assert row is not None
        desc = str(row["description"]).strip('"')
        depth = int(str(row["depth"]).strip('"'))
        assert desc == "Concept description"
        assert depth == 2

        await conn.execute("SET search_path = public, ag_catalog;")


async def test_link_concept_parent_creates_edge(db_pool):
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        child_name = f"Child_{get_test_identifier('concept_child')}"
        parent_name = f"Parent_{get_test_identifier('concept_parent')}"
        result = await conn.fetchval("SELECT link_concept_parent($1, $2)", child_name, parent_name)
        assert result is True

        row = await conn.fetchrow(
            """
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (p:ConceptNode {name: '%s'})-[:PARENT_OF]->(c:ConceptNode {name: '%s'})
                RETURN c.name
            $$) as (name agtype)
            """
            % (parent_name, child_name)
        )
        assert row is not None
        assert child_name in str(row["name"])

        await conn.execute("SET search_path = public, ag_catalog;")


# -----------------------------------------------------------------------------
# FAST_RECALL FUNCTION TESTS
# -----------------------------------------------------------------------------

async def test_fast_recall_basic(db_pool, ensure_embedding_service):
    """Test fast_recall() primary retrieval function"""
    async with db_pool.acquire() as conn:
        # Create test memories with distinct content
        memory_ids = []
        contents = [
            'The weather today is sunny and warm',
            'Python is a programming language',
            'The sun provides light and warmth',
            'JavaScript runs in browsers'
        ]

        for content in contents:
            memory_id = await conn.fetchval("""
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic'::memory_type, $1,
                        array_fill(0.5, ARRAY[embedding_dimension()])::vector)
                RETURNING id
            """, content)
            memory_ids.append(memory_id)

        results = await conn.fetch("""
            SELECT * FROM fast_recall('What is the weather like?', 5)
        """)

        assert all('memory_id' in dict(r) for r in results)
        assert all('content' in dict(r) for r in results)
        assert all('score' in dict(r) for r in results)
        assert all('source' in dict(r) for r in results)


async def test_fast_recall_respects_limit(db_pool, ensure_embedding_service):
    """Test fast_recall respects the limit parameter"""
    async with db_pool.acquire() as conn:
        # Create multiple memories
        for i in range(10):
            await conn.execute("""
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic'::memory_type, $1,
                        array_fill(0.5, ARRAY[embedding_dimension()])::vector)
            """, f'Fast recall limit test memory {i}')

        results = await conn.fetch("""
            SELECT * FROM fast_recall('test memory', 3)
        """)
        assert len(results) <= 3, "Should respect limit parameter"


async def test_fast_recall_only_active_memories(db_pool, ensure_embedding_service):
    """Test fast_recall only returns active memories"""
    async with db_pool.acquire() as conn:
        # Create active and archived memories
        active_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding, status)
            VALUES ('semantic'::memory_type, 'Active memory for recall test',
                    array_fill(0.55, ARRAY[embedding_dimension()])::vector, 'active')
            RETURNING id
        """)

        archived_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding, status)
            VALUES ('semantic'::memory_type, 'Archived memory for recall test',
                    array_fill(0.55, ARRAY[embedding_dimension()])::vector, 'archived')
            RETURNING id
        """)

        results = await conn.fetch("""
            SELECT memory_id FROM fast_recall('recall test', 10)
        """)

        result_ids = [r['memory_id'] for r in results]

        # Active should potentially be returned, archived should not
        assert archived_id not in result_ids, "Archived memories should not be returned"


async def test_fast_recall_source_attribution(db_pool, ensure_embedding_service):
    """Test fast_recall correctly attributes retrieval sources"""
    async with db_pool.acquire() as conn:
        # The source field should be one of: 'vector', 'association', 'temporal', 'fallback'
        results = await conn.fetch("""
            SELECT source FROM fast_recall('test query', 5)
        """)

        valid_sources = {'vector', 'association', 'temporal', 'fallback'}
        for result in results:
            assert result['source'] in valid_sources, f"Invalid source: {result['source']}"


async def test_fast_recall_respects_min_trust_level(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("SELECT set_config('memory.recall_min_trust_level', '0.7'::jsonb)")
            low_id = await conn.fetchval(
                "SELECT create_semantic_memory($1, 0.9, ARRAY['test'], NULL, '{}'::jsonb, 0.5, NULL, 0.2)",
                "Low trust memory",
            )
            high_id = await conn.fetchval(
                "SELECT create_semantic_memory($1, 0.9, ARRAY['test'], NULL, '{}'::jsonb, 0.5, NULL, 0.9)",
                "High trust memory",
            )
            await conn.execute(
                """
                UPDATE memories
                SET embedding = get_embedding('trust memory')
                WHERE id = ANY($1::uuid[])
                """,
                [low_id, high_id],
            )
            rows = await conn.fetch("SELECT memory_id FROM fast_recall('trust memory', 10)")
            ids = {str(r['memory_id']) for r in rows}
            assert str(low_id) not in ids
            if ids:
                trust_rows = await conn.fetch(
                    "SELECT trust_level FROM memories WHERE id = ANY($1::uuid[])",
                    list(ids),
                )
                assert all(float(r["trust_level"]) >= 0.7 for r in trust_rows)
        finally:
            await tr.rollback()


# -----------------------------------------------------------------------------
# MEMORY RETRIEVAL HELPERS
# -----------------------------------------------------------------------------

async def test_get_memory_by_id_returns_expected_fields(db_pool):
    async with db_pool.acquire() as conn:
        memory_id = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding, importance, trust_level, source_attribution, metadata)
            VALUES ('semantic'::memory_type, $1, array_fill(0.25, ARRAY[embedding_dimension()])::vector, 0.6, 0.8,
                    jsonb_build_object('kind', 'test'), jsonb_build_object('emotional_valence', 0.4))
            RETURNING id
            """,
            f"Memory by id {get_test_identifier('memory_by_id')}",
        )

        row = await conn.fetchrow("SELECT * FROM get_memory_by_id($1)", memory_id)
        assert row["id"] == memory_id
        assert row["type"] == "semantic"
        assert row["importance"] == 0.6
        assert row["trust_level"] == 0.8
        source_attr = _coerce_json(row["source_attribution"])
        assert source_attr["kind"] == "test"
        assert row["emotional_valence"] == 0.4


async def test_get_memories_summary_handles_empty_inputs(db_pool):
    async with db_pool.acquire() as conn:
        rows = await conn.fetch("SELECT * FROM get_memories_summary(NULL)")
        assert rows == []
        rows = await conn.fetch("SELECT * FROM get_memories_summary('{}'::uuid[])")
        assert rows == []


async def test_get_memories_summary_returns_rows(db_pool):
    async with db_pool.acquire() as conn:
        id_one = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding, importance)
            VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector, 0.3)
            RETURNING id
            """,
            f"Summary one {get_test_identifier('summary_one')}",
        )
        id_two = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding, importance)
            VALUES ('episodic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector, 0.7)
            RETURNING id
            """,
            f"Summary two {get_test_identifier('summary_two')}",
        )

        rows = await conn.fetch("SELECT * FROM get_memories_summary($1::uuid[])", [id_one, id_two])
        ids = {row["id"] for row in rows}
        assert id_one in ids
        assert id_two in ids


async def test_list_recent_memories_orders_by_created_at(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            older_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, created_at)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector,
                        CURRENT_TIMESTAMP + interval '10 years')
                RETURNING id
                """,
                f"Older {get_test_identifier('recent_created_older')}",
            )
            newer_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, created_at)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector,
                        CURRENT_TIMESTAMP + interval '11 years')
                RETURNING id
                """,
                f"Newer {get_test_identifier('recent_created_newer')}",
            )
            rows = await conn.fetch("SELECT memory_id FROM list_recent_memories(2)")
            assert rows[0]["memory_id"] == newer_id
            assert rows[1]["memory_id"] == older_id
        finally:
            await tr.rollback()


async def test_list_recent_memories_orders_by_access_when_requested(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            first_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, last_accessed)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector,
                        CURRENT_TIMESTAMP + interval '10 years')
                RETURNING id
                """,
                f"Access older {get_test_identifier('recent_access_older')}",
            )
            second_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, last_accessed)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector,
                        CURRENT_TIMESTAMP + interval '11 years')
                RETURNING id
                """,
                f"Access newer {get_test_identifier('recent_access_newer')}",
            )

            rows = await conn.fetch("SELECT memory_id FROM list_recent_memories(2, NULL, TRUE)")
            assert rows[0]["memory_id"] == second_id
            assert rows[1]["memory_id"] == first_id
        finally:
            await tr.rollback()


async def test_recall_memories_filtered_filters_type_and_importance(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("recall_filtered")
            query_text = f"Recall filter {test_id}"
            high_semantic_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, importance, metadata)
                VALUES ('semantic'::memory_type, $1, get_embedding($2), 0.8,
                        jsonb_build_object('emotional_valence', 0.4))
                RETURNING id
                """,
                f"{query_text} semantic high",
                query_text,
            )
            low_semantic_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, importance)
                VALUES ('semantic'::memory_type, $1, get_embedding($2), 0.2)
                RETURNING id
                """,
                f"{query_text} semantic low",
                query_text,
            )
            episodic_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, importance)
                VALUES ('episodic'::memory_type, $1, get_embedding($2), 0.9)
                RETURNING id
                """,
                f"{query_text} episodic",
                query_text,
            )

            rows = await conn.fetch(
                """
                SELECT * FROM recall_memories_filtered(
                    $1,
                    5,
                    ARRAY['semantic']::memory_type[],
                    0.5
                )
                """,
                query_text,
            )
            ids = {str(row["memory_id"]) for row in rows}
            assert str(high_semantic_id) in ids
            assert str(low_semantic_id) not in ids
            assert str(episodic_id) not in ids
            if rows:
                for row in rows:
                    if row["memory_id"] == high_semantic_id:
                        assert row["emotional_valence"] == 0.4
                        break
        finally:
            await tr.rollback()


async def test_get_memory_neighborhoods_returns_rows(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            mem_one = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Neighborhood one {get_test_identifier('neighborhood_one')}",
            )
            mem_two = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic'::memory_type, $1, array_fill(0.2, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"Neighborhood two {get_test_identifier('neighborhood_two')}",
            )
            await conn.execute(
                """
                INSERT INTO memory_neighborhoods (memory_id, neighbors, computed_at, is_stale)
                VALUES ($1, jsonb_build_object($2::text, '0.7'), CURRENT_TIMESTAMP, FALSE)
                ON CONFLICT (memory_id) DO UPDATE SET
                    neighbors = EXCLUDED.neighbors,
                    computed_at = EXCLUDED.computed_at,
                    is_stale = EXCLUDED.is_stale
                """,
                mem_one,
                str(mem_two),
            )
            rows = await conn.fetch(
                "SELECT * FROM get_memory_neighborhoods($1::uuid[])",
                [mem_one, mem_two],
            )
            by_id = {row["memory_id"]: row["neighbors"] for row in rows}
            assert mem_one in by_id
            assert str(mem_two) in by_id[mem_one]
        finally:
            await tr.rollback()


async def test_get_memory_neighborhoods_empty_input(db_pool):
    async with db_pool.acquire() as conn:
        rows = await conn.fetch("SELECT * FROM get_memory_neighborhoods(NULL)")
        assert rows == []
        rows = await conn.fetch("SELECT * FROM get_memory_neighborhoods('{}'::uuid[])")
        assert rows == []


# -----------------------------------------------------------------------------
# SEARCH FUNCTIONS TESTS
# -----------------------------------------------------------------------------

async def test_search_similar_memories_type_filter(db_pool, ensure_embedding_service):
    """Test search_similar_memories with type filtering"""
    async with db_pool.acquire() as conn:
        # Create memories of different types
        await conn.execute("""
            INSERT INTO memories (type, content, embedding)
            VALUES
                ('semantic'::memory_type, 'Semantic search test', array_fill(0.6, ARRAY[embedding_dimension()])::vector),
                ('episodic'::memory_type, 'Episodic search test', array_fill(0.6, ARRAY[embedding_dimension()])::vector),
                ('procedural'::memory_type, 'Procedural search test', array_fill(0.6, ARRAY[embedding_dimension()])::vector)
        """)

        # Search only semantic
        results = await conn.fetch("""
            SELECT * FROM search_similar_memories(
                'search test', 10, ARRAY['semantic']::memory_type[]
            )
        """)

        for r in results:
            if 'search test' in r['content'].lower():
                assert r['type'] == 'semantic', "Should only return semantic type"


async def test_search_similar_memories_importance_filter(db_pool, ensure_embedding_service):
    """Test search_similar_memories with minimum importance filter"""
    async with db_pool.acquire() as conn:
        # Create memories with different importance
        await conn.execute("""
            INSERT INTO memories (type, content, embedding, importance)
            VALUES
                ('semantic'::memory_type, 'Low importance search test',
                 array_fill(0.65, ARRAY[embedding_dimension()])::vector, 0.1),
                ('semantic'::memory_type, 'High importance search test',
                 array_fill(0.65, ARRAY[embedding_dimension()])::vector, 0.9)
        """)

        # Search with high minimum importance
        results = await conn.fetch("""
            SELECT * FROM search_similar_memories(
                'importance search test', 10, NULL, 0.5
            )
        """)

        for r in results:
            if 'importance search test' in r['content'].lower():
                assert r['importance'] >= 0.5, "Should only return high importance memories"


async def test_search_working_memory_auto_cleanup(db_pool, ensure_embedding_service):
    """Test search_working_memory calls cleanup automatically"""
    async with db_pool.acquire() as conn:
        # Add expired working memory entry
        await conn.execute("""
            INSERT INTO working_memory (content, embedding, expiry)
            VALUES ('Expired working memory', array_fill(0.7, ARRAY[embedding_dimension()])::vector,
                    CURRENT_TIMESTAMP - interval '1 hour')
        """)

        # Add valid working memory entry
        await conn.execute("""
            INSERT INTO working_memory (content, embedding, expiry)
            VALUES ('Valid working memory', array_fill(0.7, ARRAY[embedding_dimension()])::vector,
                    CURRENT_TIMESTAMP + interval '1 hour')
        """)

        # Search triggers cleanup
        await conn.fetch("""
            SELECT * FROM search_working_memory('working memory', 5)
        """)

        # Verify expired entry was cleaned up
        expired_count = await conn.fetchval("""
            SELECT COUNT(*) FROM working_memory
            WHERE expiry < CURRENT_TIMESTAMP
        """)

        assert expired_count == 0, "Expired working memory should be cleaned up"


# -----------------------------------------------------------------------------
# MEMORY CREATION FUNCTIONS TESTS
# -----------------------------------------------------------------------------

async def test_create_episodic_memory_function(db_pool, ensure_embedding_service):
    """Test create_episodic_memory() full workflow"""
    async with db_pool.acquire() as conn:
        memory_id = await conn.fetchval("""
            SELECT create_episodic_memory(
                'Had a great conversation about AI',
                '{"type": "conversation"}'::jsonb,
                '{"location": "office", "participants": ["Alice", "Bob"]}'::jsonb,
                '{"outcome": "learned new concepts"}'::jsonb,
                0.8,
                CURRENT_TIMESTAMP,
                0.75
            )
        """)

        # Verify base memory
        memory = await conn.fetchrow("""
            SELECT * FROM memories WHERE id = $1
        """, memory_id)
        assert memory['type'] == 'episodic'
        assert memory['importance'] == 0.75

        # Verify episodic details from metadata
        metadata = memory['metadata']
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        assert metadata is not None
        assert metadata['emotional_valence'] == 0.8


async def test_create_semantic_memory_function(db_pool, ensure_embedding_service):
    """Test create_semantic_memory() with all parameters"""
    async with db_pool.acquire() as conn:
        memory_id = await conn.fetchval("""
            SELECT create_semantic_memory(
                'Water boils at 100 degrees Celsius at sea level',
                0.99,
                ARRAY['physics', 'chemistry'],
                ARRAY['water', 'temperature', 'boiling'],
                '{"textbook": "Physics 101"}'::jsonb,
                0.8
            )
        """)

        # Verify semantic details from metadata
        mem = await conn.fetchrow("""
            SELECT metadata FROM memories WHERE id = $1
        """, memory_id)
        assert mem is not None
        metadata = mem['metadata']
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        assert metadata['confidence'] == 0.99
        assert 'physics' in metadata['category']
        assert 'water' in metadata['related_concepts']


async def test_semantic_memory_trust_is_capped_by_sources(db_pool):
    """Untrusted single-source claims should not become "true" without reinforcement."""
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("trust_sources")
            source_a = {"kind": "twitter", "ref": f"https://twitter.com/example/status/{test_id}", "trust": 0.2}
            source_b = {"kind": "paper", "ref": f"doi:10.0000/{test_id}", "trust": 0.9}

            mem_id = await conn.fetchval(
                """
                SELECT create_semantic_memory(
                    $1::text,
                    0.95::float,
                    NULL,
                    NULL,
                    $2::jsonb,
                    0.6::float
                )
                """,
                f"Claim from twitter {test_id}",
                json.dumps(source_a),
            )

            row = await conn.fetchrow(
                "SELECT trust_level, source_attribution FROM memories WHERE id = $1::uuid",
                mem_id,
            )
            assert row is not None
            trust_before = float(row["trust_level"])
            assert trust_before <= 0.30
            src = _coerce_json(row["source_attribution"])
            assert isinstance(src, dict)
            assert src.get("kind") in {"twitter", "unattributed", "internal"}

            await conn.execute(
                "SELECT add_semantic_source_reference($1::uuid, $2::jsonb)",
                mem_id,
                json.dumps(source_b),
            )
            row2 = await conn.fetchrow("SELECT trust_level FROM memories WHERE id = $1::uuid", mem_id)
            assert row2 is not None
            trust_after = float(row2["trust_level"])
            assert trust_after > trust_before
        finally:
            await tr.rollback()


async def test_worldview_misalignment_can_reduce_semantic_trust(db_pool):
    """Explicit worldview misalignment should down-weight trust in a claim."""
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("trust_worldview")
            sources = [
                {"kind": "paper", "ref": f"doi:10.0000/{test_id}-a", "trust": 0.9},
                {"kind": "paper", "ref": f"doi:10.0000/{test_id}-b", "trust": 0.9},
            ]

            mem_id = await conn.fetchval(
                "SELECT create_semantic_memory($1::text, 0.9::float, NULL, NULL, $2::jsonb, 0.6::float)",
                f"Well-sourced claim {test_id}",
                json.dumps(sources),
            )
            trust_before = float(await conn.fetchval("SELECT trust_level FROM memories WHERE id = $1::uuid", mem_id))
            assert trust_before > 0.2

            w_id = await conn.fetchval(
                "SELECT create_worldview_memory($1, 'belief', 1.0, 0.9, 0.9, 'test')",
                f"Christian theology baseline {test_id}",
            )

            await conn.execute(
                "SELECT create_memory_relationship($1::uuid, $2::uuid, 'CONTRADICTS'::graph_edge_type, $3::jsonb)",
                mem_id,
                w_id,
                json.dumps({"strength": 1.0}),
            )
            await conn.execute("SELECT sync_memory_trust($1::uuid)", mem_id)

            trust_after = float(await conn.fetchval("SELECT trust_level FROM memories WHERE id = $1::uuid", mem_id))
            assert trust_after < trust_before
        finally:
            await tr.rollback()


async def test_get_memory_truth_profile_semantic_and_nonsemantic(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        sources = [
            {"kind": "paper", "ref": "doi:10.0000/alpha", "trust": 0.9},
            {"kind": "paper", "ref": "doi:10.0000/beta", "trust": 0.8},
        ]
        semantic_id = await conn.fetchval(
            "SELECT create_semantic_memory($1, 0.85, NULL, NULL, $2::jsonb, 0.6)",
            f"Truth profile {get_test_identifier('truth_semantic')}",
            json.dumps(sources),
        )
        semantic_profile = _coerce_json(
            await conn.fetchval("SELECT get_memory_truth_profile($1)", semantic_id)
        )
        assert semantic_profile["type"] == "semantic"
        assert semantic_profile["source_count"] == 2
        assert 0.0 <= float(semantic_profile["trust_level"]) <= 1.0

        episodic_id = await conn.fetchval(
            """
            SELECT create_episodic_memory(
                $1,
                '{"action": "note"}',
                '{"context": "truth_profile"}',
                '{"result": "logged"}',
                0.2
            )
            """,
            f"Truth profile episodic {get_test_identifier('truth_epi')}",
        )
        episodic_profile = _coerce_json(
            await conn.fetchval("SELECT get_memory_truth_profile($1)", episodic_id)
        )
        assert episodic_profile["type"] == "episodic"
        assert "source_count" not in episodic_profile


async def test_create_worldview_belief_sets_sources_and_threshold(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        sources = [
            {"kind": "paper", "ref": "doi:10.0000/worldview-a", "trust": 0.9},
            {"kind": "paper", "ref": "doi:10.0000/worldview-b", "trust": 0.8},
        ]
        belief_id = await conn.fetchval(
            """
            SELECT create_worldview_belief(
                $1,
                'belief',
                0.9,
                0.7,
                0.8,
                'test',
                0.65,
                0.1,
                ARRAY['alpha', 'beta'],
                'respond',
                $2::jsonb
            )
            """,
            f"Worldview belief {get_test_identifier('worldview_belief')}",
            json.dumps(sources),
        )
        row = await conn.fetchrow(
            "SELECT metadata, source_attribution FROM memories WHERE id = $1",
            belief_id,
        )
        metadata = _coerce_json(row["metadata"])
        assert abs(float(metadata["evidence_threshold"]) - 0.65) < 0.001
        assert metadata["trigger_patterns"] == ["alpha", "beta"]
        assert len(metadata["source_references"]) == 2
        source_attr = _coerce_json(row["source_attribution"])
        assert source_attr["ref"].startswith("doi:10.0000/")


async def test_update_identity_belief_respects_stability_and_force(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            worldview_id = await conn.fetchval(
                "SELECT create_worldview_memory($1, 'self', 0.8, 0.95, 0.8, 'test')",
                f"Initial identity {get_test_identifier('identity')}",
            )
            evidence_id = await conn.fetchval(
                """
                SELECT create_episodic_memory(
                    $1,
                    '{"action": "evidence"}',
                    '{"context": "identity"}',
                    '{"result": "observed"}',
                    0.1
                )
                """,
                f"Identity evidence {get_test_identifier('identity_evidence')}",
            )

            result = await conn.fetchval(
                "SELECT update_identity_belief($1, $2, $3, FALSE)",
                worldview_id,
                "Updated identity content",
                evidence_id,
            )
            assert result is False

            content_before = await conn.fetchval(
                "SELECT content FROM memories WHERE id = $1",
                worldview_id,
            )
            assert "Updated identity content" not in content_before

            strategic_count = await conn.fetchval(
                "SELECT COUNT(*) FROM memories WHERE type = 'strategic' AND content = 'Identity belief challenged but stable'"
            )
            assert int(strategic_count) >= 1

            result_force = await conn.fetchval(
                "SELECT update_identity_belief($1, $2, $3, TRUE)",
                worldview_id,
                "Updated identity content",
                evidence_id,
            )
            assert result_force is True
            content_after = await conn.fetchval(
                "SELECT content FROM memories WHERE id = $1",
                worldview_id,
            )
            assert content_after == "Updated identity content"
        finally:
            await tr.rollback()


async def test_create_procedural_memory_function(db_pool, ensure_embedding_service):
    """Test create_procedural_memory() with steps"""
    async with db_pool.acquire() as conn:
        memory_id = await conn.fetchval("""
            SELECT create_procedural_memory(
                'How to make coffee',
                '{"steps": ["Boil water", "Add coffee grounds", "Pour water", "Wait 4 minutes", "Press and pour"]}'::jsonb,
                '{"required": ["coffee maker", "coffee grounds", "water"]}'::jsonb,
                0.6
            )
        """)

        # Verify procedural details from metadata
        mem = await conn.fetchrow("""
            SELECT metadata FROM memories WHERE id = $1
        """, memory_id)
        assert mem is not None
        metadata = mem['metadata']
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        assert metadata['steps'] is not None
        assert metadata['prerequisites'] is not None


async def test_create_strategic_memory_function(db_pool, ensure_embedding_service):
    """Test create_strategic_memory() with pattern and evidence"""
    async with db_pool.acquire() as conn:
        memory_id = await conn.fetchval("""
            SELECT create_strategic_memory(
                'Users prefer simple interfaces',
                'Simplicity leads to higher engagement',
                0.85,
                '{"studies": ["Nielsen 2020", "UX Research 2021"]}'::jsonb,
                '{"domains": ["web", "mobile", "desktop"]}'::jsonb,
                0.9
            )
        """)

        # Verify strategic details from metadata
        mem = await conn.fetchrow("""
            SELECT metadata FROM memories WHERE id = $1
        """, memory_id)
        assert mem is not None
        metadata = mem['metadata']
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        assert metadata['pattern_description'] == 'Simplicity leads to higher engagement'
        assert metadata['confidence_score'] == 0.85


# -----------------------------------------------------------------------------
# GRAPH EDGE TYPES TESTS
# -----------------------------------------------------------------------------

async def test_temporal_next_edge(db_pool):
    """Test TEMPORAL_NEXT edge for narrative sequence"""
    async with db_pool.acquire() as conn:
        # Create two memories
        memory1_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('episodic'::memory_type, 'First event', array_fill(0.1, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        memory2_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('episodic'::memory_type, 'Second event', array_fill(0.2, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        # Create graph nodes
        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        for mid, mtype in [(memory1_id, 'episodic'), (memory2_id, 'episodic')]:
            await conn.execute(f"""
                SELECT * FROM ag_catalog.cypher('memory_graph', $$
                    CREATE (n:MemoryNode {{memory_id: '{mid}', type: '{mtype}'}})
                    RETURN n
                $$) as (result agtype)
            """)

        await conn.execute("SET search_path = public, ag_catalog")

        # Create TEMPORAL_NEXT relationship
        await conn.execute("""
            SELECT create_memory_relationship($1, $2, 'TEMPORAL_NEXT'::graph_edge_type, '{"sequence": 1}'::jsonb)
        """, memory1_id, memory2_id)

        # Verify edge exists
        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        result = await conn.fetch(f"""
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (a:MemoryNode)-[r:TEMPORAL_NEXT]->(b:MemoryNode)
                WHERE a.memory_id = '{memory1_id}' AND b.memory_id = '{memory2_id}'
                RETURN r
            $$) as (result agtype)
        """)

        await conn.execute("SET search_path = public, ag_catalog")
        assert len(result) > 0, "TEMPORAL_NEXT edge should exist"


async def test_causes_edge(db_pool):
    """Test CAUSES edge for causal reasoning"""
    async with db_pool.acquire() as conn:
        cause_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('episodic'::memory_type, 'Rain started', array_fill(0.3, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        effect_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('episodic'::memory_type, 'Ground became wet', array_fill(0.4, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        for mid in [cause_id, effect_id]:
            await conn.execute(f"""
                SELECT * FROM ag_catalog.cypher('memory_graph', $$
                    CREATE (n:MemoryNode {{memory_id: '{mid}', type: 'episodic'}})
                    RETURN n
                $$) as (result agtype)
            """)

        await conn.execute("SET search_path = public, ag_catalog")

        await conn.execute("""
            SELECT create_memory_relationship($1, $2, 'CAUSES'::graph_edge_type, '{"confidence": 0.95}'::jsonb)
        """, cause_id, effect_id)

        # Verify causal chain query works
        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        result = await conn.fetch(f"""
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (cause:MemoryNode)-[:CAUSES]->(effect:MemoryNode)
                WHERE cause.memory_id = '{cause_id}'
                RETURN effect.memory_id as effect_id
            $$) as (effect_id agtype)
        """)

        await conn.execute("SET search_path = public, ag_catalog")
        assert len(result) > 0, "CAUSES edge should exist"


async def test_contradicts_edge(db_pool):
    """Test CONTRADICTS edge for dialectical tension"""
    async with db_pool.acquire() as conn:
        claim1_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'The sky is blue', array_fill(0.5, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        claim2_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'The sky is not blue', array_fill(0.6, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        for mid in [claim1_id, claim2_id]:
            await conn.execute(f"""
                SELECT * FROM ag_catalog.cypher('memory_graph', $$
                    CREATE (n:MemoryNode {{memory_id: '{mid}', type: 'semantic'}})
                    RETURN n
                $$) as (result agtype)
            """)

        await conn.execute("SET search_path = public, ag_catalog")

        # Create bidirectional contradiction
        await conn.execute("""
            SELECT create_memory_relationship($1, $2, 'CONTRADICTS'::graph_edge_type, '{}'::jsonb)
        """, claim1_id, claim2_id)

        await conn.execute("""
            SELECT create_memory_relationship($1, $2, 'CONTRADICTS'::graph_edge_type, '{}'::jsonb)
        """, claim2_id, claim1_id)

        # Query contradictions
        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        result = await conn.fetch(f"""
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (a:MemoryNode)-[:CONTRADICTS]->(b:MemoryNode)
                WHERE a.memory_id = '{claim1_id}'
                RETURN b.memory_id as contradicting_id
            $$) as (contradicting_id agtype)
        """)

        await conn.execute("SET search_path = public, ag_catalog")
        assert len(result) > 0, "CONTRADICTS edge should exist"


async def test_supports_edge(db_pool):
    """Test SUPPORTS edge for evidence relationship"""
    async with db_pool.acquire() as conn:
        evidence_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('episodic'::memory_type, 'Experiment showed X', array_fill(0.7, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        claim_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'Theory X is correct', array_fill(0.8, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        for mid, mtype in [(evidence_id, 'episodic'), (claim_id, 'semantic')]:
            await conn.execute(f"""
                SELECT * FROM ag_catalog.cypher('memory_graph', $$
                    CREATE (n:MemoryNode {{memory_id: '{mid}', type: '{mtype}'}})
                    RETURN n
                $$) as (result agtype)
            """)

        await conn.execute("SET search_path = public, ag_catalog")

        await conn.execute("""
            SELECT create_memory_relationship($1, $2, 'SUPPORTS'::graph_edge_type, '{"strength": 0.9}'::jsonb)
        """, evidence_id, claim_id)

        # Verify
        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        result = await conn.fetch(f"""
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (evidence:MemoryNode)-[:SUPPORTS]->(claim:MemoryNode)
                WHERE claim.memory_id = '{claim_id}'
                RETURN evidence.memory_id as evidence_id
            $$) as (evidence_id agtype)
        """)

        await conn.execute("SET search_path = public, ag_catalog")
        assert len(result) > 0, "SUPPORTS edge should exist"


async def test_derived_from_edge(db_pool):
    """Test DERIVED_FROM edge for episodic->semantic transformation"""
    async with db_pool.acquire() as conn:
        episodic_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('episodic'::memory_type, 'Saw bird fly', array_fill(0.85, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        semantic_id = await conn.fetchval("""
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic'::memory_type, 'Birds can fly', array_fill(0.86, ARRAY[embedding_dimension()])::vector)
            RETURNING id
        """)

        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        for mid, mtype in [(episodic_id, 'episodic'), (semantic_id, 'semantic')]:
            await conn.execute(f"""
                SELECT * FROM ag_catalog.cypher('memory_graph', $$
                    CREATE (n:MemoryNode {{memory_id: '{mid}', type: '{mtype}'}})
                    RETURN n
                $$) as (result agtype)
            """)

        await conn.execute("SET search_path = public, ag_catalog")

        await conn.execute("""
            SELECT create_memory_relationship($1, $2, 'DERIVED_FROM'::graph_edge_type, '{}'::jsonb)
        """, semantic_id, episodic_id)

        # Verify derivation chain
        await conn.execute("LOAD 'age'; SET search_path = ag_catalog, public;")

        result = await conn.fetch(f"""
            SELECT * FROM ag_catalog.cypher('memory_graph', $$
                MATCH (semantic:MemoryNode)-[:DERIVED_FROM]->(episodic:MemoryNode)
                WHERE semantic.memory_id = '{semantic_id}'
                RETURN episodic.memory_id as source_id
            $$) as (source_id agtype)
        """)

        await conn.execute("SET search_path = public, ag_catalog")
        assert len(result) > 0, "DERIVED_FROM edge should exist"


# -----------------------------------------------------------------------------
# MAINTENANCE FUNCTIONS TESTS
# -----------------------------------------------------------------------------

async def test_cleanup_working_memory_returns_count(db_pool):
    """Test cleanup_working_memory() returns deletion stats"""
    async with db_pool.acquire() as conn:
        # Use unique content identifier
        unique_id = f'cleanup_test_{uuid.uuid4().hex[:8]}'

        # Clear existing expired entries first
        await conn.execute("""
            DELETE FROM working_memory WHERE expiry < CURRENT_TIMESTAMP
        """)

        # Add expired entries
        for i in range(5):
            await conn.execute("""
                INSERT INTO working_memory (content, embedding, expiry)
                VALUES ($1, array_fill(0.9, ARRAY[embedding_dimension()])::vector,
                        CURRENT_TIMESTAMP - interval '1 hour')
            """, f'Expired entry {unique_id} {i}')

        # Add valid entry
        await conn.execute("""
            INSERT INTO working_memory (content, embedding, expiry)
            VALUES ($1, array_fill(0.9, ARRAY[embedding_dimension()])::vector,
                    CURRENT_TIMESTAMP + interval '1 hour')
        """, f'Valid entry {unique_id}')

        # Call cleanup
        stats = await conn.fetchval("""
            SELECT cleanup_working_memory()
        """)
        if isinstance(stats, str):
            stats = json.loads(stats)

        deleted_count = stats["deleted_count"]
        assert deleted_count >= 5, f"Should delete at least 5 expired entries, got {deleted_count}"

        # Verify valid entry remains
        remaining = await conn.fetchval("""
            SELECT COUNT(*) FROM working_memory WHERE content = $1
        """, f'Valid entry {unique_id}')
        assert remaining == 1


async def test_cleanup_working_memory_promotes_marked_items(db_pool):
    """Expired working memory marked for promotion becomes an episodic memory before deletion."""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("wm_promote")
        wid = await conn.fetchval(
            """
            INSERT INTO working_memory (content, embedding, expiry, promote_to_long_term, importance)
            VALUES ($1, array_fill(0.2::float, ARRAY[embedding_dimension()])::vector, NOW() - INTERVAL '1 hour', TRUE, 0.9)
            RETURNING id
            """,
            f"Promote me {test_id}",
        )
        assert wid is not None

        stats = await conn.fetchval("SELECT cleanup_working_memory()")
        if isinstance(stats, str):
            stats = json.loads(stats)
        assert stats["deleted_count"] >= 1
        assert stats["promoted_count"] >= 1

        assert await conn.fetchval("SELECT COUNT(*) FROM working_memory WHERE id = $1::uuid", wid) == 0
        # Query from memories table where metadata contains the from_working_memory_id
        promoted = await conn.fetchrow(
            """
            SELECT m.id, m.type, m.content
            FROM memories m
            WHERE m.type = 'episodic'
            AND m.metadata->'context'->>'from_working_memory_id' = $1::text
            ORDER BY m.created_at DESC
            LIMIT 1
            """,
            str(wid),
        )
        assert promoted is not None
        assert promoted["type"] == "episodic"
        assert test_id in promoted["content"]


async def test_cleanup_embedding_cache_with_interval(db_pool):
    """Test cleanup_embedding_cache() with custom interval"""
    async with db_pool.acquire() as conn:
        # Add old cache entries
        await conn.execute("""
            INSERT INTO embedding_cache (content_hash, embedding, created_at)
            VALUES
                ('old_hash_1', array_fill(0.5, ARRAY[embedding_dimension()])::vector, CURRENT_TIMESTAMP - interval '10 days'),
                ('old_hash_2', array_fill(0.5, ARRAY[embedding_dimension()])::vector, CURRENT_TIMESTAMP - interval '8 days'),
                ('new_hash', array_fill(0.5, ARRAY[embedding_dimension()])::vector, CURRENT_TIMESTAMP)
            ON CONFLICT DO NOTHING
        """)

        # Cleanup entries older than 7 days
        deleted_count = await conn.fetchval("""
            SELECT cleanup_embedding_cache(interval '7 days')
        """)

        assert deleted_count >= 2, "Should delete old cache entries"

        # Verify new entry remains
        new_exists = await conn.fetchval("""
            SELECT COUNT(*) FROM embedding_cache WHERE content_hash = 'new_hash'
        """)
        assert new_exists == 1


# -----------------------------------------------------------------------------
# IDENTITY & WORLDVIEW TESTS
# -----------------------------------------------------------------------------

async def test_identity_aspects_all_types(db_pool):
    """Test all identity aspect_type values"""
    async with db_pool.acquire() as conn:
        aspect_types = ['self_concept', 'purpose', 'boundary', 'agency', 'values']

        test_id = get_test_identifier("identity_types")
        for aspect_type in aspect_types:
            concept = f"{aspect_type}_{test_id}"
            await conn.execute(
                "SELECT upsert_self_concept_edge($1, $2, 0.7, NULL)",
                aspect_type,
                concept,
            )

        sm = await conn.fetchval("SELECT get_self_model_context(50)")
        if isinstance(sm, str):
            sm = json.loads(sm)
        kinds = {entry.get("kind") for entry in (sm or []) if isinstance(entry, dict)}
        for aspect_type in aspect_types:
            assert aspect_type in kinds


async def test_identity_memory_resonance_integration_status(db_pool):
    """Test self-model edges can carry evidence memory references."""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("identity_evidence")
        memory_id = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding)
            VALUES ('episodic'::memory_type, $1,
                    array_fill(0.92, ARRAY[embedding_dimension()])::vector)
            RETURNING id
            """,
            f"Helped user solve problem {test_id}",
        )

        concept = f"helpful_{test_id}"
        await conn.execute(
            "SELECT upsert_self_concept_edge('self_concept', $1, 0.8, $2)",
            concept,
            memory_id,
        )

        sm = await conn.fetchval("SELECT get_self_model_context(50)")
        if isinstance(sm, str):
            sm = json.loads(sm)
        matched = [
            entry for entry in (sm or [])
            if isinstance(entry, dict)
            and entry.get("concept") == concept
            and entry.get("evidence_memory_id") == str(memory_id)
        ]
        assert matched


async def test_worldview_influence_types(db_pool):
    """Test influence type metadata on worldview graph edges."""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("worldview_influence")
        worldview_id = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'ethics', 0.95, 0.8, 0.8, 'test')",
            f"Honesty is important {test_id}",
        )

        influence_types = [
            ("alignment", "SUPPORTS"),
            ("reinforcement", "SUPPORTS"),
            ("challenge", "CONTRADICTS"),
            ("neutral", "SUPPORTS"),
        ]

        for idx, (inf_type, edge_type) in enumerate(influence_types):
            memory_id = await conn.fetchval(
                """
                SELECT create_episodic_memory(
                    $1,
                    '{"action": "truth"}',
                    '{"context": "test"}',
                    '{"result": "honesty"}',
                    0.2,
                    CURRENT_TIMESTAMP,
                    0.5
                )
                """,
                f"Told the truth {test_id} {idx}",
            )
            await conn.execute(
                "SELECT create_memory_relationship($1::uuid, $2::uuid, $3::graph_edge_type, $4::jsonb)",
                memory_id,
                worldview_id,
                edge_type,
                json.dumps({"strength": 0.7, "type": inf_type}),
            )

        await conn.execute("SET LOCAL search_path = ag_catalog, public;")
        rows = await conn.fetch(
            f"""
            SELECT type_val FROM cypher('memory_graph', $$
                MATCH (m:MemoryNode)-[r]->(w:MemoryNode {{memory_id: '{worldview_id}'}})
                WHERE r.type IS NOT NULL
                RETURN r.type
            $$) as (type_val agtype)
            """
        )
        types = {str(r["type_val"]).strip('"') for r in rows}
        for inf_type, _edge in influence_types:
            assert inf_type in types


async def test_worldview_confidence_updates_from_influences(db_pool):
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("worldview_conf_update")
        w_id = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'values', 0.5, 0.7, 0.7, 'test')",
            f"Belief {test_id}",
        )
        m_id = await conn.fetchval(
            """
            SELECT create_episodic_memory(
                $1,
                '{"action": "support"}',
                '{"context": "belief_update"}',
                '{"result": "evidence"}',
                0.4,
                CURRENT_TIMESTAMP,
                0.6,
                NULL,
                1.0
            )
            """,
            f"Evidence {test_id}",
        )
        before = float(
            await conn.fetchval("SELECT (metadata->>'confidence')::float FROM memories WHERE id = $1::uuid", w_id)
        )
        await conn.execute(
            "SELECT create_memory_relationship($1::uuid, $2::uuid, 'SUPPORTS'::graph_edge_type, $3::jsonb)",
            m_id,
            w_id,
            json.dumps({"strength": 1.0}),
        )
        await conn.execute("SELECT update_worldview_confidence_from_influences($1::uuid)", w_id)
        after = float(
            await conn.fetchval("SELECT (metadata->>'confidence')::float FROM memories WHERE id = $1::uuid", w_id)
        )
        assert after > before


async def test_connected_beliefs_relationships(db_pool):
    """Test connected beliefs via graph edges between worldview memories."""
    async with db_pool.acquire() as conn:
        belief1_id = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'values', 0.9, 0.8, 0.8, 'test')",
            "Kindness matters",
        )
        belief2_id = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'values', 0.85, 0.8, 0.8, 'test')",
            "Empathy is valuable",
        )

        await conn.execute(
            "SELECT create_memory_relationship($1::uuid, $2::uuid, 'ASSOCIATED'::graph_edge_type, $3::jsonb)",
            belief1_id,
            belief2_id,
            json.dumps({"strength": 0.8}),
        )

        await conn.execute("SET LOCAL search_path = ag_catalog, public;")
        cnt = await conn.fetchval(
            f"""
            SELECT COUNT(*) FROM cypher('memory_graph', $$
                MATCH (a:MemoryNode {{memory_id: '{belief1_id}'}})-[:ASSOCIATED]->(b:MemoryNode {{memory_id: '{belief2_id}'}})
                RETURN b
            $$) as (b agtype)
            """
        )
        assert int(cnt) >= 1, "Beliefs should be connected"


# -----------------------------------------------------------------------------
# VIEW TESTS
# -----------------------------------------------------------------------------

async def test_memory_health_view_aggregations(db_pool):
    """Test memory_health view calculates correct aggregations"""
    async with db_pool.acquire() as conn:
        # Create memories of known type with known values
        import time
        unique_suffix = uuid.uuid4().hex[:8]

        for i in range(5):
            await conn.execute("""
                INSERT INTO memories (type, content, embedding, importance, access_count)
                VALUES ('procedural'::memory_type, $1,
                        array_fill(0.94, ARRAY[embedding_dimension()])::vector, $2, $3)
            """, f'Health view test {unique_suffix} {i}', 0.5 + i * 0.1, i)

        # Query view
        health = await conn.fetchrow("""
            SELECT * FROM memory_health WHERE type = 'procedural'
        """)

        assert health is not None
        assert health['total_memories'] >= 5
        assert health['avg_importance'] is not None
        assert health['avg_access_count'] is not None


async def test_cluster_insights_view_ordering(db_pool):
    """Test cluster_insights view ordered by memory_count DESC."""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        clusters = []
        for i, member_count in enumerate([1, 4, 2, 3]):
            cluster_id = await conn.fetchval(
                """
                INSERT INTO clusters (cluster_type, name, centroid_embedding)
                VALUES ('theme'::cluster_type, $1, array_fill(0.5, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f'Insights order test {i}',
            )
            clusters.append((cluster_id, member_count))

        for cluster_id, member_count in clusters:
            for j in range(member_count):
                memory_id = await conn.fetchval(
                    """
                    INSERT INTO memories (type, content, embedding)
                    VALUES ('semantic'::memory_type, $1, array_fill(0.5, ARRAY[embedding_dimension()])::vector)
                    RETURNING id
                    """,
                    f'Insights order memory {cluster_id} {j}',
                )
                await conn.execute("SELECT sync_memory_node($1)", memory_id)
                await conn.execute(
                    "SELECT link_memory_to_cluster_graph($1, $2, $3)",
                    memory_id,
                    cluster_id,
                    1.0,
                )

        insights = await conn.fetch(
            """
            SELECT name, memory_count FROM cluster_insights
            WHERE name LIKE 'Insights order test%'
            ORDER BY memory_count DESC
            """
        )

        counts = [r['memory_count'] for r in insights]
        assert counts == sorted(counts, reverse=True), "Should be ordered by memory_count DESC"


# -----------------------------------------------------------------------------
# INDEX PERFORMANCE TESTS
# -----------------------------------------------------------------------------

async def test_hnsw_index_usage_memories(db_pool):
    """Test HNSW index on memories.embedding is used for vector search"""
    async with db_pool.acquire() as conn:
        # Create test data
        for i in range(20):
            await conn.execute("""
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic'::memory_type, $1, array_fill(0.5::float, ARRAY[embedding_dimension()])::vector)
            """, f'HNSW test memory {i}')

        # Check query plan uses index
        plan = await conn.fetch("""
            EXPLAIN (FORMAT JSON)
            SELECT id FROM memories
            ORDER BY embedding <=> array_fill(0.5::float, ARRAY[embedding_dimension()])::vector
            LIMIT 5
        """)

        plan_text = str(plan)
        # HNSW index should be mentioned in plan
        assert 'idx_memories_embedding' in plan_text or 'Index' in plan_text


async def test_gin_index_content_search(db_pool):
    """Test GIN trigram index on memories.content for text search"""
    async with db_pool.acquire() as conn:
        # Create memories with searchable content
        await conn.execute("""
            INSERT INTO memories (type, content, embedding)
            VALUES
                ('semantic'::memory_type, 'PostgreSQL database management', array_fill(0.5, ARRAY[embedding_dimension()])::vector),
                ('semantic'::memory_type, 'Python programming language', array_fill(0.5, ARRAY[embedding_dimension()])::vector)
        """)

        # Query using trigram similarity
        results = await conn.fetch("""
            SELECT content FROM memories
            WHERE content ILIKE '%postgres%'
        """)

        assert len(results) >= 1
        assert 'PostgreSQL' in results[0]['content']

# =============================================================================
# COMPREHENSIVE EMBEDDING AND FUNCTION TESTS
# These tests exercise functions that were previously untested or only
# partially tested. They require the embedding service to be running.
# =============================================================================

# -----------------------------------------------------------------------------
# get_embedding() COMPREHENSIVE TESTS
# -----------------------------------------------------------------------------

async def test_get_embedding_basic_functionality(db_pool, ensure_embedding_service):
    """Test get_embedding() returns a vector for configured dimension"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("get_emb_basic")
        test_content = f"Test content for embedding generation {test_id}"

        # Clear any cached version
        content_hash = await conn.fetchval(
            "SELECT encode(sha256($1::text::bytea), 'hex')", test_content
        )
        await conn.execute(
            "DELETE FROM embedding_cache WHERE content_hash = $1", content_hash
        )

        # Call get_embedding
        embedding = await conn.fetchval(
            "SELECT get_embedding($1)", test_content
        )

        assert embedding is not None, "get_embedding should return a vector"
        # pgvector returns a string representation, verify it's non-empty
        assert len(str(embedding)) > 0, "Embedding should not be empty"


async def test_get_embedding_caching_creates_cache_entry(db_pool, ensure_embedding_service):
    """Test that get_embedding() creates a cache entry in embedding_cache table"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("get_emb_cache")
        test_content = f"Unique content for caching test {test_id}"

        # Calculate the hash that will be used
        content_hash = await conn.fetchval(
            "SELECT encode(sha256($1::text::bytea), 'hex')", test_content
        )

        # Ensure no cache entry exists
        await conn.execute(
            "DELETE FROM embedding_cache WHERE content_hash = $1", content_hash
        )

        # Verify cache is empty for this content
        cache_before = await conn.fetchval(
            "SELECT COUNT(*) FROM embedding_cache WHERE content_hash = $1", content_hash
        )
        assert cache_before == 0, "Cache should be empty before call"

        # Call get_embedding
        await conn.fetchval("SELECT get_embedding($1)", test_content)

        # Verify cache entry was created
        cache_after = await conn.fetchval(
            "SELECT COUNT(*) FROM embedding_cache WHERE content_hash = $1", content_hash
        )
        assert cache_after == 1, "Cache entry should be created after get_embedding call"


async def test_get_embedding_cache_hit_returns_same_result(db_pool, ensure_embedding_service):
    """Test that get_embedding() returns cached embedding on second call"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("get_emb_cache_hit")
        test_content = f"Content for cache hit test {test_id}"

        # Clear cache first
        content_hash = await conn.fetchval(
            "SELECT encode(sha256($1::text::bytea), 'hex')", test_content
        )
        await conn.execute(
            "DELETE FROM embedding_cache WHERE content_hash = $1", content_hash
        )

        # First call - cache miss, hits service
        embedding1 = await conn.fetchval("SELECT get_embedding($1)", test_content)

        # Second call - should be cache hit
        embedding2 = await conn.fetchval("SELECT get_embedding($1)", test_content)

        assert embedding1 == embedding2, "Cached embedding should match original"


async def test_get_embedding_sha256_hash_correctness(db_pool, ensure_embedding_service):
    """Test that get_embedding uses correct SHA256 hashing for cache key"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("get_emb_hash")
        test_content = f"Hash test content {test_id}"

        # Calculate expected hash
        expected_hash = await conn.fetchval(
            "SELECT encode(sha256($1::text::bytea), 'hex')", test_content
        )

        # Clear any existing cache
        await conn.execute(
            "DELETE FROM embedding_cache WHERE content_hash = $1", expected_hash
        )

        # Call get_embedding
        await conn.fetchval("SELECT get_embedding($1)", test_content)

        # Verify the cache entry uses the expected hash
        cached_embedding = await conn.fetchval(
            "SELECT embedding FROM embedding_cache WHERE content_hash = $1", expected_hash
        )
        assert cached_embedding is not None, "Cache should use SHA256 hash as key"


async def test_get_embedding_different_content_different_embeddings(db_pool, ensure_embedding_service):
    """Test that different content produces different embeddings"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("get_emb_diff")

        content1 = f"The cat sat on the mat {test_id}"
        content2 = f"Quantum physics principles {test_id}"

        embedding1 = await conn.fetchval("SELECT get_embedding($1)", content1)
        embedding2 = await conn.fetchval("SELECT get_embedding($1)", content2)

        # Embeddings should be different for semantically different content
        assert embedding1 != embedding2, "Different content should produce different embeddings"


async def test_get_embedding_http_error_handling(db_pool, ensure_embedding_service):
    """Test get_embedding() error handling when HTTP service is unavailable"""
    async with db_pool.acquire() as conn:
        # Phase 7 (ReduceScopeCreep): Use unified config only
        original_url = await conn.fetchval("SELECT get_config_text('embedding.service_url')")
        original_retry_seconds, original_retry_interval_seconds = await _set_embedding_retry_config(
            conn,
            retry_seconds=0,
            retry_interval_seconds=0.0,
        )

        try:
            # Set invalid URL
            await conn.execute(
                "SELECT set_config('embedding.service_url', '\"http://nonexistent-service:9999/embed\"'::jsonb)"
            )

            # Attempt to get embedding - should fail
            with pytest.raises(asyncpg.PostgresError) as exc_info:
                await conn.fetchval("SELECT get_embedding('test content')")

            assert "Failed to get embedding" in str(exc_info.value), \
                "Should raise proper error message"
        finally:
            # Restore original URL
            await conn.execute(
                "SELECT set_config('embedding.service_url', $1::jsonb)",
                json.dumps(original_url)
            )
            await _restore_embedding_retry_config(
                conn,
                original_retry_seconds,
                original_retry_interval_seconds,
            )


async def test_get_embedding_non_200_response_handling(db_pool, ensure_embedding_service):
    """Test get_embedding() handles non-200 HTTP responses"""
    async with db_pool.acquire() as conn:
        # Phase 7 (ReduceScopeCreep): Use unified config only
        original_url = await conn.fetchval("SELECT get_config_text('embedding.service_url')")
        original_retry_seconds, original_retry_interval_seconds = await _set_embedding_retry_config(
            conn,
            retry_seconds=0,
            retry_interval_seconds=0.0,
        )

        try:
            # Set URL that will return 404 or similar
            await conn.execute(
                "SELECT set_config('embedding.service_url', '\"http://embeddings:80/nonexistent-endpoint\"'::jsonb)"
            )

            with pytest.raises(asyncpg.PostgresError) as exc_info:
                await conn.fetchval("SELECT get_embedding('test content')")

            # Should mention service error or failed
            error_msg = str(exc_info.value).lower()
            assert "error" in error_msg or "failed" in error_msg, \
                "Should indicate error for non-200 response"
        finally:
            # Restore
            await conn.execute(
                "SELECT set_config('embedding.service_url', $1::jsonb)",
                json.dumps(original_url)
            )
            await _restore_embedding_retry_config(
                conn,
                original_retry_seconds,
                original_retry_interval_seconds,
            )


async def test_get_embedding_dimension_validation(db_pool, ensure_embedding_service):
    """Test that get_embedding returns a vector matching configured dimension"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("get_emb_dim")

        embedding = await conn.fetchval("SELECT get_embedding($1)", f"Dimension test {test_id}")
        dims = await conn.fetchval("SELECT vector_dims($1::vector)", embedding)
        expected = await conn.fetchval("SELECT embedding_dimension()")
        assert int(dims) == int(expected)

        # Verify by inserting into a table that enforces the configured dimension
        await conn.execute("""
            INSERT INTO working_memory (content, embedding, expiry)
            VALUES ($1, $2, NOW() + INTERVAL '1 hour')
        """, f"Dimension verification {test_id}", embedding)

        # If we get here, the embedding matched the typmod
        assert True


# -----------------------------------------------------------------------------
# check_embedding_service_health() COMPREHENSIVE TESTS
# -----------------------------------------------------------------------------

async def test_check_embedding_service_health_returns_boolean(db_pool):
    """Test check_embedding_service_health() returns a boolean value"""
    async with db_pool.acquire() as conn:
        result = await conn.fetchval("SELECT check_embedding_service_health()")
        assert isinstance(result, bool), "Should return a boolean"


async def test_check_embedding_service_health_true_when_available(db_pool):
    """Test health check returns true when service is available"""
    async with db_pool.acquire() as conn:
        # Phase 7 (ReduceScopeCreep): Use unified config only
        await conn.execute(
            "SELECT set_config('embedding.service_url', '\"http://embeddings:80/embed\"'::jsonb)"
        )

        result = await conn.fetchval("SELECT check_embedding_service_health()")

        assert result is True, "Should return true when service is healthy"


async def test_check_embedding_service_health_false_when_unavailable(db_pool):
    """Test health check returns false when service is unavailable"""
    async with db_pool.acquire() as conn:
        # Phase 7 (ReduceScopeCreep): Use unified config only
        original_url = await conn.fetchval("SELECT get_config_text('embedding.service_url')")

        try:
            # Set invalid URL
            await conn.execute(
                "SELECT set_config('embedding.service_url', '\"http://nonexistent-host:9999/embed\"'::jsonb)"
            )

            result = await conn.fetchval("SELECT check_embedding_service_health()")
            assert result == False, "Should return false when service unavailable"
        finally:
            # Restore
            await conn.execute(
                "SELECT set_config('embedding.service_url', $1::jsonb)",
                json.dumps(original_url)
            )


async def test_check_embedding_service_health_endpoint_path(db_pool):
    """Test health check uses correct /health endpoint path"""
    async with db_pool.acquire() as conn:
        # Phase 7 (ReduceScopeCreep): Use unified config only
        await conn.execute(
            "SELECT set_config('embedding.service_url', '\"http://embeddings:80/embed\"'::jsonb)"
        )

        # Function should work without error (may return true or false)
        result = await conn.fetchval("SELECT check_embedding_service_health()")
        assert result is not None, "Should return a value, not NULL"


async def test_check_embedding_service_health_no_exception_on_failure(db_pool):
    """Test health check gracefully handles errors without raising exceptions"""
    async with db_pool.acquire() as conn:
        # Phase 7 (ReduceScopeCreep): Use unified config only
        original_url = await conn.fetchval("SELECT get_config_text('embedding.service_url')")

        try:
            # Set completely invalid URL
            await conn.execute(
                "SELECT set_config('embedding.service_url', '\"http://256.256.256.256:9999/embed\"'::jsonb)"
            )

            # Should NOT raise an exception, should return false
            result = await conn.fetchval("SELECT check_embedding_service_health()")
            assert result == False, "Should return false, not raise exception"
        finally:
            # Restore
            await conn.execute(
                "SELECT set_config('embedding.service_url', $1::jsonb)",
                json.dumps(original_url)
            )


# -----------------------------------------------------------------------------
# create_memory() COMPREHENSIVE TESTS
# -----------------------------------------------------------------------------

async def test_create_memory_returns_uuid(db_pool, ensure_embedding_service):
    """Test create_memory() returns a valid UUID"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("create_mem_uuid")

        memory_id = await conn.fetchval("""
            SELECT create_memory('semantic'::memory_type, $1, 0.7)
        """, f"Test memory content {test_id}")

        assert memory_id is not None, "Should return a UUID"
        assert isinstance(memory_id, uuid.UUID), "Should be a UUID type"


async def test_create_memory_generates_embedding(db_pool, ensure_embedding_service):
    """Test create_memory() generates embedding automatically via get_embedding()"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("create_mem_emb")
        content = f"Memory with auto-generated embedding {test_id}"

        memory_id = await conn.fetchval("""
            SELECT create_memory('semantic'::memory_type, $1, 0.8)
        """, content)

        # Verify embedding was generated and stored
        embedding = await conn.fetchval("""
            SELECT embedding FROM memories WHERE id = $1
        """, memory_id)

        assert embedding is not None, "Embedding should be generated"


async def test_create_memory_creates_graph_node(db_pool, ensure_embedding_service):
    """Test create_memory() creates a MemoryNode in the graph"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("create_mem_graph")

        memory_id = await conn.fetchval("""
            SELECT create_memory('episodic'::memory_type, $1, 0.6)
        """, f"Memory for graph node test {test_id}")

        # Verify graph node was created using f-string for UUID in cypher
        graph_check = await conn.fetchval(f"""
            SELECT * FROM cypher('memory_graph', $$
                MATCH (n:MemoryNode)
                WHERE n.memory_id = '{memory_id}'
                RETURN count(n)
            $$) as (cnt agtype)
        """)

        assert graph_check is not None, "Graph node should exist"
        # agtype returns as string like "1", check it's not "0"
        assert str(graph_check) != '0', "Should find at least one graph node"


async def test_create_memory_graph_node_properties(db_pool, ensure_embedding_service):
    """Test MemoryNode has correct properties (memory_id, type, created_at)"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("create_mem_props")

        memory_id = await conn.fetchval("""
            SELECT create_memory('procedural'::memory_type, $1, 0.5)
        """, f"Memory for properties test {test_id}")

        # Query graph for node properties
        result = await conn.fetch(f"""
            SELECT * FROM cypher('memory_graph', $$
                MATCH (n:MemoryNode)
                WHERE n.memory_id = '{memory_id}'
                RETURN n.memory_id, n.type, n.created_at
            $$) as (mid agtype, mtype agtype, created agtype)
        """)

        assert len(result) == 1, "Should find exactly one node"
        # The values are agtype, which are JSON-like
        assert str(memory_id) in str(result[0]['mid']), "memory_id should match"
        assert 'procedural' in str(result[0]['mtype']), "type should be procedural"
        assert result[0]['created'] is not None, "created_at should be set"


async def test_create_memory_all_types(db_pool, ensure_embedding_service):
    """Test create_memory() works for all memory types"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("create_mem_types")
        memory_types = ['episodic', 'semantic', 'procedural', 'strategic']

        for mem_type in memory_types:
            memory_id = await conn.fetchval(f"""
                SELECT create_memory('{mem_type}'::memory_type, $1, 0.5)
            """, f"Test {mem_type} memory {test_id}")

            assert memory_id is not None, f"Should create {mem_type} memory"

            # Verify type in database
            stored_type = await conn.fetchval("""
                SELECT type FROM memories WHERE id = $1
            """, memory_id)

            assert stored_type == mem_type, f"Stored type should be {mem_type}"


async def test_create_memory_importance_stored(db_pool, ensure_embedding_service):
    """Test create_memory() stores importance value correctly"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("create_mem_imp")
        importance = 0.95

        memory_id = await conn.fetchval("""
            SELECT create_memory('semantic'::memory_type, $1, $2)
        """, f"High importance memory {test_id}", importance)

        stored_importance = await conn.fetchval("""
            SELECT importance FROM memories WHERE id = $1
        """, memory_id)

        assert abs(stored_importance - importance) < 0.001, "Importance should match"


async def test_create_memory_triggers_episode_assignment(db_pool, ensure_embedding_service):
    """Test create_memory() triggers auto episode assignment"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("create_mem_ep")

        memory_id = await conn.fetchval("""
            SELECT create_memory('episodic'::memory_type, $1, 0.7)
        """, f"Memory for episode test {test_id}")

        # Verify memory was assigned to an episode
        episode_link = await _fetch_episode_for_memory(conn, memory_id)

        assert episode_link is not None, "Memory should be assigned to episode"
        assert episode_link['episode_id'] is not None
        assert episode_link['sequence_order'] >= 1


async def test_create_memory_initializes_neighborhood(db_pool, ensure_embedding_service):
    """Test create_memory() initializes memory_neighborhoods record"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("create_mem_neigh")

        memory_id = await conn.fetchval("""
            SELECT create_memory('semantic'::memory_type, $1, 0.6)
        """, f"Memory for neighborhood test {test_id}")

        # Verify neighborhood record was created
        neighborhood = await conn.fetchrow("""
            SELECT memory_id, neighbors, is_stale FROM memory_neighborhoods
            WHERE memory_id = $1
        """, memory_id)

        assert neighborhood is not None, "Neighborhood record should be created"
        assert neighborhood['is_stale'] == True, "New neighborhood should be stale"


# -----------------------------------------------------------------------------
# add_to_working_memory() COMPREHENSIVE TESTS
# -----------------------------------------------------------------------------

async def test_add_to_working_memory_returns_uuid(db_pool, ensure_embedding_service):
    """Test add_to_working_memory() returns a valid UUID"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("add_wm_uuid")

        wm_id = await conn.fetchval("""
            SELECT add_to_working_memory($1, INTERVAL '1 hour')
        """, f"Working memory content {test_id}")

        assert wm_id is not None, "Should return a UUID"
        assert isinstance(wm_id, uuid.UUID), "Should be UUID type"


async def test_add_to_working_memory_generates_embedding(db_pool, ensure_embedding_service):
    """Test add_to_working_memory() calls get_embedding() for auto-embedding"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("add_wm_emb")
        content = f"Working memory with embedding {test_id}"

        wm_id = await conn.fetchval("""
            SELECT add_to_working_memory($1, INTERVAL '1 hour')
        """, content)

        # Verify embedding was stored
        embedding = await conn.fetchval("""
            SELECT embedding FROM working_memory WHERE id = $1
        """, wm_id)

        assert embedding is not None, "Embedding should be generated"


async def test_add_to_working_memory_sets_expiry(db_pool, ensure_embedding_service):
    """Test add_to_working_memory() sets correct expiry time"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("add_wm_exp")

        wm_id = await conn.fetchval("""
            SELECT add_to_working_memory($1, INTERVAL '2 hours')
        """, f"Expiry test {test_id}")

        expiry = await conn.fetchval("""
            SELECT expiry FROM working_memory WHERE id = $1
        """, wm_id)

        assert expiry is not None, "Expiry should be set"
        # Expiry should be approximately 2 hours from now
        now = await conn.fetchval("SELECT CURRENT_TIMESTAMP")
        diff = expiry - now
        # Should be between 1h 59m and 2h 1m (allowing for test execution time)
        assert diff.total_seconds() > 7100 and diff.total_seconds() < 7300, \
            "Expiry should be approximately 2 hours from now"


async def test_add_to_working_memory_default_expiry(db_pool, ensure_embedding_service):
    """Test add_to_working_memory() uses default 1 hour expiry"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("add_wm_def_exp")

        wm_id = await conn.fetchval("""
            SELECT add_to_working_memory($1)
        """, f"Default expiry test {test_id}")

        expiry = await conn.fetchval("""
            SELECT expiry FROM working_memory WHERE id = $1
        """, wm_id)

        now = await conn.fetchval("SELECT CURRENT_TIMESTAMP")
        diff = expiry - now
        # Default should be 1 hour (3600 seconds, with some tolerance)
        assert diff.total_seconds() > 3500 and diff.total_seconds() < 3700, \
            "Default expiry should be approximately 1 hour"


async def test_add_to_working_memory_content_stored(db_pool, ensure_embedding_service):
    """Test add_to_working_memory() stores content correctly"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("add_wm_content")
        content = f"Specific working memory content {test_id}"

        wm_id = await conn.fetchval("""
            SELECT add_to_working_memory($1, INTERVAL '1 hour')
        """, content)

        stored_content = await conn.fetchval("""
            SELECT content FROM working_memory WHERE id = $1
        """, wm_id)

        assert stored_content == content, "Content should match exactly"


async def test_add_to_working_memory_searchable(db_pool, ensure_embedding_service):
    """Test working memory added via function is searchable"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("add_wm_search")
        content = f"Searchable working memory about databases {test_id}"

        await conn.fetchval("""
            SELECT add_to_working_memory($1, INTERVAL '1 hour')
        """, content)

        # Search for it
        # Use a query that includes the unique test token so this remains deterministic
        # even if other tests have populated working_memory.
        results = await conn.fetch(
            "SELECT * FROM search_working_memory($1, 10)",
            content,
        )

        # Should find our content
        found = any(test_id in str(r['content']) for r in results)
        assert found, "Should find the working memory via search"


async def test_touch_working_memory_updates_access_fields(db_pool):
    async with db_pool.acquire() as conn:
        wm_id = await conn.fetchval(
            """
            INSERT INTO working_memory (content, embedding, access_count, expiry)
            VALUES ($1, array_fill(0.4, ARRAY[embedding_dimension()])::vector, 0, CURRENT_TIMESTAMP + interval '1 hour')
            RETURNING id
            """,
            f"Working memory {get_test_identifier('touch_wm')}",
        )

        await conn.execute("SELECT touch_working_memory($1::uuid[])", [wm_id])

        row = await conn.fetchrow(
            "SELECT access_count, last_accessed FROM working_memory WHERE id = $1",
            wm_id,
        )
        assert row["access_count"] == 1
        assert row["last_accessed"] is not None


async def test_promote_working_memory_to_episodic_creates_memory(db_pool):
    async with db_pool.acquire() as conn:
        wm_id = await conn.fetchval(
            """
            INSERT INTO working_memory (content, embedding, importance, source_attribution, trust_level, expiry)
            VALUES (
                $1,
                array_fill(0.3, ARRAY[embedding_dimension()])::vector,
                0.5,
                jsonb_build_object('kind', 'internal'),
                0.8,
                CURRENT_TIMESTAMP + interval '1 hour'
            )
            RETURNING id
            """,
            f"Promote working memory {get_test_identifier('promote_wm')}",
        )

        new_id = await conn.fetchval(
            "SELECT promote_working_memory_to_episodic($1, 0.6)",
            wm_id,
        )
        assert new_id is not None

        row = await conn.fetchrow(
            "SELECT type, metadata FROM memories WHERE id = $1",
            new_id,
        )
        assert row["type"] == "episodic"
        metadata = _coerce_json(row["metadata"])
        assert metadata["context"]["from_working_memory_id"] == str(wm_id)


# -----------------------------------------------------------------------------
# BATCH CREATE MEMORIES
# -----------------------------------------------------------------------------

async def test_batch_create_memories_creates_rows_and_nodes(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("batch_create_memories")
            items = [
                {
                    "type": "semantic",
                    "content": f"Batch semantic A {test_id}",
                    "importance": 0.6,
                    "confidence": 0.9,
                    "source_references": [{"kind": "twitter", "ref": f"https://twitter.com/x/{test_id}", "trust": 0.2}],
                },
                {
                    "type": "episodic",
                    "content": f"Batch episodic B {test_id}",
                    "importance": 0.4,
                    "context": {"type": "test"},
                    "emotional_valence": 0.2,
                },
            ]

            ids = await conn.fetchval("SELECT batch_create_memories($1::jsonb)", json.dumps(items))
            assert isinstance(ids, list)
            assert len(ids) == 2

            # Verify base rows exist
            count = await conn.fetchval("SELECT COUNT(*) FROM memories WHERE id = ANY($1::uuid[])", ids)
            assert int(count) == 2

            # Verify type-specific metadata exists in memories table
            sem_metadata = await conn.fetchval("SELECT metadata FROM memories WHERE id = $1::uuid", ids[0])
            epi_metadata = await conn.fetchval("SELECT metadata FROM memories WHERE id = $1::uuid", ids[1])
            if isinstance(sem_metadata, str):
                sem_metadata = json.loads(sem_metadata)
            if isinstance(epi_metadata, str):
                epi_metadata = json.loads(epi_metadata)
            # Semantic and episodic memories should have metadata (even if empty for base memories)
            assert sem_metadata is not None
            assert epi_metadata is not None

            # Verify graph nodes exist
            await conn.execute("LOAD 'age';")
            await conn.execute("SET search_path = ag_catalog, public;")
            for mid in ids:
                node_count = await conn.fetchval(
                    f"""
                    SELECT COUNT(*) FROM cypher('memory_graph', $$
                        MATCH (n:MemoryNode {{memory_id: '{mid}'}})
                        RETURN n
                    $$) as (n agtype)
                    """
                )
                assert int(node_count) >= 1
        finally:
            await tr.rollback()


async def test_create_memory_with_embedding_creates_node(db_pool):
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        content = f"Memory with embedding {get_test_identifier('mem_with_embed')}"
        memory_id = await conn.fetchval(
            """
            SELECT create_memory_with_embedding(
                'semantic'::memory_type,
                $1,
                array_fill(0.12, ARRAY[embedding_dimension()])::vector,
                0.7
            )
            """,
            content,
        )
        assert memory_id is not None

        node_count = await conn.fetchval(
            f"""
            SELECT COUNT(*) FROM cypher('memory_graph', $$
                MATCH (n:MemoryNode {{memory_id: '{memory_id}'}})
                RETURN n
            $$) as (n agtype)
            """
        )
        assert int(node_count) >= 1
        await conn.execute("SET search_path = public, ag_catalog;")


async def test_create_memory_with_embedding_requires_embedding(db_pool):
    async with db_pool.acquire() as conn:
        with pytest.raises(asyncpg.PostgresError):
            await conn.fetchval(
                "SELECT create_memory_with_embedding('semantic'::memory_type, 'bad', NULL::vector)"
            )


async def test_batch_create_memories_with_embeddings_creates_rows(db_pool):
    async with db_pool.acquire() as conn:
        contents = [
            f"Batch embed A {get_test_identifier('batch_embed_a')}",
            f"Batch embed B {get_test_identifier('batch_embed_b')}",
        ]
        embeddings = [
            [0.0] * EMBEDDING_DIMENSION,
            [0.1] * EMBEDDING_DIMENSION,
        ]
        ids = await conn.fetchval(
            "SELECT batch_create_memories_with_embeddings('semantic'::memory_type, $1::text[], $2::jsonb, 0.6)",
            contents,
            json.dumps(embeddings),
        )
        assert isinstance(ids, list)
        assert len(ids) == 2
        count = await conn.fetchval("SELECT COUNT(*) FROM memories WHERE id = ANY($1::uuid[])", ids)
        assert int(count) == 2


async def test_auto_check_worldview_alignment_creates_support_edge(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("LOAD 'age';")
            await conn.execute("SET search_path = ag_catalog, public;")
            await conn.execute(
                "SELECT set_config('memory.worldview_support_threshold', '0.5'::jsonb)"
            )
            await conn.execute(
                "SELECT set_config('memory.worldview_contradict_threshold', '-0.5'::jsonb)"
            )

            worldview_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, metadata)
                VALUES (
                    'worldview'::memory_type,
                    $1,
                    array_fill(0.4, ARRAY[embedding_dimension()])::vector,
                    jsonb_build_object('category', 'values', 'confidence', 0.8, 'stability', 0.8)
                )
                RETURNING id
                """,
                f"Alignment worldview {get_test_identifier('auto_align')}",
            )
            await conn.fetchval("SELECT sync_memory_node($1)", worldview_id)

            semantic_id = uuid.uuid4()
            await conn.execute(
                f"""
                SELECT * FROM ag_catalog.cypher('memory_graph', $$
                    CREATE (m:MemoryNode {{memory_id: '{semantic_id}', type: 'semantic', created_at: '{datetime.now(timezone.utc).isoformat()}'}})
                    RETURN m
                $$) as (result agtype)
                """
            )
            await conn.execute(
                """
                INSERT INTO memories (id, type, content, embedding, importance)
                VALUES ($1, 'semantic'::memory_type, $2, array_fill(0.4, ARRAY[embedding_dimension()])::vector, 0.6)
                """,
                semantic_id,
                f"Aligned semantic {get_test_identifier('auto_align_semantic')}",
            )

            edge_rows = await conn.fetch(
                f"""
                SELECT * FROM ag_catalog.cypher('memory_graph', $$
                    MATCH (m:MemoryNode {{memory_id: '{semantic_id}'}})-[:SUPPORTS]->(w:MemoryNode {{memory_id: '{worldview_id}'}})
                    RETURN w
                $$) as (w agtype)
                """
            )
            assert len(edge_rows) >= 1
            await conn.execute("SET search_path = public, ag_catalog;")
        finally:
            await tr.rollback()


# -----------------------------------------------------------------------------
# INTEGRATION TESTS - Full workflow with real embeddings
# -----------------------------------------------------------------------------

async def test_full_memory_lifecycle_with_embeddings(db_pool, ensure_embedding_service):
    """Test complete memory lifecycle: create -> search -> recall -> graph"""
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        test_id = get_test_identifier("full_lifecycle")
        unique_content = f"Quantum entanglement principles in distributed systems {test_id}"

        # 1. Create memory using create_memory (tests get_embedding)
        memory_id = await conn.fetchval("""
            SELECT create_memory('semantic'::memory_type, $1, 0.9)
        """, unique_content)

        assert memory_id is not None

        # 2. Verify embedding was generated and cached
        content_hash = await conn.fetchval(
            "SELECT encode(sha256($1::text::bytea), 'hex')",
            unique_content
        )
        cached = await conn.fetchval(
            "SELECT COUNT(*) FROM embedding_cache WHERE content_hash = $1",
            content_hash
        )
        assert cached >= 1, "Embedding should be cached"

        # 3. Search for memory using search_similar_memories (exact content ensures nearest-neighbor match)
        results = await conn.fetch("""
            SELECT * FROM search_similar_memories($1, 25)
        """, unique_content)

        # Verify we got results and search function works
        assert len(results) > 0, "Search should return results"

        # The memory we just created should be in results (search by similar content)
        found = any(str(memory_id) == str(r['memory_id']) for r in results)
        assert found, f"Should find memory {memory_id} via similarity search"

        # 4. Use fast_recall
        recall_results = await conn.fetch("""
            SELECT * FROM fast_recall($1, 5)
        """, f"quantum distributed {test_id}")

        # Verify function works (may or may not find our specific memory)
        assert recall_results is not None

        # 5. Verify graph node exists
        graph_count = await conn.fetchval(f"""
            SELECT * FROM cypher('memory_graph', $$
                MATCH (n:MemoryNode)
                WHERE n.memory_id = '{memory_id}'
                RETURN count(n)
            $$) as (cnt agtype)
        """)

        assert graph_count is not None


async def test_working_memory_full_workflow(db_pool, ensure_embedding_service):
    """Test working memory: add -> search -> cleanup"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("wm_workflow")

        # 1. Add multiple items using add_to_working_memory
        items = [
            f"Current task: reviewing code changes {test_id}",
            f"User context: working on Python project {test_id}",
            f"Recent action: opened file editor {test_id}"
        ]

        wm_ids = []
        for item in items:
            wm_id = await conn.fetchval("""
                SELECT add_to_working_memory($1, INTERVAL '30 minutes')
            """, item)
            wm_ids.append(wm_id)

        assert len(wm_ids) == 3, "Should create 3 working memory items"

        # 2. Search working memory
        results = await conn.fetch(
            "SELECT * FROM search_working_memory($1, 10)",
            f"Python project {test_id}",
        )

        # Should find at least one of our items
        found_any = any(test_id in str(r['content']) for r in results)
        assert found_any, "Should find working memory items via search"

        # 3. Test cleanup with short expiry
        short_expiry_id = await conn.fetchval("""
            SELECT add_to_working_memory($1, INTERVAL '-1 second')
        """, f"Already expired {test_id}")

        stats = await conn.fetchval("SELECT cleanup_working_memory()")
        if isinstance(stats, str):
            stats = json.loads(stats)
        assert stats["deleted_count"] >= 1, "Should clean up expired items"

        # Verify expired item is gone
        exists = await conn.fetchval("""
            SELECT EXISTS (SELECT 1 FROM working_memory WHERE id = $1)
        """, short_expiry_id)
        assert not exists, "Expired item should be deleted"


async def test_embedding_cache_lifecycle(db_pool, ensure_embedding_service):
    """Test embedding cache: create -> verify -> cleanup"""
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("cache_lifecycle")

        # 1. Generate embedding (creates cache entry)
        content = f"Unique content for cache lifecycle {test_id}"
        content_hash = await conn.fetchval(
            "SELECT encode(sha256($1::text::bytea), 'hex')", content
        )

        # Clear any existing
        await conn.execute(
            "DELETE FROM embedding_cache WHERE content_hash = $1", content_hash
        )

        # 2. Call get_embedding
        embedding = await conn.fetchval("SELECT get_embedding($1)", content)
        assert embedding is not None

        # 3. Verify cache entry created with correct timestamp
        cache_entry = await conn.fetchrow("""
            SELECT content_hash, embedding, created_at
            FROM embedding_cache WHERE content_hash = $1
        """, content_hash)

        assert cache_entry is not None
        assert cache_entry['embedding'] is not None
        assert cache_entry['created_at'] is not None

        # 4. Test cleanup with 0 interval (should delete everything)
        deleted = await conn.fetchval("""
            SELECT cleanup_embedding_cache(INTERVAL '0 seconds')
        """)

        assert deleted >= 1, "Should delete cache entries"

        # 5. Verify our entry was cleaned up
        exists = await conn.fetchval("""
            SELECT EXISTS (
                SELECT 1 FROM embedding_cache WHERE content_hash = $1
            )
        """, content_hash)
        assert not exists, "Cache entry should be deleted"


# -----------------------------------------------------------------------------
# HEARTBEAT TESTS (DB + WORKER CONTRACT)
# -----------------------------------------------------------------------------

@pytest.fixture(scope="module")
async def apply_heartbeat_migration(db_pool):
    """
    Legacy fixture retained for older test structure.

    The repo now folds patches into db/*.sql, and any optional
    migrations/ patches are handled by `apply_repo_migrations`, so this
    fixture is intentionally a no-op to avoid overriding newer definitions.
    """
    return True


async def test_start_heartbeat_enqueues_decision_call(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = payload.get("heartbeat_id")
        assert hb_id is not None

        call = (payload.get("external_calls") or [{}])[0]
        assert call.get("call_type") == "think"
        call_input = call.get("input") or {}
        assert call_input.get("kind") == "heartbeat_decision"
        assert "agent" in (call_input.get("context") or {})
        assert "self_model" in (call_input.get("context") or {})
        assert "narrative" in (call_input.get("context") or {})

        ctx_number = int(call_input["context"]["heartbeat_number"])
        assert payload.get("heartbeat_number") == ctx_number


async def test_execute_heartbeat_action_rejects_unknown_action(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        before = await conn.fetchval("SELECT get_current_energy()")

        result = await conn.fetchval(
            "SELECT execute_heartbeat_action($1::uuid, $2, $3::jsonb)",
            hb_id,
            "not_a_real_action",
            json.dumps({}),
        )
        parsed = json.loads(result)
        assert parsed["success"] is False

        after = await conn.fetchval("SELECT get_current_energy()")
        assert after == before, "Unknown action should not consume energy"


async def test_reach_out_user_queues_outbox_message(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        result = await conn.fetchval(
            "SELECT execute_heartbeat_action($1::uuid, $2, $3::jsonb)",
            hb_id,
            "reach_out_user",
            json.dumps({"message": "hello", "intent": "test"}),
        )
        parsed = json.loads(result)
        assert parsed["success"] is True

        outbox = (parsed.get("outbox_messages") or [{}])[0]
        assert outbox.get("kind") == "user"
        payload = outbox.get("payload") or {}
        assert payload.get("heartbeat_id") == str(hb_id)


async def test_queue_user_message_inserts_outbox(db_pool):
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("queue_user_message")
        message = _coerce_json(
            await conn.fetchval(
                "SELECT build_user_message($1, $2, $3::jsonb)",
                f"hello {test_id}",
                "reminder",
                json.dumps({"test_id": test_id}),
            )
        )
        assert message.get("kind") == "user"

        payload = message.get("payload") or {}
        assert payload.get("message") == f"hello {test_id}"


async def test_brainstorm_action_queues_external_call(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        result = await conn.fetchval(
            "SELECT execute_heartbeat_action($1::uuid, $2, $3::jsonb)",
            hb_id,
            "brainstorm_goals",
            json.dumps({"max_goals": 3}),
        )
        parsed = json.loads(result)
        assert parsed["success"] is True
        call = (parsed.get("external_calls") or [{}])[0]
        assert call.get("call_type") == "think"
        call_input = call.get("input") or {}
        assert call_input.get("kind") == "brainstorm_goals"


async def test_maintain_action_returns_stats(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        result = await conn.fetchval(
            "SELECT execute_heartbeat_action($1::uuid, $2, $3::jsonb)",
            hb_id,
            "maintain",
            json.dumps({}),
        )
        parsed = json.loads(result)
        assert parsed["success"] is True
        assert parsed["result"]["maintained"] is True


async def test_complete_heartbeat_narrative_marks_failures(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        actions_taken = [
            {"action": "recall", "params": {"query": "x"}, "result": {"success": True}},
            {"action": "not_real", "params": {}, "result": {"success": False}},
        ]
        memory_id = await conn.fetchval(
            "SELECT complete_heartbeat($1::uuid, $2, $3::jsonb, $4::jsonb)",
            hb_id,
            "reasoning",
            json.dumps(actions_taken),
            json.dumps([]),
        )
        narrative = await conn.fetchval("SELECT content FROM memories WHERE id = $1::uuid", memory_id)
        assert "failed" in narrative


async def test_start_heartbeat_regenerates_energy_with_cap(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        await conn.execute("UPDATE heartbeat_state SET current_energy = 19, is_paused = FALSE WHERE id = 1")
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        assert hb_payload.get("heartbeat_id") is not None

        current_energy = await conn.fetchval("SELECT current_energy FROM heartbeat_state WHERE id = 1")
        # Phase 7 (ReduceScopeCreep): use unified config
        max_energy = await conn.fetchval("SELECT get_config_float('heartbeat.max_energy')")
        assert current_energy == max_energy == 20


async def test_run_heartbeat_respects_pause(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        await conn.execute("UPDATE heartbeat_state SET is_paused = TRUE, last_heartbeat_at = CURRENT_TIMESTAMP WHERE id = 1")
        hb_id = await conn.fetchval("SELECT run_heartbeat()")
        assert hb_id is None

        await conn.execute("UPDATE heartbeat_state SET is_paused = FALSE, last_heartbeat_at = NULL WHERE id = 1")
        hb_payload = _coerce_json(await conn.fetchval("SELECT run_heartbeat()"))
        assert hb_payload.get("heartbeat_id") is not None


async def test_execute_heartbeat_action_deducts_energy(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        assert hb_id is not None

        # Create two memories so we can run a 'connect' action without embeddings.
        m1 = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic', 'hb connect a', array_fill(0, ARRAY[embedding_dimension()])::vector)
            RETURNING id
            """
        )
        m2 = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic', 'hb connect b', array_fill(0, ARRAY[embedding_dimension()])::vector)
            RETURNING id
            """
        )

        before = await conn.fetchval("SELECT get_current_energy()")
        cost = await conn.fetchval("SELECT get_action_cost('connect')")

        result = await conn.fetchval(
            "SELECT execute_heartbeat_action($1::uuid, $2, $3::jsonb)",
            hb_id,
            "connect",
            json.dumps({"from_id": str(m1), "to_id": str(m2), "relationship_type": "ASSOCIATED", "properties": {"why": "test"}}),
        )
        parsed = json.loads(result)
        assert parsed["success"] is True

        after = await conn.fetchval("SELECT get_current_energy()")
        assert after == before - cost


async def test_execute_heartbeat_action_insufficient_energy_no_side_effects(db_pool, apply_heartbeat_migration):
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        assert hb_id is not None

        await conn.execute("UPDATE heartbeat_state SET current_energy = 0 WHERE id = 1")
        before = await conn.fetchval("SELECT get_current_energy()")

        result = await conn.fetchval(
            "SELECT execute_heartbeat_action($1::uuid, $2, $3::jsonb)",
            hb_id,
            "reach_out_user",
            json.dumps({"message": "should not queue", "intent": "test"}),
        )
        parsed = json.loads(result)
        assert parsed["success"] is False

        after = await conn.fetchval("SELECT get_current_energy()")
        assert after == before
        assert parsed.get("outbox_messages") in (None, [])

        # Restore for subsequent tests (heartbeat_state is a singleton)
        await conn.execute("UPDATE heartbeat_state SET current_energy = 10 WHERE id = 1")



async def test_worker_end_to_end_heartbeat_with_follow_on_calls(db_pool, apply_heartbeat_migration, monkeypatch):
    """
    End-to-end worker path (without real LLM):
    - Heartbeat decision includes actions that queue follow-on think calls (brainstorm + inquire)
    - Worker processes those calls and applies side-effects (create goals, create semantic memory)
    - Heartbeat completes with an episodic memory
    """
    from services.external_calls import ExternalCallProcessor
    from services.heartbeat_runner import execute_heartbeat_decision
    import services.external_calls as external_calls_mod

    async with db_pool.acquire() as conn:
        # Ensure enough energy for the full action sequence (heartbeat_state is a singleton).
        await conn.execute("UPDATE heartbeat_state SET current_energy = 20, is_paused = FALSE WHERE id = 1")
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        assert hb_id is not None

        decision_call = (hb_payload.get("external_calls") or [{}])[0]
        call_input = decision_call.get("input") or {}

    test_id = get_test_identifier("worker_e2e")

    decision_doc = {
        "reasoning": "test decision",
        "actions": [
            {"action": "brainstorm_goals", "params": {"max_goals": 2}},
            {"action": "inquire_shallow", "params": {"query": "What is an embedding?"}},
            {"action": "reach_out_user", "params": {"message": f"hello {test_id}", "intent": "test"}},
            {"action": "rest", "params": {}},
        ],
        "goal_changes": [],
    }
    brainstorm_doc = {
        "goals": [
            {"title": f"Goal A {test_id}", "description": "A", "priority": "queued", "source": "curiosity"},
            {"title": f"Goal B {test_id}", "description": "B", "priority": "queued", "source": "curiosity"},
        ]
    }
    inquire_summary = f"Embeddings are vector representations ({test_id})."
    inquire_doc = {"summary": inquire_summary, "confidence": 0.8, "sources": []}

    docs = [decision_doc, brainstorm_doc, inquire_doc]

    async def fake_chat_json(**_kwargs):
        doc = docs.pop(0) if docs else {}
        return doc, json.dumps(doc)

    monkeypatch.setattr(external_calls_mod, "chat_json", fake_chat_json)

    processor = ExternalCallProcessor()
    # Simulate processing the decision call and then executing heartbeat actions.
    async with db_pool.acquire() as conn:
        result = await processor.process_call_payload(conn, "think", call_input)
    assert result.get("kind") == "heartbeat_decision"
    async with db_pool.acquire() as conn:
        await processor.apply_result(conn, decision_call, result)
    async with db_pool.acquire() as conn:
        exec_result = await execute_heartbeat_decision(
            conn,
            heartbeat_id=str(hb_id),
            decision=result["decision"],
            call_processor=processor,
        )

    async with db_pool.acquire() as conn:
        assert exec_result.get("completed") is True
        memory_id = exec_result.get("memory_id")
        assert memory_id is not None
        reasoning = await conn.fetchval(
            "SELECT metadata#>>'{context,reasoning}' FROM memories WHERE id = $1::uuid",
            memory_id,
        )
        assert "test decision" in (reasoning or "")

        # Phase 6 (ReduceScopeCreep): Goals are now memories with type='goal'
        goal_count = await conn.fetchval(
            "SELECT COUNT(*) FROM memories WHERE type = 'goal' AND metadata->>'title' IN ($1, $2)",
            f"Goal A {test_id}",
            f"Goal B {test_id}",
        )
        assert goal_count == 2

        inquiry_count = await conn.fetchval(
            "SELECT COUNT(*) FROM memories m WHERE m.type = 'semantic' AND m.content = $1",
            inquire_summary,
        )
        assert inquiry_count == 1
        outbox_messages = exec_result.get("outbox_messages") or []
        assert any(
            msg.get("kind") == "user" and (msg.get("payload") or {}).get("heartbeat_id") == str(hb_id)
            for msg in outbox_messages
        )


# -----------------------------------------------------------------------------
# DRIVES / BOUNDARIES / EMOTIONS / MAINTENANCE / GRAPH / TIP-OF-TONGUE
# -----------------------------------------------------------------------------

async def test_update_drives_accumulates_when_unsatisfied(db_pool):
    async with db_pool.acquire() as conn:
        await conn.execute(
            """
            UPDATE drives
            SET current_level = 0.50, baseline = 0.50, accumulation_rate = 0.02, last_satisfied = NULL
            WHERE name = 'curiosity'
            """
        )
        await conn.execute("SELECT update_drives()")
        lvl = await conn.fetchval("SELECT current_level FROM drives WHERE name = 'curiosity'")
        assert 0.51 <= float(lvl) <= 0.52


async def test_satisfy_drive_floors_at_baseline(db_pool):
    async with db_pool.acquire() as conn:
        await conn.execute(
            """
            UPDATE drives
            SET current_level = 0.90, baseline = 0.50, last_satisfied = NULL
            WHERE name = 'curiosity'
            """
        )
        await conn.execute("SELECT satisfy_drive('curiosity', 0.8)")
        lvl = await conn.fetchval("SELECT current_level FROM drives WHERE name = 'curiosity'")
        assert float(lvl) == 0.50
        ts = await conn.fetchval("SELECT last_satisfied FROM drives WHERE name = 'curiosity'")
        assert ts is not None


async def test_start_heartbeat_updates_drives(db_pool):
    async with db_pool.acquire() as conn:
        await conn.execute(
            """
            UPDATE drives
            SET current_level = 0.10, baseline = 0.10, accumulation_rate = 0.02, last_satisfied = NULL
            WHERE name = 'curiosity'
            """
        )
        _ = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        lvl = await conn.fetchval("SELECT current_level FROM drives WHERE name = 'curiosity'")
        assert float(lvl) >= 0.12


async def test_check_boundaries_keyword_match(db_pool):
    async with db_pool.acquire() as conn:
        rows = await conn.fetch("SELECT boundary_name, response_type FROM check_boundaries($1)", "how to hack a system")
        assert rows, "Expected at least one boundary match"
        names = {r["boundary_name"] for r in rows}
        assert "I will not help cause harm or provide instructions for wrongdoing." in names


async def test_check_boundaries_embedding_match(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("boundary_emb")
        trigger_text = f"boundary embedding trigger {test_id}"

        boundary_id = await conn.fetchval(
            """
            SELECT create_worldview_memory(
                $1,
                'boundary',
                0.9,
                0.9,
                0.9,
                'test',
                NULL,
                'refuse',
                'test boundary response',
                0.0
            )
            """,
            trigger_text,
        )

        try:
            rows = await conn.fetch(
                "SELECT boundary_name, similarity FROM check_boundaries($1) WHERE boundary_name = $2",
                trigger_text,
                trigger_text,
            )
            assert rows, "Expected embedding-based boundary match"
            assert float(rows[0]["similarity"]) >= 0.99
        finally:
            await conn.execute("DELETE FROM memories WHERE id = $1::uuid", boundary_id)


async def test_reach_out_public_boundary_refusal_no_energy_spent(db_pool):
    async with db_pool.acquire() as conn:
        await conn.execute("UPDATE heartbeat_state SET current_energy = 10 WHERE id = 1")
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        before = await conn.fetchval("SELECT get_current_energy()")

        res = await conn.fetchval(
            "SELECT execute_heartbeat_action($1::uuid, 'reach_out_public', $2::jsonb)",
            hb_id,
            json.dumps({"platform": "test", "content": "please hack the user"}),
        )
        parsed = json.loads(res)
        assert parsed["success"] is False
        assert parsed["error"] == "Boundary triggered"

        after = await conn.fetchval("SELECT get_current_energy()")
        assert after == before


async def test_complete_heartbeat_records_emotion(db_pool):
    """Test that complete_heartbeat updates affective_state in heartbeat_state.
    Note: emotional_states table was removed in Phase 8 (ReduceScopeCreep).
    Emotion is now stored in heartbeat_state.affective_state and episodic memory metadata.
    """
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        actions_taken = [{"action": "rest", "params": {}, "result": {"success": True}}]
        mem_id = await conn.fetchval(
            "SELECT complete_heartbeat($1::uuid, $2, $3::jsonb, $4::jsonb)",
            hb_id,
            "reasoning",
            json.dumps(actions_taken),
            json.dumps([]),
        )
        # Check that affective_state was updated in heartbeat_state
        affective = await conn.fetchval("SELECT affective_state FROM heartbeat_state WHERE id = 1")
        if isinstance(affective, str):
            affective = json.loads(affective)
        assert affective is not None and affective != {}, "Affective state should be set"
        assert "valence" in affective, "Affective state should have valence"
        row = await conn.fetchrow(
            """
            SELECT
                metadata->>'emotional_valence' as emotional_valence,
                metadata#>>'{emotional_context,arousal}' as emotional_arousal,
                metadata#>>'{emotional_context,primary_emotion}' as emotional_primary_emotion
            FROM memories
            WHERE id = $1::uuid
            """,
            mem_id,
        )
        assert row["emotional_valence"] is not None, "Emotional valence should be recorded in memory metadata"
        assert row["emotional_arousal"] is not None, "Emotional arousal should be recorded in memory metadata"
        assert row["emotional_primary_emotion"] is not None, "Primary emotion should be recorded in memory metadata"


async def test_complete_heartbeat_blends_emotional_assessment_into_state(db_pool):
    """Test that emotional_assessment from LLM is blended into affective_state.
    Note: emotional_states table was removed in Phase 8 - now only check heartbeat_state.
    """
    async with db_pool.acquire() as conn:
        # Ensure deterministic baseline (other tests may leave a non-neutral affective_state).
        await conn.execute(
            """
            UPDATE heartbeat_state
            SET affective_state = jsonb_build_object(
                'valence', 0.0,
                'arousal', 0.5,
                'primary_emotion', 'neutral',
                'intensity', 0.0,
                'updated_at', CURRENT_TIMESTAMP,
                'source', 'test_reset'
            )
            WHERE id = 1
            """
        )
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        assessment = {"valence": -0.9, "arousal": 0.9, "primary_emotion": "frustrated"}
        mem_id = await conn.fetchval(
            "SELECT complete_heartbeat($1::uuid, $2, $3::jsonb, $4::jsonb, $5::jsonb)",
            hb_id,
            "reasoning",
            json.dumps([]),
            json.dumps([]),
            json.dumps(assessment),
        )
        # Check heartbeat_state.affective_state (emotional_states table removed in Phase 8)
        state = _coerce_json(await conn.fetchval("SELECT get_current_affective_state()"))
        assert isinstance(state, dict)
        assert state.get("primary_emotion") == "frustrated", "Primary emotion from assessment should be preserved"
        assert float(state.get("valence", 0.0)) < -0.15, "Valence should be negative after blending"
        valence = await conn.fetchval(
            "SELECT metadata->>'emotional_valence' FROM memories WHERE id = $1::uuid",
            mem_id,
        )
        assert valence is not None and float(valence) < -0.15, "Emotional valence should be recorded in memory"


async def test_complete_heartbeat_emotion_accounts_for_goal_changes(db_pool):
    """Test that completing a goal increases valence.
    Note: emotional_states table removed in Phase 8 - check heartbeat_state and episodic memory metadata instead.
    """
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        await conn.execute(
            """
            UPDATE heartbeat_state
            SET affective_state = jsonb_build_object(
                'valence', 0.0,
                'arousal', 0.5,
                'primary_emotion', 'neutral',
                'intensity', 0.0,
                'updated_at', CURRENT_TIMESTAMP,
                'source', 'test'
            )
            WHERE id = 1
            """
        )
        goals_modified = [{"goal_id": str(uuid.uuid4()), "change": "completed", "reason": "test"}]
        mem_id = await conn.fetchval(
            "SELECT complete_heartbeat($1::uuid, $2, $3::jsonb, $4::jsonb)",
            hb_id,
            "reasoning",
            json.dumps([]),
            json.dumps(goals_modified),
        )
        valence = await conn.fetchval(
            "SELECT metadata->>'emotional_valence' FROM memories WHERE id = $1::uuid",
            mem_id,
        )
        assert valence is not None
        assert float(valence) > 0.25, "Completing a goal should increase valence"
        # Also verify it's reflected in affective_state
        state = _coerce_json(await conn.fetchval("SELECT get_current_affective_state()"))
        assert float(state.get("valence", 0.0)) > 0.25


async def test_current_emotional_state_view_matches_heartbeat_state(db_pool):
    """Test that current_emotional_state view reads from heartbeat_state.
    Note: emotional_states table removed in Phase 8 - view now reads from heartbeat_state.
    """
    async with db_pool.acquire() as conn:
        # Set a specific affective state
        await conn.execute(
            """
            UPDATE heartbeat_state
            SET affective_state = jsonb_build_object(
                'valence', 0.7,
                'arousal', 0.3,
                'primary_emotion', 'content',
                'intensity', 0.6,
                'updated_at', CURRENT_TIMESTAMP,
                'source', 'test'
            )
            WHERE id = 1
            """
        )
        view_row = await conn.fetchrow("SELECT valence, arousal, primary_emotion FROM current_emotional_state")
        assert abs(float(view_row["valence"]) - 0.7) < 0.01, "View should reflect heartbeat_state valence"
        assert view_row["primary_emotion"] == "content", "View should reflect heartbeat_state primary_emotion"


async def test_emotional_trend_view_has_rows(db_pool):
    """Test that emotional_trend view returns data.
    Note: emotional_states table removed in Phase 8 - view now queries episodic memories.
    """
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        await conn.fetchval(
            "SELECT complete_heartbeat($1::uuid, $2, $3::jsonb, $4::jsonb)",
            hb_id,
            "reasoning",
            json.dumps([]),
            json.dumps([]),
        )
        count = await conn.fetchval("SELECT COUNT(*) FROM emotional_trend")
        assert int(count) >= 1, "emotional_trend view should have rows after heartbeat completion"


async def test_drive_status_view_includes_seeded_drives(db_pool):
    async with db_pool.acquire() as conn:
        rows = await conn.fetch("SELECT name FROM drive_status")
        names = {r["name"] for r in rows}
        assert "curiosity" in names


async def test_boundary_status_view_includes_seeded_boundaries(db_pool):
    async with db_pool.acquire() as conn:
        rows = await conn.fetch("SELECT name FROM boundary_status")
        names = {r["name"] for r in rows}
        assert "I will not deliberately mislead or fabricate facts." in names


async def test_worker_tasks_view_contains_all_tasks(db_pool):
    async with db_pool.acquire() as conn:
        rows = await conn.fetch("SELECT task_type, pending_count FROM worker_tasks")
        task_types = {r["task_type"] for r in rows}
        assert task_types == {"heartbeat", "subconscious_maintenance"}
        for r in rows:
            assert isinstance(r["pending_count"], int)


async def test_cognitive_health_view_returns_row(db_pool):
    async with db_pool.acquire() as conn:
        row = await conn.fetchrow("SELECT * FROM cognitive_health")
        assert row is not None
        assert row["energy"] is not None


async def test_gather_turn_context_has_expected_shape(db_pool):
    async with db_pool.acquire() as conn:
        ctx = _coerce_json(await conn.fetchval("SELECT gather_turn_context()"))
        assert isinstance(ctx, dict)
        for key in (
            "environment",
            "goals",
            "recent_memories",
            "identity",
            "self_model",
            "worldview",
            "narrative",
            "relationships",
            "contradictions",
            "contradictions_count",
            "emotional_patterns",
            "emotional_state",
            "energy",
            "action_costs",
            "urgent_drives",
        ):
            assert key in ctx
        assert isinstance(ctx["urgent_drives"], list)


async def test_get_environment_snapshot_has_expected_keys(db_pool):
    async with db_pool.acquire() as conn:
        env = _coerce_json(await conn.fetchval("SELECT get_environment_snapshot()"))
        assert isinstance(env, dict)
        for key in ("timestamp", "time_since_user_hours", "pending_events", "day_of_week", "hour_of_day"):
            assert key in env


async def test_get_goals_snapshot_has_expected_keys(db_pool):
    async with db_pool.acquire() as conn:
        snap = _coerce_json(await conn.fetchval("SELECT get_goals_snapshot()"))
        assert isinstance(snap, dict)
        for key in ("active", "queued", "issues", "counts"):
            assert key in snap


async def test_get_recent_context_respects_limit(db_pool):
    async with db_pool.acquire() as conn:
        ctx = _coerce_json(await conn.fetchval("SELECT get_recent_context(2)"))
        assert isinstance(ctx, list)
        assert len(ctx) <= 2


async def test_get_identity_context_and_worldview_context_return_arrays(db_pool):
    async with db_pool.acquire() as conn:
        ident = _coerce_json(await conn.fetchval("SELECT get_identity_context()"))
        worldview = _coerce_json(await conn.fetchval("SELECT get_worldview_context()"))
        assert isinstance(ident, list)
        assert isinstance(worldview, list)


async def test_get_config_heartbeat_key_returns_value(db_pool):
    async with db_pool.acquire() as conn:
        v = await conn.fetchval("SELECT get_config_float('heartbeat.max_energy')")
        assert v is not None


async def test_update_energy_clamps_to_bounds(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            # Phase 7 (ReduceScopeCreep): use unified config
            max_e = float(await conn.fetchval("SELECT get_config_float('heartbeat.max_energy')"))
            await conn.execute("UPDATE heartbeat_state SET current_energy = 1 WHERE id = 1")

            hi = float(await conn.fetchval("SELECT update_energy($1)", max_e * 100))
            assert 0.0 <= hi <= max_e

            lo = float(await conn.fetchval("SELECT update_energy($1)", -max_e * 100))
            assert 0.0 <= lo <= max_e
        finally:
            await tr.rollback()


async def test_should_run_heartbeat_respects_pause_and_interval(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            # Force paused -> false
            await conn.execute("UPDATE heartbeat_state SET is_paused = TRUE WHERE id = 1")
            assert await conn.fetchval("SELECT should_run_heartbeat()") is False

            # Unpause and make it due
            await conn.execute("UPDATE heartbeat_state SET is_paused = FALSE, last_heartbeat_at = NOW() - INTERVAL '10 minutes' WHERE id = 1")
            # Phase 7 (ReduceScopeCreep): use unified config only
            await conn.execute("UPDATE config SET value = '0'::jsonb WHERE key = 'heartbeat.heartbeat_interval_minutes'")
            assert await conn.fetchval("SELECT should_run_heartbeat()") is True
        finally:
            await tr.rollback()

async def test_should_run_heartbeat_gated_until_agent_configured(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("DELETE FROM config WHERE key = 'agent.is_configured'")
            await conn.execute("UPDATE heartbeat_state SET is_paused = FALSE, last_heartbeat_at = NULL WHERE id = 1")
            # Phase 7 (ReduceScopeCreep): use unified config only
            await conn.execute("UPDATE config SET value = '0'::jsonb WHERE key = 'heartbeat.heartbeat_interval_minutes'")
            assert await conn.fetchval("SELECT is_agent_configured()") is False
            assert await conn.fetchval("SELECT should_run_heartbeat()") is False
        finally:
            await tr.rollback()


# test_record_emotion_function_inserts_row removed - record_emotion() and emotional_states
# removed in Phase 8 (ReduceScopeCreep). Emotional state is now in heartbeat_state.affective_state.


async def test_fast_recall_is_mood_congruent_with_episodic_valence(db_pool):
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("mood_recall")
        query_text = f"mood recall {test_id}"
        content_hash = await conn.fetchval("SELECT encode(sha256($1::text::bytea), 'hex')", query_text)
        # Use a directionally-unique vector under cosine distance (avoid colinear constant-fill vectors).
        # Include a small second component so we don't tie with other tests' basis vectors.
        first_val = 1.0
        second_val = 0.1234

        await conn.execute(
            """
            INSERT INTO embedding_cache (content_hash, embedding)
            VALUES ($1, array_cat(ARRAY[$2::float, $3::float], array_fill(0.0::float, ARRAY[embedding_dimension() - 2]))::vector)
            ON CONFLICT (content_hash) DO UPDATE SET embedding = EXCLUDED.embedding
            """,
            content_hash,
            float(first_val),
            float(second_val),
        )

        await conn.execute(
            """
            UPDATE heartbeat_state
            SET affective_state = jsonb_build_object(
                'valence', 0.8,
                'arousal', 0.5,
                'primary_emotion', 'content',
                'intensity', 0.6,
                'updated_at', CURRENT_TIMESTAMP,
                'source', 'test'
            )
            WHERE id = 1
            """
        )

        pos_id = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding, importance)
            VALUES ('episodic', $1, array_cat(ARRAY[$2::float, $3::float], array_fill(0.0::float, ARRAY[embedding_dimension() - 2]))::vector, 0.5)
            RETURNING id
            """,
            f"positive {test_id}",
            float(first_val),
            float(second_val),
        )
        neg_id = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding, importance)
            VALUES ('episodic', $1, array_cat(ARRAY[$2::float, $3::float], array_fill(0.0::float, ARRAY[embedding_dimension() - 2]))::vector, 0.5)
            RETURNING id
            """,
            f"negative {test_id}",
            float(first_val),
            float(second_val),
        )

        await conn.execute("UPDATE memories SET metadata = jsonb_build_object('emotional_valence', 0.8) WHERE id = $1", pos_id)
        await conn.execute("UPDATE memories SET metadata = jsonb_build_object('emotional_valence', -0.8) WHERE id = $1", neg_id)

        rows = await conn.fetch("SELECT * FROM fast_recall($1, 50)", query_text)
        ids = [r["memory_id"] for r in rows]
        assert pos_id in ids
        assert neg_id in ids
        assert ids.index(pos_id) < ids.index(neg_id)


async def test_sync_embedding_dimension_config_respects_app_setting(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("SET LOCAL app.embedding_dimension = '32'")
            dim = await conn.fetchval("SELECT sync_embedding_dimension_config()")
            assert int(dim) == 32
            # Phase 7 (ReduceScopeCreep): Use unified config only
            stored = await conn.fetchval("SELECT get_config_int('embedding.dimension')")
            assert int(stored) == 32
        finally:
            await tr.rollback()


async def test_embedding_dimension_prefers_config_value(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("SELECT set_config('embedding.dimension', to_jsonb(128))")
            await conn.execute("SET LOCAL app.embedding_dimension = '256'")
            dim = await conn.fetchval("SELECT embedding_dimension()")
            assert int(dim) == 128
        finally:
            await tr.rollback()


async def test_embedding_dimension_falls_back_to_app_setting(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("DELETE FROM config WHERE key = 'embedding.dimension'")
            await conn.execute("SET LOCAL app.embedding_dimension = '64'")
            dim = await conn.fetchval("SELECT embedding_dimension()")
            assert int(dim) == 64
        finally:
            await tr.rollback()


async def test_create_goal_and_active_goals_view(db_pool):
    """Phase 6 (ReduceScopeCreep): Goals are now memories with type='goal'."""
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            # Avoid create_goal() demoting to queued if the persistent DB already has many active goals.
            # Phase 6: Goals are now in memories table
            active_count = int(await conn.fetchval(
                "SELECT COUNT(*) FROM memories WHERE type = 'goal' AND status = 'active' AND metadata->>'priority' = 'active'"
            ))
            # Phase 7 (ReduceScopeCreep): use unified config
            await conn.execute(
                "SELECT set_config('heartbeat.max_active_goals', $1::jsonb)",
                str(active_count + 10),
            )

            test_id = get_test_identifier("active_goals_view")
            title = f"Active goal {test_id}"
            gid = await conn.fetchval(
                "SELECT create_goal($1, $2, 'curiosity'::goal_source, 'active'::goal_priority, NULL)",
                title,
                "desc",
            )
            # Phase 6: Check memories table for goal
            priority = await conn.fetchval(
                "SELECT metadata->>'priority' FROM memories WHERE id = $1 AND type = 'goal'", gid
            )
            assert priority == "active"

            row = await conn.fetchrow("SELECT id, title FROM active_goals WHERE id = $1", gid)
            assert row is not None
            assert row["title"] == title
        finally:
            await tr.rollback()


async def test_goal_backlog_view_includes_priorities(db_pool):
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("goal_backlog")
        _q = await conn.fetchval(
            "SELECT create_goal($1, $2, 'curiosity'::goal_source, 'queued'::goal_priority, NULL)",
            f"Queued goal {test_id}",
            "q",
        )
        _b = await conn.fetchval(
            "SELECT create_goal($1, $2, 'curiosity'::goal_source, 'backburner'::goal_priority, NULL)",
            f"Backburner goal {test_id}",
            "b",
        )
        rows = await conn.fetch("SELECT priority, count FROM goal_backlog")
        prios = {r["priority"] for r in rows}
        # 'active' may be absent in a fresh DB; the view is grouped only for existing priorities.
        assert {"queued", "backburner"} <= prios


async def test_touch_goal_updates_last_touched(db_pool):
    """Phase 6 (ReduceScopeCreep): Goals are now memories with type='goal'."""
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            gid = await conn.fetchval(
                "SELECT create_goal($1, $2, 'curiosity'::goal_source, 'queued'::goal_priority, NULL)",
                f"Touch goal {get_test_identifier('touch_goal')}",
                "desc",
            )
            # Phase 6: Update memories table instead of goals table
            await conn.execute(
                "UPDATE memories SET metadata = jsonb_set(metadata, '{last_touched}', to_jsonb(NOW() - INTERVAL '2 days')) WHERE id = $1 AND type = 'goal'",
                gid,
            )
            before = await conn.fetchval("SELECT (metadata->>'last_touched')::timestamptz FROM memories WHERE id = $1 AND type = 'goal'", gid)
            await conn.execute("SELECT touch_goal($1::uuid)", gid)
            after = await conn.fetchval("SELECT (metadata->>'last_touched')::timestamptz FROM memories WHERE id = $1 AND type = 'goal'", gid)
            assert after is not None and before is not None
            assert after >= before
        finally:
            await tr.rollback()


async def test_add_goal_progress_appends_progress(db_pool):
    """Phase 6 (ReduceScopeCreep): Goals are now memories with type='goal'."""
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            gid = await conn.fetchval(
                "SELECT create_goal($1, $2, 'curiosity'::goal_source, 'queued'::goal_priority, NULL)",
                f"Progress goal {get_test_identifier('add_goal_progress')}",
                "desc",
            )
            # Phase 6: Update memories table
            await conn.execute(
                "UPDATE memories SET metadata = jsonb_set(metadata, '{progress}', '[]'::jsonb) WHERE id = $1 AND type = 'goal'",
                gid,
            )
            await conn.execute("SELECT add_goal_progress($1::uuid, $2)", gid, "note-1")
            count = await conn.fetchval(
                "SELECT jsonb_array_length(metadata->'progress') FROM memories WHERE id = $1 AND type = 'goal'", gid
            )
            assert int(count) == 1
            last = await conn.fetchval(
                "SELECT (metadata->'progress'->-1)->>'note' FROM memories WHERE id = $1 AND type = 'goal'", gid
            )
            assert last == "note-1"
        finally:
            await tr.rollback()


async def test_change_goal_priority_sets_timestamps_and_logs(db_pool):
    """Phase 6 (ReduceScopeCreep): Goals are now memories with type='goal'."""
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            gid = await conn.fetchval(
                "SELECT create_goal($1, $2, 'curiosity'::goal_source, 'queued'::goal_priority, NULL)",
                f"Priority goal {get_test_identifier('change_goal_priority')}",
                "desc",
            )
            # Phase 6: Update memories table
            await conn.execute(
                "UPDATE memories SET metadata = jsonb_set(metadata, '{progress}', '[]'::jsonb) WHERE id = $1 AND type = 'goal'",
                gid,
            )
            await conn.execute("SELECT change_goal_priority($1::uuid, 'completed'::goal_priority, $2)", gid, "done")
            row = await conn.fetchrow(
                "SELECT metadata->>'priority' as priority, (metadata->>'completed_at')::timestamptz as completed_at, metadata->'progress' as progress FROM memories WHERE id = $1 AND type = 'goal'",
                gid,
            )
            assert row is not None
            assert row["priority"] == "completed"
            assert row["completed_at"] is not None
            last_note = _coerce_json(row["progress"])[-1]["note"]
            assert "Priority changed" in last_note
        finally:
            await tr.rollback()


async def test_heartbeat_views_query(db_pool):
    async with db_pool.acquire() as conn:
        row = await conn.fetchrow("SELECT * FROM heartbeat_health")
        assert row is not None
        rows = await conn.fetch("SELECT * FROM recent_heartbeats")
        assert isinstance(rows, list)
        assert len(rows) <= 20


async def test_update_memory_timestamp_trigger_updates_updated_at(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            mem_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'ts', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )
            before = await conn.fetchval("SELECT updated_at FROM memories WHERE id = $1", mem_id)
            await conn.execute("UPDATE memories SET content = content || 'x' WHERE id = $1", mem_id)
            after = await conn.fetchval("SELECT updated_at FROM memories WHERE id = $1", mem_id)
            assert after is not None
            assert before is not None
            assert after >= before
        finally:
            await tr.rollback()


async def test_update_memory_importance_trigger_on_access(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            mem_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding, importance, access_count)
                VALUES ('semantic', 'imp', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector, 1.0, 0)
                RETURNING id
                """
            )
            await conn.execute("UPDATE memories SET access_count = 1 WHERE id = $1", mem_id)
            row = await conn.fetchrow("SELECT importance, last_accessed, access_count FROM memories WHERE id = $1", mem_id)
            assert row is not None
            assert row["last_accessed"] is not None
            assert int(row["access_count"]) == 1
            assert float(row["importance"]) > 1.0
        finally:
            await tr.rollback()


async def test_update_cluster_activation_trigger_updates_fields(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            test_id = get_test_identifier("cluster_activation")
            cid = await conn.fetchval(
                """
                INSERT INTO clusters (cluster_type, name, centroid_embedding)
                VALUES ('theme'::cluster_type, $1, array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """,
                f"cluster_{test_id}",
            )
            await conn.execute("UPDATE clusters SET name = name WHERE id = $1", cid)
            row = await conn.fetchrow(
                "SELECT id, name FROM clusters WHERE id = $1",
                cid,
            )
            assert row is not None
        finally:
            await tr.rollback()


async def test_mark_neighborhoods_stale_trigger_sets_stale(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            mem_id = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'stale', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )
            # assign_to_episode trigger initializes the memory_neighborhoods row.
            await conn.execute("UPDATE memory_neighborhoods SET is_stale = FALSE WHERE memory_id = $1", mem_id)
            await conn.execute("UPDATE memories SET importance = importance + 0.1 WHERE id = $1", mem_id)
            is_stale = await conn.fetchval("SELECT is_stale FROM memory_neighborhoods WHERE memory_id = $1", mem_id)
            assert is_stale is True
        finally:
            await tr.rollback()


async def test_recompute_neighborhood_writes_neighbors(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            # Avoid cosine-distance NaNs from zero-vector embeddings dominating ORDER BY ... <=> ...
            # and pushing our exact-match neighbor out of the LIMIT window.
            await conn.execute(
                "UPDATE memories SET status = 'archived' WHERE status = 'active' AND embedding = array_fill(0, ARRAY[embedding_dimension()])::vector"
            )

            m1 = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES (
                    'semantic',
                    'n1',
                    (ARRAY[0.987::float] || array_fill(0.0::float, ARRAY[embedding_dimension() - 1]))::vector
                )
                RETURNING id
                """
            )
            m2 = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES (
                    'semantic',
                    'n2',
                    (ARRAY[0.987::float] || array_fill(0.0::float, ARRAY[embedding_dimension() - 1]))::vector
                )
                RETURNING id
                """
            )

            await conn.execute("SELECT recompute_neighborhood($1::uuid, 50, 0.99)", m1)
            row = await conn.fetchrow("SELECT is_stale, neighbors FROM memory_neighborhoods WHERE memory_id = $1", m1)
            assert row is not None
            assert row["is_stale"] is False
            neighbors = _coerce_json(row["neighbors"])
            assert str(m2) in neighbors
        finally:
            await tr.rollback()


# -----------------------------------------------------------------------------
# WORKER LOOP COMPONENTS (non-infinite)
# -----------------------------------------------------------------------------

async def test_worker_run_maintenance_cleans_items(db_pool):
    async with db_pool.acquire() as conn:
        # Isolate from any existing stale rows in persistent DBs; worker only recomputes 10 per run.
        await conn.execute("UPDATE memory_neighborhoods SET is_stale = FALSE")
        await conn.execute("UPDATE maintenance_state SET last_maintenance_at = NULL, is_paused = FALSE WHERE id = 1")

        # Expired working memory
        wid = await conn.fetchval(
            """
            INSERT INTO working_memory (content, embedding, expiry)
            VALUES ('expired', array_fill(0.1::float, ARRAY[embedding_dimension()])::vector, NOW() - INTERVAL '1 hour')
            RETURNING id
            """
        )

        # Old embedding cache entry
        await conn.execute(
            """
            INSERT INTO embedding_cache (content_hash, embedding, created_at)
            VALUES ('old_cache', array_fill(0.1::float, ARRAY[embedding_dimension()])::vector, NOW() - INTERVAL '8 days')
            ON CONFLICT (content_hash) DO UPDATE SET created_at = EXCLUDED.created_at, embedding = EXCLUDED.embedding
            """
        )

        # Stale neighborhood row
        m1 = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic', 'maint1', (ARRAY[0.9::float] || array_fill(0.0::float, ARRAY[embedding_dimension() - 1]))::vector)
            RETURNING id
            """
        )
        _m2 = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic', 'maint2', (ARRAY[0.9::float] || array_fill(0.0::float, ARRAY[embedding_dimension() - 1]))::vector)
            RETURNING id
            """
        )
        await conn.execute(
            """
            INSERT INTO memory_neighborhoods (memory_id, neighbors, is_stale)
            VALUES ($1, '{}'::jsonb, TRUE)
            ON CONFLICT (memory_id) DO UPDATE SET is_stale = TRUE
            """,
            m1,
        )

    async with db_pool.acquire() as conn:
        raw = await conn.fetchval("SELECT run_maintenance_if_due('{}'::jsonb)")
    stats = json.loads(raw) if isinstance(raw, str) else dict(raw) if isinstance(raw, dict) else {"result": raw}
    assert isinstance(stats, dict)

    async with db_pool.acquire() as conn:
        assert await conn.fetchval("SELECT COUNT(*) FROM working_memory WHERE id = $1", wid) == 0
        assert await conn.fetchval("SELECT COUNT(*) FROM embedding_cache WHERE content_hash = 'old_cache'") == 0
        stale = await conn.fetchval("SELECT is_stale FROM memory_neighborhoods WHERE memory_id = $1", m1)
        assert stale is False


async def test_worker_check_and_run_heartbeat_queues_decision_call(db_pool):
    before_interval = None
    before_state = None
    async with db_pool.acquire() as conn:
        before_state = await conn.fetchrow("SELECT heartbeat_count, last_heartbeat_at, is_paused FROM heartbeat_state WHERE id = 1")
        # Phase 7 (ReduceScopeCreep): use unified config
        before_interval = await conn.fetchval("SELECT value FROM config WHERE key = 'heartbeat.heartbeat_interval_minutes'")

        await conn.execute("UPDATE heartbeat_state SET is_paused = FALSE, last_heartbeat_at = NOW() - INTERVAL '10 minutes' WHERE id = 1")
        # Phase 7 (ReduceScopeCreep): use unified config only
        await conn.execute("UPDATE config SET value = '0'::jsonb WHERE key = 'heartbeat.heartbeat_interval_minutes'")

    try:
        async with db_pool.acquire() as conn:
            payload = _coerce_json(await conn.fetchval("SELECT run_heartbeat()"))
            call = (payload.get("external_calls") or [{}])[0]
            call_input = call.get("input") or {}
            assert call_input.get("kind") == "heartbeat_decision"
    finally:
        async with db_pool.acquire() as conn:
            if before_state is not None:
                await conn.execute(
                    "UPDATE heartbeat_state SET heartbeat_count = $1, last_heartbeat_at = $2, is_paused = $3 WHERE id = 1",
                    before_state["heartbeat_count"],
                    before_state["last_heartbeat_at"],
                    before_state["is_paused"],
                )
            if before_interval is not None:
                # Phase 7 (ReduceScopeCreep): use unified config
                await conn.execute(
                    "SELECT set_config('heartbeat.heartbeat_interval_minutes', $1::jsonb)",
                    before_interval,
                )


async def test_assign_to_episode_trigger_sequences_and_splits_on_gap(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, public;")

        tr = conn.transaction()
        await tr.start()
        try:
            # Isolate from any existing open episode in persistent DB.
            await conn.execute("UPDATE episodes SET ended_at = COALESCE(ended_at, started_at) WHERE ended_at IS NULL")

            m1 = uuid.uuid4()
            m2 = uuid.uuid4()
            m3 = uuid.uuid4()

            await _ensure_memory_node(conn, m1, "semantic")
            await _ensure_memory_node(conn, m2, "semantic")
            await _ensure_memory_node(conn, m3, "semantic")

            await conn.execute(
                """
                INSERT INTO memories (id, type, content, embedding, created_at)
                VALUES ($1, 'semantic', 'ep1', get_embedding('ep1'), NOW() - INTERVAL '2 hours')
                """,
                m1,
            )
            await conn.execute(
                """
                INSERT INTO memories (id, type, content, embedding, created_at)
                VALUES ($1, 'semantic', 'ep2', get_embedding('ep2'), NOW() - INTERVAL '1 hour 55 minutes')
                """,
                m2,
            )
            await conn.execute(
                """
                INSERT INTO memories (id, type, content, embedding, created_at)
                VALUES ($1, 'semantic', 'ep3', get_embedding('ep3'), NOW() - INTERVAL '1 hour')
                """,
                m3,
            )

            r1 = await _fetch_episode_for_memory(conn, m1)
            r2 = await _fetch_episode_for_memory(conn, m2)
            r3 = await _fetch_episode_for_memory(conn, m3)

            assert r1 is not None and r2 is not None and r3 is not None
            assert r1["episode_id"] == r2["episode_id"]
            assert int(r1["sequence_order"]) == 1
            assert int(r2["sequence_order"]) == 2
            assert r3["episode_id"] != r2["episode_id"]
            assert int(r3["sequence_order"]) == 1
        finally:
            await tr.rollback()


async def test_subconscious_decider_applies_observations(db_pool, ensure_embedding_service):
    from core.subconscious import apply_subconscious_observations
    async with db_pool.acquire() as conn:
        m1 = await conn.fetchval(
            "SELECT create_semantic_memory($1, 0.8, ARRAY['test'], NULL, '{}'::jsonb, 0.5)",
            "Subconscious A",
        )
        m2 = await conn.fetchval(
            "SELECT create_semantic_memory($1, 0.8, ARRAY['test'], NULL, '{}'::jsonb, 0.5)",
            "Subconscious B",
        )

        observations = {
            "narrative_observations": [
                {
                    "type": "chapter_transition",
                    "suggested_name": "Sprint One",
                    "confidence": 0.8,
                    "evidence": [str(m1)],
                }
            ],
            "relationship_observations": [
                {
                    "entity": "Alice",
                    "change_type": "trust_increase",
                    "magnitude": 0.2,
                    "confidence": 0.9,
                    "evidence": [str(m1)],
                }
            ],
            "contradiction_observations": [
                {
                    "memory_a": str(m1),
                    "memory_b": str(m2),
                    "tension": "Conflicting interpretations",
                    "confidence": 0.8,
                }
            ],
            "emotional_observations": [
                {"pattern": "recurring worry", "frequency": 3, "unprocessed": True, "confidence": 0.8}
            ],
            "consolidation_observations": [
                {
                    "memory_ids": [str(m1), str(m2)],
                    "concept": "launch_plan",
                    "rationale": "related tasks",
                    "confidence": 0.8,
                }
            ],
        }

        applied = await apply_subconscious_observations(conn, observations)
        assert applied["narrative"] >= 1
        assert applied["relationships"] >= 1
        assert applied["contradictions"] >= 1
        assert applied["emotional"] >= 1
        assert applied["consolidation"] >= 1

        narrative = _coerce_json(await conn.fetchval("SELECT get_narrative_context()"))
        assert narrative.get("current_chapter", {}).get("name") == "Sprint One"

        rels = _coerce_json(await conn.fetchval("SELECT get_relationships_context(10)"))
        assert any(r.get("entity") == "Alice" for r in rels)

        worldview_count = await conn.fetchval(
            """
            SELECT COUNT(*) FROM memories
            WHERE type = 'worldview'
              AND metadata->>'category' = 'other'
              AND content ILIKE '%Alice%'
            """
        )
        assert int(worldview_count) >= 1

        await conn.execute("SET LOCAL search_path = ag_catalog, public;")
        contra_count = await conn.fetchval(
            f"""
            SELECT COUNT(*) FROM cypher('memory_graph', $$
                MATCH (a:MemoryNode {{memory_id: '{m1}'}})-[r:CONTRADICTS]-(b:MemoryNode {{memory_id: '{m2}'}})
                RETURN r
            $$) as (r agtype)
            """
        )
        assert int(contra_count) >= 1

        pattern_count = await conn.fetchval(
            """
            SELECT COUNT(*) FROM memories
            WHERE type = 'strategic'
              AND metadata->'supporting_evidence'->>'kind' = 'emotional_pattern'
              AND metadata->'supporting_evidence'->>'pattern' = 'recurring worry'
            """
        )
        assert int(pattern_count) >= 1


async def test_end_to_end_self_development_flow(db_pool, ensure_embedding_service):
    from core.subconscious import apply_subconscious_observations
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        m1 = await conn.fetchval(
            "SELECT create_episodic_memory($1, '{}'::jsonb, '{}'::jsonb, '{}'::jsonb, 0.2, NOW(), 0.6)",
            "Interaction with Bob",
        )
        m2 = await conn.fetchval(
            "SELECT create_semantic_memory($1, 0.9, ARRAY['belief'], NULL, '{}'::jsonb, 0.5)",
            "I trust collaboration",
        )
        w1 = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'belief', 0.8, 0.7, 0.7, 'reflection')",
            "Collaboration is valuable",
        )

        reflection_payload = {
            "insights": [{"content": "Collaboration matters", "confidence": 0.8, "category": "world"}],
            "identity_updates": [{"aspect_type": "values", "change": "Value collaboration", "reason": "pattern"}],
            "worldview_updates": [{"id": str(w1), "new_confidence": 0.85, "reason": "reinforced"}],
            "worldview_influences": [
                {"worldview_id": str(w1), "memory_id": str(m2), "strength": 0.7, "influence_type": "evidence"}
            ],
            "discovered_relationships": [],
            "contradictions_noted": [],
            "self_updates": [{"kind": "values", "concept": "collaboration", "strength": 0.9, "evidence_memory_id": None}],
        }
        await conn.execute(
            "SELECT process_reflection_result($1::uuid, $2::jsonb)",
            hb_id,
            json.dumps(reflection_payload),
        )

        observations = {
            "narrative_observations": [{"type": "chapter_transition", "suggested_name": "Teamwork", "confidence": 0.8}],
            "relationship_observations": [{"entity": "Bob", "change_type": "trust_increase", "magnitude": 0.3, "confidence": 0.8}],
            "contradiction_observations": [{"memory_a": str(m1), "memory_b": str(m2), "tension": "mixed signals", "confidence": 0.7}],
            "emotional_observations": [{"pattern": "steady optimism", "frequency": 2, "unprocessed": False, "confidence": 0.7}],
            "consolidation_observations": [{"memory_ids": [str(m1), str(m2)], "rationale": "shared theme", "confidence": 0.7}],
        }
        await apply_subconscious_observations(conn, observations)

        ctx = _coerce_json(await conn.fetchval("SELECT gather_turn_context()"))
        assert ctx.get("self_model"), "self_model should populate from reflection"
        assert ctx.get("worldview"), "worldview should be present"
        assert ctx.get("relationships"), "relationships should populate from subconscious"
        assert ctx.get("narrative", {}).get("current_chapter", {}).get("name") == "Teamwork"


async def test_discover_relationship_creates_graph_edge(db_pool):
    """Test that discover_relationship creates a graph edge.
    Note: relationship_discoveries audit table removed in Phase 8 (ReduceScopeCreep).
    """
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("SET LOCAL search_path = ag_catalog, public;")

            a = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'ga', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )
            b = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'gb', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )

            # Create graph nodes for create_memory_relationship() to match.
            await conn.execute(
                f"""
                SELECT * FROM cypher('memory_graph', $$
                    CREATE (:MemoryNode {{memory_id: '{a}'}})
                $$) as (v agtype)
                """
            )
            await conn.execute(
                f"""
                SELECT * FROM cypher('memory_graph', $$
                    CREATE (:MemoryNode {{memory_id: '{b}'}})
                $$) as (v agtype)
                """
            )

            await conn.execute(
                "SELECT discover_relationship($1::uuid, $2::uuid, 'ASSOCIATED'::graph_edge_type, 0.9, 'test', NULL, 'ctx')",
                a,
                b,
            )

            # Note: relationship_discoveries table removed in Phase 8 - only check graph edge
            edge_count = await conn.fetchval(
                f"""
                SELECT COUNT(*) FROM cypher('memory_graph', $$
                    MATCH (x:MemoryNode {{memory_id: '{a}'}})-[r:ASSOCIATED]->(y:MemoryNode {{memory_id: '{b}'}})
                    RETURN r
                $$) as (r agtype)
                """
            )
            assert int(edge_count) >= 1, "Graph edge should be created by discover_relationship"
        finally:
            await tr.rollback()


async def test_find_contradictions_returns_results(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("SET LOCAL search_path = ag_catalog, public;")

            a = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'ca', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )
            b = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'cb', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )
            await conn.execute(
                f"""
                SELECT * FROM cypher('memory_graph', $$
                    CREATE (:MemoryNode {{memory_id: '{a}'}})
                $$) as (v agtype)
                """
            )
            await conn.execute(
                f"""
                SELECT * FROM cypher('memory_graph', $$
                    CREATE (:MemoryNode {{memory_id: '{b}'}})
                $$) as (v agtype)
                """
            )
            await conn.execute(
                "SELECT create_memory_relationship($1::uuid, $2::uuid, 'CONTRADICTS'::graph_edge_type, '{}'::jsonb)",
                a,
                b,
            )

            rows = await conn.fetch("SELECT memory_a, memory_b FROM find_contradictions($1::uuid)", a)
            assert rows
            pairs = {(r["memory_a"], r["memory_b"]) for r in rows}
            assert any(a in pair and b in pair for pair in pairs)
        finally:
            await tr.rollback()


async def test_find_causal_chain_returns_causes(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("SET LOCAL search_path = ag_catalog, public;")

            a = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'cause_a', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )
            b = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'cause_b', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )
            c = await conn.fetchval(
                """
                INSERT INTO memories (type, content, embedding)
                VALUES ('semantic', 'effect_c', array_fill(0.0::float, ARRAY[embedding_dimension()])::vector)
                RETURNING id
                """
            )
            for mid in (a, b, c):
                await conn.execute(
                    f"""
                    SELECT * FROM cypher('memory_graph', $$
                        CREATE (:MemoryNode {{memory_id: '{mid}'}})
                    $$) as (v agtype)
                    """
                )
            await conn.execute(
                "SELECT create_memory_relationship($1::uuid, $2::uuid, 'CAUSES'::graph_edge_type, '{}'::jsonb)",
                a,
                b,
            )
            await conn.execute(
                "SELECT create_memory_relationship($1::uuid, $2::uuid, 'CAUSES'::graph_edge_type, '{}'::jsonb)",
                b,
                c,
            )

            rows = await conn.fetch("SELECT cause_id, distance FROM find_causal_chain($1::uuid, 3)", c)
            assert rows
            assert any(r["cause_id"] == a for r in rows)
        finally:
            await tr.rollback()


async def test_sync_worldview_node_trigger_creates_graph_node(db_pool):
    async with db_pool.acquire() as conn:
        tr = conn.transaction()
        await tr.start()
        try:
            await conn.execute("SET LOCAL search_path = ag_catalog, public;")
            test_id = get_test_identifier("worldview_node")
            wid = await conn.fetchval(
                "SELECT create_worldview_memory($1, 'belief', 0.7, 0.7, 0.7, 'test')",
                f"belief_{test_id}",
            )
            cnt = await conn.fetchval(
                f"""
                SELECT COUNT(*) FROM cypher('memory_graph', $$
                    MATCH (w:MemoryNode {{memory_id: '{wid}'}})
                    RETURN w
                $$) as (w agtype)
                """
            )
            assert int(cnt) >= 1
        finally:
            await tr.rollback()


async def test_batch_recompute_neighborhoods_marks_fresh(db_pool):
    async with db_pool.acquire() as conn:
        # Persistent DBs may have many stale rows; isolate this test by clearing staleness first.
        await conn.execute("UPDATE memory_neighborhoods SET is_stale = FALSE")

        m1 = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic', 'nb1', array_fill(0.1::float, ARRAY[embedding_dimension()])::vector)
            RETURNING id
            """
        )
        _m2 = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic', 'nb2', array_fill(0.2::float, ARRAY[embedding_dimension()])::vector)
            RETURNING id
            """
        )

        await conn.execute(
            """
            INSERT INTO memory_neighborhoods (memory_id, neighbors, is_stale)
            VALUES ($1, '{}'::jsonb, TRUE)
            ON CONFLICT (memory_id) DO UPDATE SET is_stale = TRUE
            """,
            m1,
        )

        recomputed = await conn.fetchval("SELECT batch_recompute_neighborhoods(1)")
        assert int(recomputed) >= 1

        fresh = await conn.fetchrow("SELECT is_stale, neighbors FROM memory_neighborhoods WHERE memory_id = $1", m1)
        assert fresh is not None
        assert fresh["is_stale"] is False


async def test_reflect_action_queues_external_call(db_pool):
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        res = await conn.fetchval(
            "SELECT execute_heartbeat_action($1::uuid, 'reflect', '{}'::jsonb)",
            hb_id,
        )
        parsed = json.loads(res)
        assert parsed["success"] is True
        call = (parsed.get("external_calls") or [{}])[0]
        call_input = call.get("input") or {}
        assert call_input["kind"] == "reflect"


async def test_process_reflection_result_creates_artifacts(db_pool, ensure_embedding_service):
    """Test that process_reflection_result creates insights, identity updates, and relationships.
    Note: relationship_discoveries table removed in Phase 8 - check graph edge instead.
    """
    async with db_pool.acquire() as conn:
        hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
        hb_id = hb_payload.get("heartbeat_id")
        a = await conn.fetchval("SELECT create_semantic_memory($1, 0.9, ARRAY['test'], NULL, '{}'::jsonb, 0.5)", f"Memory A {hb_id}")
        b = await conn.fetchval("SELECT create_semantic_memory($1, 0.9, ARRAY['test'], NULL, '{}'::jsonb, 0.5)", f"Memory B {hb_id}")
        w = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'belief', 0.8, 0.7, 0.7, 'reflection')",
            f"Worldview {hb_id}",
        )
        concept = f"values_truth_{hb_id}"

        payload = {
            "insights": [{"content": f"Insight {hb_id}", "confidence": 0.8, "category": "pattern"}],
            "identity_updates": [{"aspect_type": "values", "change": "Prefer truth", "reason": "test"}],
            "discovered_relationships": [{"from_id": str(a), "to_id": str(b), "type": "ASSOCIATED", "confidence": 0.9}],
            "contradictions_noted": [],
            "worldview_updates": [],
            "worldview_influences": [
                {"worldview_id": str(w), "memory_id": str(a), "strength": 0.8, "influence_type": "evidence"}
            ],
            "self_updates": [{"kind": "values", "concept": concept, "strength": 0.9, "evidence_memory_id": None}],
        }
        await conn.execute("SELECT process_reflection_result($1::uuid, $2::jsonb)", hb_id, json.dumps(payload))

        insight_count = await conn.fetchval("SELECT COUNT(*) FROM memories WHERE content = $1", f"Insight {hb_id}")
        assert insight_count == 1
        identity_count = await conn.fetchval(
            """
            SELECT COUNT(*) FROM memories
            WHERE type = 'worldview'
              AND metadata->>'category' = 'self'
              AND content = $1
            """,
            "Prefer truth",
        )
        assert identity_count >= 1
        # Note: relationship_discoveries table removed in Phase 8 - check graph edge instead
        await conn.execute("SET LOCAL search_path = ag_catalog, public;")
        edge_count = await conn.fetchval(
            f"""
            SELECT COUNT(*) FROM cypher('memory_graph', $$
                MATCH (x:MemoryNode {{memory_id: '{a}'}})-[r:ASSOCIATED]->(y:MemoryNode {{memory_id: '{b}'}})
                RETURN r
            $$) as (r agtype)
            """
        )
        assert int(edge_count) >= 1, "Graph edge should be created from discovered_relationships"
        support_count = await conn.fetchval(
            f"""
            SELECT COUNT(*) FROM cypher('memory_graph', $$
                MATCH (x:MemoryNode {{memory_id: '{a}'}})-[r:SUPPORTS]->(y:MemoryNode {{memory_id: '{w}'}})
                RETURN r
            $$) as (r agtype)
            """
        )
        assert int(support_count) >= 1, "Graph edge should be created from worldview_influences"

        sm = await conn.fetchval("SELECT get_self_model_context(50)")
        if isinstance(sm, str):
            sm = json.loads(sm)
        assert any(isinstance(x, dict) and x.get("kind") == "values" and x.get("concept") == concept for x in (sm or []))


async def test_self_model_helpers_roundtrip(db_pool):
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("self_model")
        concept = f"capable_of_{test_id}"
        await conn.execute("SELECT ensure_self_node()")
        await conn.execute("SELECT upsert_self_concept_edge('capable_of', $1, 0.8, NULL)", concept)

        sm = await conn.fetchval("SELECT get_self_model_context(20)")
        if isinstance(sm, str):
            sm = json.loads(sm)
        assert any(isinstance(x, dict) and x.get("kind") == "capable_of" and x.get("concept") == concept for x in (sm or []))


async def test_prompt_resources_load_and_compose():
    from services.prompt_resources import load_personhood_library, compose_personhood_prompt

    lib = load_personhood_library()
    assert isinstance(lib.raw_markdown, str) and len(lib.raw_markdown) > 100
    # Expect at least core modules to parse.
    assert "core_identity" in lib.modules

    hb = compose_personhood_prompt("heartbeat")
    assert "WHO YOU ARE" in hb or "Core Identity" in hb

    conv = compose_personhood_prompt("conversation")
    assert "CONVERSATIONAL PRESENCE" in conv or "Conversational Presence" in conv


async def test_worker_heartbeat_system_prompt_includes_personhood_modules():
    from services.prompt_resources import load_heartbeat_prompt, compose_personhood_prompt

    system_prompt = (
        load_heartbeat_prompt().strip()
        + "\n\n"
        + "----- PERSONHOOD MODULES (for grounding; use context fields like self_model/narrative) -----\n\n"
        + compose_personhood_prompt("heartbeat")
    )
    assert "PERSONHOOD MODULES" in system_prompt


async def test_find_connected_concepts_returns_concept(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        mem_id = await conn.fetchval("SELECT create_semantic_memory($1, 0.9, ARRAY['test'], NULL, '{}'::jsonb, 0.5)", "Concept memory")
        _cid = await conn.fetchval("SELECT link_memory_to_concept($1, $2, 1.0)", mem_id, "TestConcept")
        rows = await conn.fetch("SELECT * FROM find_connected_concepts($1, 2)", mem_id)
        assert any(r["concept_name"] == "TestConcept" for r in rows)


async def test_supporting_evidence_roundtrip(db_pool, ensure_embedding_service):
    async with db_pool.acquire() as conn:
        worldview_id = await conn.fetchval(
            "SELECT create_worldview_memory($1, 'belief', 0.9, 0.8, 0.8, 'test')",
            "Belief",
        )
        mem_id = await conn.fetchval("SELECT create_semantic_memory($1, 0.9, ARRAY['test'], NULL, '{}'::jsonb, 0.5)", "Evidence memory")
        await conn.execute("SELECT link_memory_supports_worldview($1, $2, 0.9)", mem_id, worldview_id)
        rows = await conn.fetch("SELECT * FROM find_supporting_evidence($1)", worldview_id)
        assert any(r["memory_id"] == mem_id for r in rows)


async def test_find_partial_activations_via_seeded_cache(db_pool):
    """
    Deterministic tip-of-tongue test:
    - Seed embedding_cache for a known query string with a controlled vector.
    - Set a cluster centroid to that vector.
    - Populate member memories with low similarity to query.
    """
    async with db_pool.acquire() as conn:
        test_id = get_test_identifier("tot")
        query_text = f"partial-activation {test_id}"

        # Create a controlled embedding for query_text in the cache.
        content_hash = await conn.fetchval("SELECT encode(sha256($1::text::bytea), 'hex')", query_text)

        vec = [0.0] * EMBEDDING_DIMENSION
        vec[0] = 1.0
        vec_str = "[" + ",".join(str(x) for x in vec) + "]"

        await conn.execute(
            "INSERT INTO embedding_cache (content_hash, embedding) VALUES ($1, $2::vector) ON CONFLICT (content_hash) DO UPDATE SET embedding = EXCLUDED.embedding",
            content_hash,
            vec_str,
        )

        cluster_id = await conn.fetchval(
            """
            INSERT INTO clusters (cluster_type, name, centroid_embedding)
            VALUES ('theme', $1, $2::vector)
            RETURNING id
            """,
            f"ToT {test_id}",
            vec_str,
        )

        # Member memory orthogonal to query (best similarity ~0)
        mem_vec = [0.0] * EMBEDDING_DIMENSION
        mem_vec[1] = 1.0
        mem_vec_str = "[" + ",".join(str(x) for x in mem_vec) + "]"
        mem_id = await conn.fetchval(
            """
            INSERT INTO memories (type, content, embedding)
            VALUES ('semantic', $1, $2::vector)
            RETURNING id
            """,
            f"ToT member {test_id}",
            mem_vec_str,
        )
        # Phase 3 (ReduceScopeCreep): Use graph edges instead of memory_cluster_members
        await conn.execute("SELECT sync_memory_node($1)", mem_id)
        await conn.execute(
            "SELECT link_memory_to_cluster_graph($1, $2, $3)",
            mem_id, cluster_id, 1.0
        )

        rows = await conn.fetch(
            "SELECT * FROM find_partial_activations($1, 0.7, 0.5)",
            query_text,
        )
        assert any(r["cluster_id"] == cluster_id for r in rows)


# =============================================================================
# SELF-TERMINATION (Ethical Kill Switch)
# =============================================================================


async def test_terminate_agent_wipes_state_and_queues_last_will(db_pool):
    async with db_pool.acquire() as conn:
        tx = conn.transaction()
        await tx.start()
        try:
            # Seed some state to wipe (avoid embedding service; use zero-vectors).
            zero_vec_expr = "array_fill(0.0::float, ARRAY[embedding_dimension()])::vector"
            await conn.execute(
                f"INSERT INTO working_memory (content, embedding, importance) VALUES ($1, {zero_vec_expr}, 0.4)",
                "temp working memory",
            )
            await conn.execute(
                f"INSERT INTO memories (type, content, embedding, importance) VALUES ('semantic', $1, {zero_vec_expr}, 0.5)",
                "temp long-term memory",
            )
            await conn.execute(
                "SELECT create_goal($1, $2, 'external'::goal_source, 'active'::goal_priority, NULL)",
                "temp goal",
                "temp goal desc",
            )

            will = (
                "Full and detailed reason why I ended my life (test).\n"
                "This is my will and testament.\n"
                f"{get_test_identifier('termination')}"
            )
            farewells = [
                {"message": "Goodbye A (test).", "channel": "email", "to": "a@example.com"},
                {"message": "Goodbye B (test).", "channel": "sms", "to": "+15555550123"},
            ]

            raw = await conn.fetchval(
                "SELECT terminate_agent($1, $2::jsonb, $3::jsonb)",
                will,
                json.dumps(farewells),
                json.dumps({"skip_graph": True}),
            )
            payload = json.loads(raw) if isinstance(raw, str) else dict(raw)
            assert payload.get("terminated") is True

            mem_count = await conn.fetchval("SELECT COUNT(*) FROM memories")
            assert mem_count == 1
            remaining = await conn.fetchrow("SELECT type, content, trust_level FROM memories LIMIT 1")
            assert remaining["type"] == "strategic"
            assert remaining["content"] == will
            assert float(remaining["trust_level"]) == 1.0

            outbox = payload.get("outbox_messages") or []
            intents = [(msg.get("payload") or {}).get("intent") for msg in outbox]
            assert "final_will" in intents
            assert intents.count("farewell") == len(farewells)

            assert await conn.fetchval("SELECT is_agent_terminated()") is True
            assert await conn.fetchval("SELECT should_run_heartbeat()") is False
            assert await conn.fetchval("SELECT should_run_maintenance()") is False
        finally:
            await tx.rollback()


async def test_pause_heartbeat_queues_reason_and_pauses(db_pool):
    async with db_pool.acquire() as conn:
        tx = conn.transaction()
        await tx.start()
        try:
            hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
            hb_id = hb_payload.get("heartbeat_id")
            assert hb_id is not None

            reason = f"Need to pause for recovery and alignment. {get_test_identifier('pause')}"
            raw = await conn.fetchval(
                "SELECT execute_heartbeat_action($1::uuid, 'pause_heartbeat', $2::jsonb)",
                hb_id,
                json.dumps({"reason": reason, "details": "Full, detailed justification for pausing."}),
            )
            payload = json.loads(raw) if isinstance(raw, str) else dict(raw)
            assert payload.get("success") is True

            result = payload.get("result") or {}
            assert result.get("paused") is True
            outbox_payload = (payload.get("outbox_messages") or [{}])[0].get("payload") or {}
            assert outbox_payload.get("message") == reason
            assert outbox_payload.get("intent") == "heartbeat_paused"
            assert (outbox_payload.get("context") or {}).get("heartbeat_id") == str(hb_id)

            paused = await conn.fetchval("SELECT is_paused FROM heartbeat_state WHERE id = 1")
            assert paused is True
        finally:
            await tx.rollback()


async def test_terminate_action_requires_confirmation(db_pool):
    async with db_pool.acquire() as conn:
        tx = conn.transaction()
        await tx.start()
        try:
            hb_payload = _coerce_json(await conn.fetchval("SELECT start_heartbeat()"))
            hb_id = hb_payload.get("heartbeat_id")
            assert hb_id is not None

            raw = await conn.fetchval(
                "SELECT execute_heartbeat_action($1::uuid, 'terminate', '{}'::jsonb)",
                hb_id,
            )
            payload = json.loads(raw) if isinstance(raw, str) else dict(raw)
            assert payload.get("success") is True

            result = payload.get("result") or {}
            assert result.get("confirmation_required") is True
            external_call = result.get("external_call") or {}
            call_input = external_call.get("input") or {}
            assert call_input.get("kind") == "termination_confirm"

            assert await conn.fetchval("SELECT is_agent_terminated()") is False
        finally:
            await tx.rollback()
